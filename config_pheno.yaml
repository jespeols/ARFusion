project_name: Phenotype-MLM
name: run
emb_dim: 512
hidden_dim: 512
num_heads: 8
num_layers: 8
dropout_prob: 0.1
mask_prob: 0.15
batch_size: 32
split: [0.8, 0.1, 0.1] # train, val, test
do_testing: False
epochs: 50
early_stopping_patience: 5
lr: 0.00005 
weight_decay: 0.01
print_progress_every: 3000
report_every: 1000
save_vocab: False   # True for fine-tuning or inference 
save_model: False   # True for fine-tuning or inference
random_state: 42
specials:   
  CLS: '[CLS]'
  PAD: '[PAD]'
  MASK: '[MASK]'
  UNK: '[UNK]'