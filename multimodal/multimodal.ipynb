{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base directory: c:\\Users\\jespe\\Documents\\GitHub_local\\ARFusion\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(os.path.abspath('')).parent\n",
    "sys.path.append(str(base_dir))\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# user-defined functions\n",
    "# from construct_vocab import construct_MM_vocab\n",
    "from data_preprocessing import preprocess_NCBI, preprocess_TESSy\n",
    "from utils import get_split_indices\n",
    "\n",
    "# set the base directory\n",
    "os.chdir(base_dir)\n",
    "print(\"base directory:\", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_dir / 'config_MM.yaml') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare unimodal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolates in the TESSy dataset: 1,437,004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>phenotypes</th>\n",
       "      <th>num_ab</th>\n",
       "      <th>num_R</th>\n",
       "      <th>num_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>AT</td>\n",
       "      <td>F</td>\n",
       "      <td>61.0</td>\n",
       "      <td>[AMP_S, CTX_S, GEN_S, TOB_S]</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>AT</td>\n",
       "      <td>M</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[AMP_S, CTX_S, GEN_S, TOB_S]</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>AT</td>\n",
       "      <td>F</td>\n",
       "      <td>79.0</td>\n",
       "      <td>[AMP_S, CTX_S, GEN_S, TOB_S]</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>AT</td>\n",
       "      <td>F</td>\n",
       "      <td>54.0</td>\n",
       "      <td>[AMP_S, CIP_S, CTX_S, GEN_S, TOB_S]</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>AT</td>\n",
       "      <td>M</td>\n",
       "      <td>63.0</td>\n",
       "      <td>[AMP_R, CAZ_S, CIP_R, CRO_S, CTX_S, GEN_S, OFX...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year country gender   age  \\\n",
       "0  2001      AT      F  61.0   \n",
       "1  2001      AT      M  37.0   \n",
       "2  2001      AT      F  79.0   \n",
       "3  2001      AT      F  54.0   \n",
       "4  2001      AT      M  63.0   \n",
       "\n",
       "                                          phenotypes  num_ab  num_R  num_S  \n",
       "0                       [AMP_S, CTX_S, GEN_S, TOB_S]       4      0      4  \n",
       "1                       [AMP_S, CTX_S, GEN_S, TOB_S]       4      0      4  \n",
       "2                       [AMP_S, CTX_S, GEN_S, TOB_S]       4      0      4  \n",
       "3                [AMP_S, CIP_S, CTX_S, GEN_S, TOB_S]       5      0      5  \n",
       "4  [AMP_R, CAZ_S, CIP_R, CRO_S, CTX_S, GEN_S, OFX...       8      3      5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = config['data']\n",
    "if data_dict['TESSy']['prepare_data']:\n",
    "    ds_TESSy = preprocess_TESSy(\n",
    "        path=data_dict['TESSy']['raw_path'],\n",
    "        pathogens=data_dict['pathogens'],\n",
    "        save_path=data_dict['TESSy']['save_path'],\n",
    "        exclude_antibiotics=data_dict['exclude_antibiotics'],\n",
    "        impute_age=data_dict['TESSy']['impute_age'],\n",
    "        impute_gender=data_dict['TESSy']['impute_gender']\n",
    "    )\n",
    "else:\n",
    "    ds_TESSy = pd.read_pickle(os.path.join(base_dir, data_dict['TESSy']['load_path']))\n",
    "num_TESSy = len(ds_TESSy)\n",
    "print(f\"Number of isolates in the TESSy dataset: {num_TESSy:,}\")\n",
    "ds_TESSy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing phenotypes...\n",
      "Parsing genotypes...\n",
      "Number of isolates before parsing: 341,565\n",
      "Removing 253 isolates with year < 1970\n",
      "Removing genotypes with assembly variants: ['=PARTIAL', '=MISTRANSLATION', '=HMM']\n",
      "Dropping 52 isolates with more than 35 genotypes\n",
      "Number of isolates after parsing: 339,349\n",
      "Number of isolates with phenotype info after parsing: 6,439\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>genotypes</th>\n",
       "      <th>phenotypes</th>\n",
       "      <th>num_ab</th>\n",
       "      <th>num_genotypes</th>\n",
       "      <th>num_point_mutations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>2013</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, parC_E84V=POINT, parE_I529L...</td>\n",
       "      <td>[AMP_R, FEP_R, CAZ_R, CRO_R, CIP_R, GEN_S, LVX...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>2014</td>\n",
       "      <td>USA</td>\n",
       "      <td>[dfrA14, glpT_E448K=POINT, parC_E84V=POINT, pa...</td>\n",
       "      <td>[AMP_R, CTX_R, CAZ_R, CRO_R, CIP_R, GEN_S, LVX...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>2012</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, aph(3'')-Ib, blaTEM-1, pmrB...</td>\n",
       "      <td>[AMP_R, CRO_S, CIP_S, GEN_S, NAL_S]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>2012</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, aph(3'')-Ib, blaTEM-1, pmrB...</td>\n",
       "      <td>[AMP_R, CRO_S, CIP_S, GEN_S, NAL_S]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>2012</td>\n",
       "      <td>USA</td>\n",
       "      <td>[floR, aph(3')-Ia=PARTIAL_END_OF_CONTIG, sul1,...</td>\n",
       "      <td>[AMP_S, CRO_S, CIP_S, GEN_S, NAL_S]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year country                                          genotypes  \\\n",
       "2819  2013     USA  [glpT_E448K=POINT, parC_E84V=POINT, parE_I529L...   \n",
       "2820  2014     USA  [dfrA14, glpT_E448K=POINT, parC_E84V=POINT, pa...   \n",
       "3210  2012     USA  [glpT_E448K=POINT, aph(3'')-Ib, blaTEM-1, pmrB...   \n",
       "3211  2012     USA  [glpT_E448K=POINT, aph(3'')-Ib, blaTEM-1, pmrB...   \n",
       "3212  2012     USA  [floR, aph(3')-Ia=PARTIAL_END_OF_CONTIG, sul1,...   \n",
       "\n",
       "                                             phenotypes  num_ab  \\\n",
       "2819  [AMP_R, FEP_R, CAZ_R, CRO_R, CIP_R, GEN_S, LVX...     9.0   \n",
       "2820  [AMP_R, CTX_R, CAZ_R, CRO_R, CIP_R, GEN_S, LVX...     9.0   \n",
       "3210                [AMP_R, CRO_S, CIP_S, GEN_S, NAL_S]     5.0   \n",
       "3211                [AMP_R, CRO_S, CIP_S, GEN_S, NAL_S]     5.0   \n",
       "3212                [AMP_S, CRO_S, CIP_S, GEN_S, NAL_S]     5.0   \n",
       "\n",
       "      num_genotypes  num_point_mutations  \n",
       "2819             18                    9  \n",
       "2820             21                    9  \n",
       "3210              8                    2  \n",
       "3211              8                    2  \n",
       "3212              6                    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data_dict['NCBI']['prepare_data']:\n",
    "    ds_NCBI = preprocess_NCBI(\n",
    "        path=data_dict['NCBI']['raw_path'],\n",
    "        save_path=data_dict['NCBI']['save_path'],\n",
    "        include_phenotype=data_dict['NCBI']['include_phenotype'],\n",
    "        ab_names_to_abbr=data_dict['antibiotics']['ab_names_to_abbr'],\n",
    "        exclude_antibiotics=data_dict['exclude_antibiotics'], \n",
    "        threshold_year=data_dict['NCBI']['threshold_year'],\n",
    "        exclude_genotypes=data_dict['NCBI']['exclude_genotypes'],\n",
    "        exclude_assembly_variants=data_dict['NCBI']['exclude_assembly_variants'],\n",
    "        exclusion_chars=data_dict['NCBI']['exclusion_chars'],\n",
    "        gene_count_threshold=data_dict['NCBI']['gene_count_threshold']\n",
    "    )\n",
    "else:\n",
    "    ds_NCBI = pd.read_pickle(os.path.join(base_dir, data_dict['NCBI']['load_path']))\n",
    "num_NCBI = len(ds_NCBI)\n",
    "ds_NCBI[ds_NCBI['num_ab'] > 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "def construct_MM_vocab(df_geno: pd.DataFrame,\n",
    "                       df_pheno: pd.DataFrame,\n",
    "                       antibiotics: list,\n",
    "                       specials: dict,\n",
    "                       savepath_vocab: Path = None):\n",
    "    token_counter = Counter()\n",
    "    ds_geno = df_geno.copy()\n",
    "    ds_pheno = df_pheno.copy()\n",
    "    \n",
    "    PAD, UNK = specials['PAD'], specials['UNK']\n",
    "    special_tokens = specials.values()\n",
    "    \n",
    "    year_geno = ds_geno[ds_geno['year'] != PAD]['year'].astype('Int16')\n",
    "    min_year = min(year_geno.min(), ds_pheno['year'].min())\n",
    "    max_year = max(year_geno.max(), ds_pheno['year'].max())\n",
    "    year_range = range(min_year, max_year+1)\n",
    "    token_counter.update([str(y) for y in year_range])\n",
    "    \n",
    "    min_age, max_age = ds_pheno['age'].min(), ds_pheno['age'].max()\n",
    "    age_range = range(int(min_age), int(max_age+1))\n",
    "    token_counter.update([str(a) for a in age_range])\n",
    "    \n",
    "    genders = ds_pheno['gender'].unique().astype(str).tolist()\n",
    "    token_counter.update(genders)\n",
    "    \n",
    "    pheno_countries = ds_pheno['country'].sort_values().unique()\n",
    "    geno_countries = ds_geno['country'].sort_values().dropna().unique()\n",
    "    countries = set(pheno_countries).union(set(geno_countries))\n",
    "    token_counter.update(countries)\n",
    "    \n",
    "    token_counter.update(list(chain(*ds_geno['genotypes'])))\n",
    "    token_counter.update([ab + '_' + res for ab in antibiotics for res in ['R', 'S']])  \n",
    "    \n",
    "    vocab = Vocab(token_counter, specials=special_tokens)\n",
    "    vocab.set_default_index(vocab[UNK])\n",
    "    if savepath_vocab:\n",
    "        torch.save(vocab, savepath_vocab)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "\n",
    "class MMPretrainDataset(Dataset):\n",
    "    # df column names\n",
    "    INDICES_MASKED = 'indices_masked' # input to BERT, token indices of the masked sequence\n",
    "    TARGET_RESISTANCES = 'target_resistances' # resistance of the masked antibiotic, what we want to predict\n",
    "    TARGET_INDICES = 'target_indices' # indices of the target tokens for the genotype masking\n",
    "    TOKEN_TYPES = 'token_types' # 0 for patient info, 1 for genotype, 2 for phenotype\n",
    "    # if sequences are included\n",
    "    MASKED_SEQUENCE = 'masked_sequence'\n",
    "    # ORIGINAL_SEQUENCE = 'original_sequence'\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        ds_geno: pd.DataFrame,\n",
    "        ds_pheno: pd.DataFrame,\n",
    "        vocab,\n",
    "        antibiotics: list,\n",
    "        specials: dict,\n",
    "        max_seq_len: int,\n",
    "        mask_prob_geno: float,\n",
    "        mask_prob_pheno: float = None,\n",
    "        num_known_ab: int = None,\n",
    "        include_sequences: bool = False,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.ds_geno = ds_geno.reset_index(drop=True)\n",
    "        self.num_geno = ds_geno.shape[0]\n",
    "        self.ds_pheno = ds_pheno.reset_index(drop=True)\n",
    "        self.num_pheno = ds_pheno.shape[0]\n",
    "        self.num_samples = self.num_geno + self.num_pheno\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(antibiotics)\n",
    "        self.ab_to_idx = {ab: idx for idx, ab in enumerate(antibiotics)}\n",
    "        self.enc_res = {'S': 0, 'R': 1}\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.CLS, self.PAD, self.MASK, self.UNK = specials.values()\n",
    "        \n",
    "        self.mask_prob_geno = mask_prob_geno\n",
    "        self.mask_prob_pheno = mask_prob_pheno\n",
    "        self.num_known_ab = num_known_ab\n",
    "        assert not (self.mask_prob_pheno and self.num_known_ab), \"Either mask_prob_pheno or num_known_ab should be given, not both\"\n",
    "        \n",
    "        self.ds_geno['source'] = 'geno'\n",
    "        self.ds_pheno['source'] = 'pheno'\n",
    "        geno_cols = ['year', 'country', 'num_genotypes', 'source']\n",
    "        pheno_cols = ['year', 'country', 'gender', 'age', 'num_ab', 'source']\n",
    "        self.combined_ds = pd.concat([self.ds_geno[geno_cols], self.ds_pheno[pheno_cols]], ignore_index=True)\n",
    "        \n",
    "        self.include_sequences = include_sequences\n",
    "        if self.include_sequences:\n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_INDICES, self.TARGET_RESISTANCES, self.TOKEN_TYPES,\n",
    "                            self.MASKED_SEQUENCE]\n",
    "        else:\n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_INDICES, self.TARGET_RESISTANCES, self.TOKEN_TYPES]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        \n",
    "        input = torch.tensor(item[self.INDICES_MASKED], dtype=torch.long, device=device)\n",
    "        target_res = torch.tensor(item[self.TARGET_RESISTANCES], dtype=torch.float32, device=device)\n",
    "        token_types = torch.tensor(item[self.TOKEN_TYPES], dtype=torch.long, device=device)\n",
    "        target_indices = torch.tensor(item[self.TARGET_INDICES], dtype=torch.long, device=device) \n",
    "        attn_mask = (input != self.vocab[self.PAD]).unsqueeze(0).unsqueeze(1) # one dim for batch, one for heads   \n",
    "        \n",
    "        if self.include_sequences:\n",
    "            # original_sequence = item[self.ORIGINAL_SEQUENCE]\n",
    "            masked_sequence = item[self.MASKED_SEQUENCE]\n",
    "            return input, target_indices, target_res, token_types, attn_mask, masked_sequence\n",
    "        else:\n",
    "            return input, target_indices, target_res, token_types, attn_mask\n",
    "    \n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        geno_sequences = deepcopy(self.ds_geno['genotypes'].tolist())\n",
    "        pheno_sequences = deepcopy(self.ds_pheno['phenotypes'].tolist())\n",
    "                \n",
    "        masked_geno_sequences, geno_target_indices = self._mask_geno_sequences(geno_sequences)\n",
    "        geno_target_resistances = [[-1]*self.num_ab for _ in range(self.num_geno)] # no ab masking for genotypes\n",
    "        geno_token_types = [[0]*3 + [1]*(self.max_seq_len - 3) for _ in range(self.num_geno)]\n",
    "        \n",
    "        # pheno sequences use ab masking, so token_mask is all False, CE-Loss won't be calculated for these sequences\n",
    "        masked_pheno_sequences, pheno_target_resistances = self._mask_pheno_sequences(pheno_sequences)\n",
    "        pheno_token_types = [[0]*5 + [2]*(self.max_seq_len - 5) for _ in range(self.num_pheno)]\n",
    "        pheno_target_indices = [[-1]*self.max_seq_len for _ in range(self.num_pheno)]\n",
    "\n",
    "        masked_sequences = masked_geno_sequences + masked_pheno_sequences\n",
    "        indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "        target_indices = geno_target_indices + pheno_target_indices\n",
    "        token_types = geno_token_types + pheno_token_types\n",
    "        target_resistances = geno_target_resistances + pheno_target_resistances\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            rows = zip(indices_masked, target_indices, target_resistances, token_types,\n",
    "                       masked_sequences)\n",
    "        else:\n",
    "            rows = zip(indices_masked, target_indices, target_resistances, token_types)\n",
    "        self.df = pd.DataFrame(rows, columns=self.columns)\n",
    "        \n",
    "        \n",
    "    def _mask_geno_sequences(self, geno_sequences):\n",
    "        masked_geno_sequences = list()\n",
    "        target_indices_list = list()\n",
    "        \n",
    "        years = self.ds_geno['year'].astype(str).tolist()\n",
    "        countries = self.ds_geno['country'].tolist()\n",
    "        seq_starts = [[self.CLS, years[i], countries[i]] for i in range(self.ds_geno.shape[0])]\n",
    "        for i, geno_seq in enumerate(geno_sequences):\n",
    "            seq_len = len(geno_seq)\n",
    "            token_mask = np.random.rand(seq_len) < self.mask_prob_geno   \n",
    "            target_indices = np.array([-1]*seq_len)\n",
    "            if not token_mask.any():\n",
    "                # if no tokens are masked, mask one random token\n",
    "                idx = np.random.randint(seq_len)\n",
    "                target_indices[idx] = self.vocab[geno_seq[idx]]\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    geno_seq[idx] = self.MASK\n",
    "                elif r < 0.9:\n",
    "                    geno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            else:\n",
    "                indices = token_mask.nonzero()[0]\n",
    "                target_indices[indices] = self.vocab.lookup_indices([geno_seq[i] for i in indices])\n",
    "                for i in indices:\n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8:\n",
    "                        geno_seq[i] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        geno_seq[i] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            geno_seq = seq_starts[i] + geno_seq\n",
    "            target_indices = [-1]*3 + target_indices.tolist() \n",
    "            masked_geno_sequences.append(geno_seq)\n",
    "            target_indices_list.append(target_indices)\n",
    "            \n",
    "        masked_geno_sequences = [seq + [self.PAD]*(self.max_seq_len - len(seq)) for seq in masked_geno_sequences]\n",
    "        target_indices_list = [indices + [-1]*(self.max_seq_len - len(indices)) for indices in target_indices_list]\n",
    "        return masked_geno_sequences, target_indices_list\n",
    "    \n",
    "    \n",
    "    def _mask_pheno_sequences(self, pheno_sequences):\n",
    "        masked_pheno_sequences = list()\n",
    "        target_resistances = list()\n",
    "        \n",
    "        years = self.ds_pheno['year'].astype('Int16').astype(str).tolist()\n",
    "        countries = self.ds_pheno['country'].tolist()\n",
    "        genders = self.ds_pheno['gender'].tolist()\n",
    "        ages = self.ds_pheno['age'].astype(int).astype(str).tolist()\n",
    "        seq_starts = [[self.CLS, years[i], countries[i], genders[i], ages[i]] for i in range(self.num_pheno)]\n",
    "\n",
    "        if self.mask_prob_pheno:\n",
    "            for i, pheno_seq in enumerate(pheno_sequences):\n",
    "                seq_len = len(pheno_seq)\n",
    "                token_mask = np.random.rand(seq_len) < self.mask_prob_pheno\n",
    "                target_res = [-1]*self.num_ab\n",
    "                if not token_mask.any():\n",
    "                    idx = np.random.randint(seq_len)\n",
    "                    ab, res = pheno_seq[idx].split('_')\n",
    "                    target_res[self.ab_to_idx[ab]] = self.enc_res[res]  \n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8:\n",
    "                        pheno_seq[idx] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        pheno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size)) \n",
    "                else:\n",
    "                    for idx in token_mask.nonzero()[0]:\n",
    "                        ab, res = pheno_seq[idx].split('_')\n",
    "                        target_res[self.ab_to_idx[ab]] = self.enc_res[res]\n",
    "                        r = np.random.rand()\n",
    "                        if r < 0.8:\n",
    "                            pheno_seq[idx] = self.MASK\n",
    "                        elif r < 0.9:\n",
    "                            pheno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "                pheno_seq = seq_starts[i] + pheno_seq\n",
    "                masked_pheno_sequences.append(pheno_seq)\n",
    "                target_resistances.append(target_res)\n",
    "        else:\n",
    "            for i, pheno_seq in enumerate(pheno_sequences):\n",
    "                seq_len = len(pheno_seq)\n",
    "                target_res = [-1]*self.num_ab\n",
    "                indices = np.random.choice(seq_len, self.num_known_ab, replace=False)\n",
    "                for idx in indices:\n",
    "                    ab, res = pheno_seq[idx].split('_')\n",
    "                    target_res[self.ab_to_idx[ab]] = self.enc_res[res]\n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8:\n",
    "                        pheno_seq[idx] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        pheno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "                pheno_seq = seq_starts[i] + pheno_seq\n",
    "                masked_pheno_sequences.append(pheno_seq)\n",
    "                target_resistances.append(target_res)\n",
    "            \n",
    "        masked_pheno_sequences = [seq + [self.PAD]*(self.max_seq_len - len(seq)) for seq in masked_pheno_sequences]\n",
    "        return masked_pheno_sequences, target_resistances\n",
    "            \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1, random_state=self.random_state)\n",
    "        self.combined_ds = self.combined_ds.loc[self.df.index].reset_index(drop=True) # combined dataset is aligned with df\n",
    "        self.df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal.models import BERT\n",
    "\n",
    "class MMBertPreTrainer(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config: dict,\n",
    "                 model: BERT,\n",
    "                 antibiotics: list, # list of antibiotics in the dataset\n",
    "                 train_set,\n",
    "                 val_set,\n",
    "                 results_dir: Path = None,\n",
    "    ):\n",
    "        super(MMBertPreTrainer, self).__init__()\n",
    "        \n",
    "        self.random_state = config[\"random_state\"]\n",
    "        np.random.seed(self.random_state)\n",
    "        torch.manual_seed(self.random_state)\n",
    "        torch.cuda.manual_seed(self.random_state)\n",
    "        \n",
    "        self.model = model\n",
    "        self.project_name = config[\"project_name\"]\n",
    "        self.wandb_name = config[\"name\"] if config[\"name\"] else datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics) \n",
    "        \n",
    "        self.train_set, self.train_size = train_set, len(train_set)\n",
    "        self.val_set, self.val_size = val_set, len(val_set) \n",
    "        assert round(self.val_size / (self.train_size + self.val_size), 2) == config[\"val_share\"], \"Validation set size does not match intended val_share\"\n",
    "        self.val_share, self.train_share = config[\"val_share\"], 1 - config[\"val_share\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.num_batches = round(self.train_size / self.batch_size)\n",
    "        self.vocab = self.train_set.vocab\n",
    "         \n",
    "        self.lr = config[\"lr\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.patience = config[\"early_stopping_patience\"]\n",
    "        self.save_model_ = config[\"save_model\"] if config[\"save_model\"] else False\n",
    "        \n",
    "        self.mask_prob_geno = self.train_set.mask_prob_geno\n",
    "        self.mask_prob_pheno = self.train_set.mask_prob_pheno\n",
    "        self.mask_probs = {'geno': self.mask_prob_geno, 'pheno': self.mask_prob_pheno}\n",
    "        self.num_known_ab = self.train_set.num_known_ab\n",
    "        \n",
    "        self.ab_criterions = [nn.BCEWithLogitsLoss().to(device) for _ in range(self.num_ab)] # the list is so that we can introduce individual weights\n",
    "        self.geno_criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device) # ignores loss where target_indices == -1\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.scheduler = None\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.9)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.98)\n",
    "                 \n",
    "        self.current_epoch = 0\n",
    "        self.report_every = config[\"report_every\"] if config[\"report_every\"] else 1000\n",
    "        self.print_progress_every = config[\"print_progress_every\"] if config[\"print_progress_every\"] else 1000\n",
    "        self._splitter_size = 70\n",
    "        self.results_dir = results_dir\n",
    "        if self.results_dir:\n",
    "            self.results_dir.mkdir(parents=True, exist_ok=True) \n",
    "        \n",
    "        \n",
    "    def print_model_summary(self):        \n",
    "        print(\"Model summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Embedding dim: {self.model.emb_dim}\")\n",
    "        print(f\"Feed-forward dim: {self.model.ff_dim}\")\n",
    "        print(f\"Hidden dim: {self.model.hidden_dim}\")\n",
    "        print(f\"Number of heads: {self.model.num_heads}\")\n",
    "        print(f\"Number of encoder layers: {self.model.num_layers}\")\n",
    "        print(f\"Dropout probability: {self.model.dropout_prob:.0%}\")\n",
    "        print(f\"Max sequence length: {self.model.max_seq_len}\")\n",
    "        print(f\"Vocab size: {len(self.vocab):,}\")\n",
    "        print(f\"Number of parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "    \n",
    "    def print_trainer_summary(self):\n",
    "        print(\"Trainer summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        if device.type == \"cuda\":\n",
    "            print(f\"Device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "        else:\n",
    "            print(f\"Device: {device}\")        \n",
    "        print(f\"Training dataset size: {self.train_size:,}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Number of batches: {self.num_batches:,}\")\n",
    "        print(f\"Number of antibiotics: {self.num_ab}\")\n",
    "        print(f\"Antibiotics: {self.antibiotics}\")\n",
    "        print(f\"CV split: {self.train_share:.0%} train | {self.val_share:.0%} val\")\n",
    "        print(f\"Mask probability (genotypes): {self.mask_prob_geno:.0%}\")\n",
    "        if self.mask_prob_pheno:\n",
    "            print(f\"Mask probability (phenotypes): {self.mask_prob_pheno:.0%}\")\n",
    "        if self.num_known_ab:\n",
    "            print(f\"Number of known antibiotics: {self.num_known_ab}\")\n",
    "        print(f\"Number of epochs: {self.epochs}\")\n",
    "        print(f\"Early stopping patience: {self.patience}\")\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Weight decay: {self.weight_decay}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def __call__(self):      \n",
    "        print(\"Initializing training...\")\n",
    "        self.wandb_run = self._init_wandb()\n",
    "        self.val_set.prepare_dataset()\n",
    "        self.val_set.shuffle() # to avoid batches of only genotypes or only phenotypes\n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf') \n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            self.model.train()\n",
    "            # Dynamic masking: New mask for training set each epoch\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            train_losses = self.train(self.current_epoch) # returns loss, averaged over batches\n",
    "            self.losses.append(train_losses['loss']) \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            print(\"Loss: {:.4f} | Genotype loss: {:.4f} | Phenotype loss: {:.4f}\".format(\n",
    "                train_losses['loss'], train_losses['geno_loss'], train_losses['pheno_loss']))\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader, self.val_set)\n",
    "            print(\"Val loss: {:.4f} | Genotype loss: {:.4f} | Phenotype loss: {:.4f}\".format(\n",
    "                val_results['loss'], val_results['geno_loss'], val_results['pheno_loss']))\n",
    "            print(\"Phenotype accuracy: {:.2%} | Phenotype isolate accuracy: {:.2%}\".format(\n",
    "                val_results['pheno_acc'], val_results['pheno_iso_acc']))\n",
    "            print(\"Genotype accuracy: {:.2%} | Genotype isolate accuracy: {:.2%}\".format(\n",
    "                val_results['geno_acc'], val_results['geno_iso_acc']))\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self._update_val_lists(val_results)\n",
    "            self._report_epoch_results()\n",
    "            early_stop = self.early_stopping()\n",
    "            if early_stop:\n",
    "                print(f\"Early stopping at epoch {self.current_epoch+1} with validation loss {self.val_losses[-1]:.4f}\")\n",
    "                print(f\"Validation stats at best epoch ({self.best_epoch+1}):\")\n",
    "                s1 = f\"Loss: {self.val_losses[self.best_epoch]:.4f}\" \n",
    "                s1 += f\"| Phenotype Loss: {self.val_pheno_losses[self.best_epoch]:.4f}\"\n",
    "                s1 += f\" | Genotype Loss: {self.val_geno_losses[self.best_epoch]:.4f}\"\n",
    "                print(s1)\n",
    "                s2 = f\" | Phenotype accuracy: {self.val_pheno_accs[self.best_epoch]:.2%}\"\n",
    "                s2 += f\" | Phenotype isolate accuracy: {self.val_pheno_iso_accs[self.best_epoch]:.2%}\"\n",
    "                print(s2)\n",
    "                s3 = f\" | Genotype accuracy: {self.val_geno_accs[self.best_epoch]:.2%}\"\n",
    "                s3 += f\" | Genotype isolate accuracy: {self.val_geno_iso_accs[self.best_epoch]:.2%}\"\n",
    "                print(s3)\n",
    "                self.wandb_run.log({\n",
    "                    \"Losses/final_val_loss\": self.best_val_loss, \n",
    "                    \"Losses/final_val_geno_loss\": self.val_geno_losses[self.best_epoch],\n",
    "                    \"Losses/final_val_pheno_loss\": self.val_pheno_losses[self.best_epoch],\n",
    "                    \"Accuracies/final_val_pheno_acc\": self.val_pheno_accs[self.best_epoch],\n",
    "                    \"Accuracies/final_val_pheno_iso_acc\": self.val_pheno_iso_accs[self.best_epoch],\n",
    "                    \"Accuracies/final_val_geno_acc\": self.val_geno_accs[self.best_epoch],\n",
    "                    \"Accuracies/final_val_geno_iso_acc\": self.val_geno_iso_accs[self.best_epoch],\n",
    "                    \"final_epoch\": self.best_epoch+1\n",
    "                })\n",
    "                print(\"=\"*self._splitter_size)\n",
    "                self.model.load_state_dict(self.best_model_state) \n",
    "                self.current_epoch = self.best_epoch\n",
    "                break\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "        if not early_stop:    \n",
    "            self.wandb_run.log({\n",
    "                    \"Losses/final_val_loss\": self.best_val_loss, \n",
    "                    \"Losses/final_val_geno_loss\": self.val_geno_losses[-1],\n",
    "                    \"Losses/final_val_pheno_loss\": self.val_pheno_losses[-1],\n",
    "                    \"Accuracies/final_val_pheno_acc\": self.val_pheno_accs[-1],\n",
    "                    \"Accuracies/final_val_pheno_iso_acc\": self.val_pheno_iso_accs[-1],\n",
    "                    \"Accuracies/final_val_geno_acc\": self.val_geno_accs[-1],\n",
    "                    \"Accuracies/final_val_geno_iso_acc\": self.val_geno_iso_accs[-1],\n",
    "                    \"final_epoch\": self.current_epoch+1\n",
    "                })\n",
    "        self.model.is_pretrained = True\n",
    "        if self.save_model_:\n",
    "            self.save_model() \n",
    "        train_time = (time.time() - start_time)/60\n",
    "        self.wandb_run.log({\"Training time (min)\": train_time})\n",
    "        disp_time = f\"{train_time//60:.0f}h {train_time % 60:.1f} min\" if train_time > 60 else f\"{train_time:.1f} min\"\n",
    "        print(f\"Training completed in {disp_time}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        if not early_stop:\n",
    "            print(\"Final validation stats:\")\n",
    "            s1 = f\"Loss: {self.val_losses[-1]:.4f} | Phenotype Loss: {self.val_pheno_losses[-1]:.4f}\"\n",
    "            s1 += f\" | Genotype Loss: {self.val_geno_losses[-1]:.4f}\"\n",
    "            print(s1)\n",
    "            s2 = f\"Phenotype accuracy: {self.val_pheno_accs[-1]:.2%}\"\n",
    "            s2 += f\" | Phenotype isolate accuracy: {self.val_pheno_iso_accs[-1]:.2%}\"\n",
    "            print(s2)\n",
    "            s3 = f\" Genotype accuracy: {self.val_geno_accs[-1]:.2%}\"\n",
    "            s3 += f\" | Genotype isolate accuracy: {self.val_geno_iso_accs[-1]:.2%}\"\n",
    "            print(s3)\n",
    "        \n",
    "        results = {\n",
    "            \"best epoch\": self.best_epoch,\n",
    "            \"train_losses\": self.losses,\n",
    "            \"val_losses\": self.val_losses,\n",
    "            \"val_pheno_losses\": self.val_pheno_losses,\n",
    "            \"val_geno_losses\": self.val_geno_losses,\n",
    "            \"val_pheno_accs\": self.val_pheno_accs,\n",
    "            \"val_geno_accs\": self.val_geno_accs,\n",
    "            \"val_pheno_iso_accs\": self.val_pheno_iso_accs,\n",
    "            \"val_geno_iso_accs\": self.val_geno_iso_accs,\n",
    "            \"train_time\": train_time,\n",
    "            \"val_iso_stats_geno\": self.val_iso_stats_geno,\n",
    "            \"val_iso_stats_pheno\": self.val_iso_stats_pheno,\n",
    "            \"val_ab_stats\": self.val_ab_stats\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_geno_loss, epoch_pheno_loss = 0, 0\n",
    "        geno_batches, pheno_batches = 0, 0\n",
    "        reporting_loss, printing_loss = 0, 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            batch_index = i + 1\n",
    "            self.optimizer.zero_grad() # zero out gradients\n",
    "            \n",
    "            input, target_indices, target_res, token_types, attn_mask = batch   \n",
    "            # input, target_indices, target_res, token_types, attn_mask, masked_sequences = batch   \n",
    "            pred_logits, token_pred = self.model(input, token_types, attn_mask) # get predictions for all antibiotics\n",
    "            ab_mask = target_res != -1 # (batch_size, num_ab), True if antibiotic is masked, False otherwise\n",
    "            \n",
    "            loss = 0\n",
    "            if ab_mask.any(): # if there are phenotypes in the batch\n",
    "                ## Phenotype loss ##\n",
    "                ab_indices = ab_mask.any(dim=0).nonzero().squeeze(-1).tolist() # list of indices of antibiotics present in the batch\n",
    "                losses = list()\n",
    "                for j in ab_indices: \n",
    "                    mask = ab_mask[:, j] # (batch_size,), indicates which samples contain the antibiotic masked\n",
    "                    # isolate the predictions and targets for the antibiotic\n",
    "                    ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                    ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                    ab_loss = self.ab_criterions[j](ab_pred_logits, ab_targets)\n",
    "                    losses.append(ab_loss)\n",
    "                pheno_loss = sum(losses) / len(losses) # average loss over antibiotics\n",
    "                epoch_pheno_loss += pheno_loss.item()\n",
    "                pheno_batches += 1\n",
    "                loss += pheno_loss\n",
    "                \n",
    "            if (target_indices != -1).any(): # if there are genotypes in the batch\n",
    "                ## Genotype loss ##\n",
    "                geno_loss = self.geno_criterion(token_pred.transpose(-1, -2), target_indices) # DOUBLE-CHECK DIMENSIONS\n",
    "                epoch_geno_loss += geno_loss.item()\n",
    "                geno_batches += 1\n",
    "                loss += geno_loss\n",
    "            reporting_loss += loss.item()\n",
    "            printing_loss += loss.item()\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step() \n",
    "            if batch_index % self.report_every == 0:\n",
    "                self._report_loss_results(batch_index, reporting_loss)\n",
    "                reporting_loss = 0 \n",
    "                \n",
    "            if batch_index % self.print_progress_every == 0:\n",
    "                time_elapsed = time.gmtime(time.time() - time_ref) \n",
    "                self._print_loss_summary(time_elapsed, batch_index, printing_loss) \n",
    "                printing_loss = 0  \n",
    "        avg_pheno_loss = epoch_pheno_loss / pheno_batches\n",
    "        avg_geno_loss = epoch_geno_loss / geno_batches\n",
    "        avg_epoch_loss = avg_geno_loss + avg_pheno_loss\n",
    "        losses = {\"loss\": avg_epoch_loss, \"geno_loss\": avg_geno_loss, \"pheno_loss\": avg_pheno_loss}\n",
    "        return losses \n",
    "    \n",
    "    \n",
    "    def early_stopping(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "        \n",
    "            \n",
    "    def evaluate(self, loader: DataLoader, ds_obj):\n",
    "        self.model.eval()\n",
    "        # prepare evaluation statistics dataframes\n",
    "        ab_stats, iso_stats_pheno, iso_stats_geno = self._init_eval_stats(ds_obj)\n",
    "        with torch.no_grad(): \n",
    "            ## Antibiotic tracking ##\n",
    "            ab_num = np.zeros((self.num_ab, 2)) # tracks the occurence for each antibiotic & resistance\n",
    "            ab_num_preds = np.zeros_like(ab_num) # tracks the number of predictions for each antibiotic & resistance\n",
    "            ab_num_correct = np.zeros_like(ab_num) # tracks the number of correct predictions for each antibiotic & resistance\n",
    "            ## General tracking ##\n",
    "            tot_pheno_loss, tot_geno_loss = 0, 0\n",
    "            geno_batches, pheno_batches = 0, 0\n",
    "            for i, batch in enumerate(loader):                \n",
    "                input, target_indices, target_res, token_types, attn_mask = batch   \n",
    "                # input, target_indices, target_res, token_types, attn_mask, sequences, masked_sequences = batch  \n",
    "                 \n",
    "                pred_logits, token_pred = self.model(input, token_types, attn_mask) # get predictions for all antibiotics\n",
    "                pred_res = torch.where(pred_logits > 0, torch.ones_like(pred_logits), torch.zeros_like(pred_logits)) # logits -> 0/1 (S/R)\n",
    "                        \n",
    "                ###### Phenotype loss ######\n",
    "                ab_mask = target_res >= 0 # (batch_size, num_ab), True if antibiotic is masked, False otherwise\n",
    "                if ab_mask.any(): # if there are phenotypes in the batch\n",
    "                    iso_stats_pheno = self._update_pheno_stats(i, pred_res, target_res, ab_mask, iso_stats_pheno)\n",
    "                    \n",
    "                    ab_indices = ab_mask.any(dim=0).nonzero().squeeze(-1).tolist() # list of indices of antibiotics present in the batch\n",
    "                    losses = list()\n",
    "                    for j in ab_indices: \n",
    "                        mask = ab_mask[:, j] # (batch_size,)\n",
    "                        \n",
    "                        # isolate the predictions and targets for the antibiotic\n",
    "                        ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                        ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                        num_R = ab_targets.sum().item()\n",
    "                        num_S = ab_targets.shape[0] - num_R\n",
    "                        ab_num[j, :] += [num_S, num_R]\n",
    "                        \n",
    "                        ab_loss = self.ab_criterions[j](ab_pred_logits, ab_targets)\n",
    "                        losses.append(ab_loss)\n",
    "                        \n",
    "                        ab_pred_res = pred_res[mask, j]\n",
    "                        ab_num_correct[j, :] += self._get_num_correct(ab_pred_res, ab_targets)    \n",
    "                        ab_num_preds[j, :] += self._get_num_preds(ab_pred_res)\n",
    "                    pheno_loss = sum(losses) / len(losses) # average loss over antibiotics\n",
    "                    tot_pheno_loss += pheno_loss.item()\n",
    "                    pheno_batches += 1\n",
    "                    \n",
    "                ###### Genotype loss ######\n",
    "                token_mask = target_indices != -1 # (batch_size, max_seq_len), True if token is masked, False otherwise\n",
    "                if token_mask.any(): # if there are genotypes in the batch\n",
    "                    iso_stats_geno = self._update_geno_stats(i, token_pred, target_indices, token_mask, iso_stats_geno)\n",
    "                    \n",
    "                    geno_loss = self.geno_criterion(token_pred.transpose(-1, -2), target_indices) \n",
    "                    tot_geno_loss += geno_loss.item()\n",
    "                    geno_batches += 1\n",
    "                    \n",
    "        avg_geno_loss = tot_geno_loss / geno_batches\n",
    "        avg_pheno_loss = tot_pheno_loss / pheno_batches\n",
    "        avg_loss = avg_geno_loss + avg_pheno_loss  \n",
    "        \n",
    "        ab_stats = self._update_ab_eval_stats(ab_stats, ab_num, ab_num_preds, ab_num_correct)\n",
    "        iso_stats_geno, iso_stats_pheno = self._calculate_iso_stats(iso_stats_geno, iso_stats_pheno)\n",
    "        \n",
    "        pheno_acc = iso_stats_pheno['num_correct'].sum() / iso_stats_pheno['num_masked'].sum()\n",
    "        pheno_iso_acc = iso_stats_pheno['all_correct'].sum() / iso_stats_pheno.shape[0]\n",
    "        geno_acc = iso_stats_geno['num_correct'].sum() / iso_stats_geno['num_masked'].sum()\n",
    "        geno_iso_acc = iso_stats_geno['all_correct'].sum() / iso_stats_geno.shape[0]\n",
    "\n",
    "        results = {\n",
    "            \"loss\": avg_loss, \n",
    "            \"geno_loss\": avg_geno_loss, \n",
    "            \"pheno_loss\": avg_pheno_loss,\n",
    "            \"pheno_acc\": pheno_acc,\n",
    "            \"pheno_iso_acc\": pheno_iso_acc,\n",
    "            \"geno_acc\": geno_acc,\n",
    "            \"geno_iso_acc\": geno_iso_acc,\n",
    "            \"ab_stats\": ab_stats,\n",
    "            \"iso_stats_pheno\": iso_stats_pheno,\n",
    "            \"iso_stats_geno\": iso_stats_geno\n",
    "        }\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def _init_result_lists(self):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_geno_losses = []\n",
    "        self.val_pheno_losses = []\n",
    "        self.val_pheno_accs = []\n",
    "        self.val_pheno_iso_accs = []\n",
    "        self.val_geno_accs = []\n",
    "        self.val_geno_iso_accs = []\n",
    "        self.val_ab_stats = []\n",
    "        self.val_iso_stats_pheno = []\n",
    "        self.val_iso_stats_geno = []\n",
    "        \n",
    "        \n",
    "    def _update_val_lists(self, results: dict):\n",
    "        self.val_losses.append(results[\"loss\"])\n",
    "        self.val_geno_losses.append(results[\"geno_loss\"])\n",
    "        self.val_pheno_losses.append(results[\"pheno_loss\"])\n",
    "        self.val_pheno_accs.append(results[\"pheno_acc\"])\n",
    "        self.val_pheno_iso_accs.append(results[\"pheno_iso_acc\"])\n",
    "        self.val_geno_accs.append(results[\"geno_acc\"])\n",
    "        self.val_geno_iso_accs.append(results[\"geno_iso_acc\"])\n",
    "        self.val_ab_stats.append(results[\"ab_stats\"])\n",
    "        self.val_iso_stats_pheno.append(results[\"iso_stats_pheno\"])\n",
    "        self.val_iso_stats_geno.append(results[\"iso_stats_geno\"])\n",
    "    \n",
    "    \n",
    "    def _init_eval_stats(self, ds_obj):\n",
    "        ab_stats = pd.DataFrame(columns=[\n",
    "            'antibiotic', 'num_tot', 'num_S', 'num_R', 'num_pred_S', 'num_pred_R', \n",
    "            'num_correct', 'num_correct_S', 'num_correct_R',\n",
    "            'accuracy', 'sensitivity', 'specificity', 'precision', 'F1'\n",
    "        ])\n",
    "        ab_stats['antibiotic'] = self.antibiotics\n",
    "        ab_stats['num_tot'], ab_stats['num_S'], ab_stats['num_R'] = 0, 0, 0\n",
    "        ab_stats['num_pred_S'], ab_stats['num_pred_R'] = 0, 0\n",
    "        ab_stats['num_correct'], ab_stats['num_correct_S'], ab_stats['num_correct_R'] = 0, 0, 0\n",
    "\n",
    "        combined_ds = ds_obj.combined_ds\n",
    "        ## Extract phenotype samples \n",
    "        iso_stats_pheno = combined_ds[combined_ds['source'] == 'pheno'].drop(columns=['source', 'num_genotypes'])\n",
    "        iso_stats_pheno['num_masked'], iso_stats_pheno['num_masked_S'], iso_stats_pheno['num_masked_R'] = 0, 0, 0\n",
    "        iso_stats_pheno['num_correct'], iso_stats_pheno['correct_S'], iso_stats_pheno['correct_R'] = 0, 0, 0\n",
    "        iso_stats_pheno['sensitivity'], iso_stats_pheno['specificity'], iso_stats_pheno['accuracy'] = 0, 0, 0\n",
    "        iso_stats_pheno['all_correct'] = False \n",
    "        \n",
    "        ## Extract genotype samples\n",
    "        iso_stats_geno = combined_ds[combined_ds['source'] == 'geno'].drop(columns=['source', 'age', 'gender', 'num_ab'])\n",
    "        iso_stats_geno.replace(self.val_set.PAD, np.nan, inplace=True)\n",
    "        iso_stats_geno['num_masked'], iso_stats_geno['num_correct'], iso_stats_geno['accuracy'] = 0, 0, 0\n",
    "        iso_stats_geno['all_correct'] = False\n",
    "      \n",
    "        return ab_stats, iso_stats_pheno, iso_stats_geno\n",
    "    \n",
    "    \n",
    "    def _update_ab_eval_stats(self, ab_stats: pd.DataFrame, num, num_preds, num_correct):\n",
    "        for j in range(self.num_ab): \n",
    "            ab_stats.loc[j, 'num_tot'] = num[j, :].sum()\n",
    "            ab_stats.loc[j, 'num_S'], ab_stats.loc[j, 'num_R'] = num[j, 0], num[j, 1]\n",
    "            ab_stats.loc[j, 'num_pred_S'], ab_stats.loc[j, 'num_pred_R'] = num_preds[j, 0], num_preds[j, 1]\n",
    "            ab_stats.loc[j, 'num_correct'] = num_correct[j, :].sum()\n",
    "            ab_stats.loc[j, 'num_correct_S'], ab_stats.loc[j, 'num_correct_R'] = num_correct[j, 0], num_correct[j, 1]\n",
    "        ab_stats['accuracy'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct']/row['num_tot'] if row['num_tot'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['sensitivity'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct_R']/row['num_R'] if row['num_R'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['specificity'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct_S']/row['num_S'] if row['num_S'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['precision'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct_R']/row['num_pred_R'] if row['num_pred_R'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['F1'] = ab_stats.apply(\n",
    "            lambda row: 2*row['precision']*row['sensitivity']/(row['precision']+row['sensitivity']) \n",
    "            if row['precision'] > 0 and row['sensitivity'] > 0 else np.nan, axis=1)\n",
    "        return ab_stats\n",
    "    \n",
    "    \n",
    "    def _get_num_correct(self, pred_res: torch.Tensor, target_res: torch.Tensor):\n",
    "        eq = torch.eq(pred_res, target_res)\n",
    "        num_correct_S = eq[target_res == 0].sum().item()\n",
    "        num_correct_R = eq[target_res == 1].sum().item()\n",
    "        return [num_correct_S, num_correct_R]\n",
    "    \n",
    "    \n",
    "    def _get_num_preds(self, pred_res: torch.Tensor):\n",
    "        num_pred_S = (pred_res == 0).sum().item()\n",
    "        num_pred_R = (pred_res == 1).sum().item()\n",
    "        return [num_pred_S, num_pred_R]\n",
    "    \n",
    "    \n",
    "    def _update_pheno_stats(self, batch_idx, pred_res: torch.Tensor, target_res: torch.Tensor, \n",
    "                          ab_mask: torch.Tensor, iso_stats_pheno: pd.DataFrame):\n",
    "        indices = ab_mask.any(dim=1).nonzero().squeeze(-1).tolist() # list of isolates where phenotypes are present\n",
    "        for idx in indices: \n",
    "            iso_ab_mask = ab_mask[idx]\n",
    "            df_idx = batch_idx * self.batch_size + idx # index of the isolate in the combined dataset\n",
    "            \n",
    "            # counts\n",
    "            num_masked_tot = iso_ab_mask.sum().item()\n",
    "            num_masked_R = target_res[idx][iso_ab_mask].sum().item()\n",
    "            num_masked_S = num_masked_tot - num_masked_R\n",
    "            \n",
    "            # statistics            \n",
    "            iso_target_res = target_res[idx][iso_ab_mask]\n",
    "            eq = torch.eq(pred_res[idx][iso_ab_mask], iso_target_res)\n",
    "            num_correct_R = eq[iso_target_res == 1].sum().item()\n",
    "            num_correct_S = eq[iso_target_res == 0].sum().item()\n",
    "            num_correct = num_correct_S + num_correct_R\n",
    "            all_correct = eq.all().item()\n",
    "            \n",
    "            data = {\n",
    "                'num_masked': num_masked_tot, 'num_masked_S': num_masked_S, 'num_masked_R': num_masked_R, \n",
    "                'num_correct': num_correct, 'correct_S': num_correct_S, 'correct_R': num_correct_R,\n",
    "                'all_correct': all_correct\n",
    "            }\n",
    "            iso_stats_pheno.loc[df_idx, data.keys()] = data.values()\n",
    "                          \n",
    "        return iso_stats_pheno\n",
    "    \n",
    "    \n",
    "    def _update_geno_stats(self, batch_idx, token_pred: torch.Tensor, target_indices: torch.Tensor, \n",
    "                           token_mask: torch.Tensor, iso_stats_geno: pd.DataFrame):\n",
    "        indices = token_mask.any(dim=1).nonzero().squeeze(-1).tolist() # list of isolates where genotypes are present\n",
    "        for idx in indices:\n",
    "            iso_token_mask = token_mask[idx]    \n",
    "            df_idx = batch_idx * self.batch_size + idx # index of the isolate in the combined dataset\n",
    "            \n",
    "            num_masked = iso_token_mask.sum().item()\n",
    "            pred_tokens = token_pred[idx, iso_token_mask].argmax(dim=-1)\n",
    "            targets = target_indices[idx, iso_token_mask]\n",
    "\n",
    "            eq = torch.eq(pred_tokens, targets)\n",
    "            data = {\n",
    "                'num_masked': num_masked, 'num_correct': eq.sum().item(), 'all_correct': eq.all().item()\n",
    "            }\n",
    "            iso_stats_geno.loc[df_idx, data.keys()] = data.values()\n",
    "                \n",
    "        return iso_stats_geno\n",
    "    \n",
    "    \n",
    "    def _calculate_iso_stats(self, iso_stats_geno: pd.DataFrame, iso_stats_pheno: pd.DataFrame):\n",
    "        \n",
    "        iso_stats_geno['accuracy'] = iso_stats_geno['num_correct'] / iso_stats_geno['num_masked']\n",
    "        \n",
    "        iso_stats_pheno['accuracy'] = iso_stats_pheno['num_correct'] / iso_stats_pheno['num_masked']\n",
    "        iso_stats_pheno['sensitivity'] = iso_stats_pheno.apply(\n",
    "            lambda row: row['correct_R']/row['num_masked_R'] if row['num_masked_R'] > 0 else np.nan, axis=1\n",
    "        )\n",
    "        iso_stats_pheno['specificity'] = iso_stats_pheno.apply(\n",
    "            lambda row: row['correct_S']/row['num_masked_S'] if row['num_masked_S'] > 0 else np.nan, axis=1\n",
    "        )\n",
    "        \n",
    "        return iso_stats_geno, iso_stats_pheno\n",
    "        \n",
    "    \n",
    "     \n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"hidden_dim\": self.model.hidden_dim,\n",
    "                \"num_layers\": self.model.num_layers,\n",
    "                \"num_heads\": self.model.num_heads,\n",
    "                \"emb_dim\": self.model.emb_dim,\n",
    "                'ff_dim': self.model.ff_dim,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"mask_probs\": self.mask_probs,\n",
    "                \"max_seq_len\": self.model.max_seq_len,\n",
    "                \"vocab_size\": len(self.vocab),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "                \"num_antibiotics\": self.num_ab,\n",
    "                \"antibiotics\": self.antibiotics,\n",
    "                \"train_size\": self.train_size,\n",
    "                \"random_state\": self.random_state,\n",
    "                'val_share': self.val_share,\n",
    "                \"val_size\": self.val_size,\n",
    "                # \"early_stopping_patience\": self.patience,\n",
    "                # \"dropout_prob\": self.model.dropout_prob,\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/live_loss\", step_metric=\"batch\")\n",
    "        self.wandb_run.define_metric(\"Losses/train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_geno_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_pheno_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_pheno_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_pheno_iso_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_geno_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_geno_iso_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Losses/final_val_geno_loss\")\n",
    "        self.wandb_run.define_metric(\"Losses/final_val_pheno_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_pheno_acc\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_pheno_iso_acc\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_geno_acc\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_geno_iso_acc\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "     \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"Losses/train_loss\": self.losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            \"Losses/val_geno_loss\": self.val_geno_losses[-1],\n",
    "            \"Losses/val_pheno_loss\": self.val_pheno_losses[-1],\n",
    "            \"Accuracies/val_pheno_acc\": self.val_pheno_accs[-1],\n",
    "            \"Accuracies/val_pheno_iso_acc\": self.val_pheno_iso_accs[-1],\n",
    "            \"Accuracies/val_geno_acc\": self.val_geno_accs[-1],\n",
    "            \"Accuracies/val_geno_iso_acc\": self.val_geno_iso_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)\n",
    "    \n",
    "        \n",
    "    def _report_loss_results(self, batch_index, tot_loss):\n",
    "        avg_loss = tot_loss / self.report_every\n",
    "        \n",
    "        global_step = self.current_epoch * self.num_batches + batch_index # global step, total #batches seen\n",
    "        self.wandb_run.log({\"batch\": global_step, \"Losses/live_loss\": avg_loss})\n",
    "    \n",
    "        \n",
    "    def _print_loss_summary(self, time_elapsed, batch_index, tot_loss):\n",
    "        progress = batch_index / self.num_batches\n",
    "        mlm_loss = tot_loss / self.print_progress_every\n",
    "          \n",
    "        s = f\"{time.strftime('%H:%M:%S', time_elapsed)}\" \n",
    "        s += f\" | Epoch: {self.current_epoch+1}/{self.epochs} | {batch_index}/{self.num_batches} ({progress:.2%}) | \"\\\n",
    "                f\"Loss: {mlm_loss:.4f}\"\n",
    "        print(s)\n",
    "    \n",
    "    \n",
    "    def save_model(self, savepath: Path = None):\n",
    "        if not savepath:\n",
    "            savepath = self.results_dir / \"model_state.pt\"\n",
    "        torch.save(self.model.state_dict(), savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "        print(\"=\"*self._splitter_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "======================================================================\n",
      "Embedding dim: 256\n",
      "Feed-forward dim: 256\n",
      "Hidden dim: 256\n",
      "Number of heads: 4\n",
      "Number of encoder layers: 6\n",
      "Dropout probability: 10%\n",
      "Max sequence length: 40\n",
      "Vocab size: 1,550\n",
      "Number of parameters: 3,178,254\n",
      "======================================================================\n",
      "Trainer summary:\n",
      "======================================================================\n",
      "Device: cuda (NVIDIA GeForce RTX 3080)\n",
      "Training dataset size: 1,504,426\n",
      "Batch size: 64\n",
      "Number of batches: 23,507\n",
      "Number of antibiotics: 18\n",
      "Antibiotics: ['OFX', 'NAL', 'CRO', 'CIP', 'LVX', 'AMX', 'MFX', 'FEP', 'AMC', 'AMP', 'CTX', 'TZP', 'NET', 'CAZ', 'NOR', 'TOB', 'GEN', 'PIP']\n",
      "CV split: 85% train | 15% val\n",
      "Mask probability (genotypes): 25%\n",
      "Mask probability (phenotypes): 25%\n",
      "Number of epochs: 100\n",
      "Early stopping patience: 3\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.01\n",
      "======================================================================\n",
      "Initializing training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jespe\\Documents\\GitHub_local\\ARFusion\\wandb\\run-20231217_200002-p9kyrx8v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jespeols/Multimodal-MLM/runs/p9kyrx8v' target=\"_blank\">pt_test</a></strong> to <a href='https://wandb.ai/jespeols/Multimodal-MLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jespeols/Multimodal-MLM' target=\"_blank\">https://wandb.ai/jespeols/Multimodal-MLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jespeols/Multimodal-MLM/runs/p9kyrx8v' target=\"_blank\">https://wandb.ai/jespeols/Multimodal-MLM/runs/p9kyrx8v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "00:00:39 | Epoch: 1/100 | 1000/23507 (4.25%) | Loss: 3.4236\n",
      "00:01:18 | Epoch: 1/100 | 2000/23507 (8.51%) | Loss: 2.4712\n",
      "00:01:56 | Epoch: 1/100 | 3000/23507 (12.76%) | Loss: 2.1257\n",
      "00:02:35 | Epoch: 1/100 | 4000/23507 (17.02%) | Loss: 1.9206\n",
      "00:03:14 | Epoch: 1/100 | 5000/23507 (21.27%) | Loss: 1.8152\n",
      "00:03:52 | Epoch: 1/100 | 6000/23507 (25.52%) | Loss: 1.6995\n",
      "00:04:31 | Epoch: 1/100 | 7000/23507 (29.78%) | Loss: 1.6559\n",
      "00:05:09 | Epoch: 1/100 | 8000/23507 (34.03%) | Loss: 1.5799\n",
      "00:05:48 | Epoch: 1/100 | 9000/23507 (38.29%) | Loss: 1.5414\n",
      "00:06:26 | Epoch: 1/100 | 10000/23507 (42.54%) | Loss: 1.4960\n",
      "00:07:04 | Epoch: 1/100 | 11000/23507 (46.79%) | Loss: 1.4640\n",
      "00:07:42 | Epoch: 1/100 | 12000/23507 (51.05%) | Loss: 1.4326\n",
      "00:08:20 | Epoch: 1/100 | 13000/23507 (55.30%) | Loss: 1.4398\n",
      "00:08:59 | Epoch: 1/100 | 14000/23507 (59.56%) | Loss: 1.3851\n",
      "00:09:37 | Epoch: 1/100 | 15000/23507 (63.81%) | Loss: 1.3754\n",
      "00:10:15 | Epoch: 1/100 | 16000/23507 (68.06%) | Loss: 1.3606\n",
      "00:10:53 | Epoch: 1/100 | 17000/23507 (72.32%) | Loss: 1.3518\n",
      "00:11:32 | Epoch: 1/100 | 18000/23507 (76.57%) | Loss: 1.3290\n",
      "00:12:10 | Epoch: 1/100 | 19000/23507 (80.83%) | Loss: 1.3041\n",
      "00:12:48 | Epoch: 1/100 | 20000/23507 (85.08%) | Loss: 1.3206\n",
      "00:13:26 | Epoch: 1/100 | 21000/23507 (89.34%) | Loss: 1.2843\n",
      "00:14:04 | Epoch: 1/100 | 22000/23507 (93.59%) | Loss: 1.3027\n",
      "00:14:42 | Epoch: 1/100 | 23000/23507 (97.84%) | Loss: 1.2759\n",
      "Epoch completed in 15.0 min\n",
      "Loss: 1.6167 | Genotype loss: 1.3751 | Phenotype loss: 0.2416\n",
      "Evaluating on validation set...\n",
      "Val loss: 1.1593 | Genotype loss: 0.9425 | Phenotype loss: 0.2168\n",
      "Phenotype accuracy: 91.34% | Phenotype isolate accuracy: 85.79%\n",
      "Genotype accuracy: 75.95% | Genotype isolate accuracy: 67.23%\n",
      "Elapsed time: 00:24:37\n",
      "Epoch 2/100\n",
      "00:00:38 | Epoch: 2/100 | 1000/23507 (4.25%) | Loss: 1.2378\n",
      "00:01:16 | Epoch: 2/100 | 2000/23507 (8.51%) | Loss: 1.2555\n",
      "00:01:54 | Epoch: 2/100 | 3000/23507 (12.76%) | Loss: 1.2190\n",
      "00:02:30 | Epoch: 2/100 | 4000/23507 (17.02%) | Loss: 1.2165\n",
      "00:03:06 | Epoch: 2/100 | 5000/23507 (21.27%) | Loss: 1.2006\n",
      "00:03:41 | Epoch: 2/100 | 6000/23507 (25.52%) | Loss: 1.2115\n",
      "00:04:17 | Epoch: 2/100 | 7000/23507 (29.78%) | Loss: 1.2171\n",
      "00:04:53 | Epoch: 2/100 | 8000/23507 (34.03%) | Loss: 1.2167\n",
      "00:05:29 | Epoch: 2/100 | 9000/23507 (38.29%) | Loss: 1.1962\n",
      "00:06:05 | Epoch: 2/100 | 10000/23507 (42.54%) | Loss: 1.1996\n",
      "00:06:41 | Epoch: 2/100 | 11000/23507 (46.79%) | Loss: 1.1934\n",
      "00:07:17 | Epoch: 2/100 | 12000/23507 (51.05%) | Loss: 1.1335\n",
      "00:07:53 | Epoch: 2/100 | 13000/23507 (55.30%) | Loss: 1.1686\n",
      "00:08:29 | Epoch: 2/100 | 14000/23507 (59.56%) | Loss: 1.1669\n",
      "00:09:05 | Epoch: 2/100 | 15000/23507 (63.81%) | Loss: 1.1738\n",
      "00:09:41 | Epoch: 2/100 | 16000/23507 (68.06%) | Loss: 1.1306\n",
      "00:10:17 | Epoch: 2/100 | 17000/23507 (72.32%) | Loss: 1.1370\n",
      "00:10:52 | Epoch: 2/100 | 18000/23507 (76.57%) | Loss: 1.1482\n",
      "00:11:28 | Epoch: 2/100 | 19000/23507 (80.83%) | Loss: 1.1545\n",
      "00:12:04 | Epoch: 2/100 | 20000/23507 (85.08%) | Loss: 1.1619\n",
      "00:12:40 | Epoch: 2/100 | 21000/23507 (89.34%) | Loss: 1.1110\n",
      "00:13:16 | Epoch: 2/100 | 22000/23507 (93.59%) | Loss: 1.1393\n",
      "00:13:52 | Epoch: 2/100 | 23000/23507 (97.84%) | Loss: 1.1036\n",
      "Epoch completed in 14.2 min\n",
      "Loss: 1.1759 | Genotype loss: 0.9560 | Phenotype loss: 0.2199\n",
      "Evaluating on validation set...\n",
      "Val loss: 1.0432 | Genotype loss: 0.8317 | Phenotype loss: 0.2115\n",
      "Phenotype accuracy: 91.42% | Phenotype isolate accuracy: 85.80%\n",
      "Genotype accuracy: 78.50% | Genotype isolate accuracy: 69.92%\n",
      "Elapsed time: 00:49:21\n",
      "Epoch 3/100\n",
      "00:00:35 | Epoch: 3/100 | 1000/23507 (4.25%) | Loss: 1.1229\n",
      "00:01:10 | Epoch: 3/100 | 2000/23507 (8.51%) | Loss: 1.0877\n",
      "00:01:45 | Epoch: 3/100 | 3000/23507 (12.76%) | Loss: 1.1291\n",
      "00:02:20 | Epoch: 3/100 | 4000/23507 (17.02%) | Loss: 1.1123\n",
      "00:02:56 | Epoch: 3/100 | 5000/23507 (21.27%) | Loss: 1.0985\n",
      "00:03:31 | Epoch: 3/100 | 6000/23507 (25.52%) | Loss: 1.0926\n",
      "00:04:06 | Epoch: 3/100 | 7000/23507 (29.78%) | Loss: 1.1020\n",
      "00:04:41 | Epoch: 3/100 | 8000/23507 (34.03%) | Loss: 1.0933\n",
      "00:05:16 | Epoch: 3/100 | 9000/23507 (38.29%) | Loss: 1.0839\n",
      "00:05:51 | Epoch: 3/100 | 10000/23507 (42.54%) | Loss: 1.0927\n",
      "00:06:26 | Epoch: 3/100 | 11000/23507 (46.79%) | Loss: 1.0869\n",
      "00:07:04 | Epoch: 3/100 | 12000/23507 (51.05%) | Loss: 1.0869\n",
      "00:07:41 | Epoch: 3/100 | 13000/23507 (55.30%) | Loss: 1.0643\n",
      "00:08:18 | Epoch: 3/100 | 14000/23507 (59.56%) | Loss: 1.0713\n",
      "00:08:54 | Epoch: 3/100 | 15000/23507 (63.81%) | Loss: 1.0911\n",
      "00:09:30 | Epoch: 3/100 | 16000/23507 (68.06%) | Loss: 1.0936\n",
      "00:10:07 | Epoch: 3/100 | 17000/23507 (72.32%) | Loss: 1.0565\n",
      "00:10:43 | Epoch: 3/100 | 18000/23507 (76.57%) | Loss: 1.0961\n",
      "00:11:20 | Epoch: 3/100 | 19000/23507 (80.83%) | Loss: 1.0829\n",
      "00:11:56 | Epoch: 3/100 | 20000/23507 (85.08%) | Loss: 1.0802\n",
      "00:12:33 | Epoch: 3/100 | 21000/23507 (89.34%) | Loss: 1.0842\n",
      "00:13:09 | Epoch: 3/100 | 22000/23507 (93.59%) | Loss: 1.0602\n",
      "00:13:45 | Epoch: 3/100 | 23000/23507 (97.84%) | Loss: 1.0504\n",
      "Epoch completed in 14.1 min\n",
      "Loss: 1.0863 | Genotype loss: 0.8719 | Phenotype loss: 0.2144\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.9881 | Genotype loss: 0.7812 | Phenotype loss: 0.2069\n",
      "Phenotype accuracy: 91.56% | Phenotype isolate accuracy: 86.12%\n",
      "Genotype accuracy: 79.81% | Genotype isolate accuracy: 71.26%\n",
      "Elapsed time: 01:12:36\n",
      "Epoch 4/100\n",
      "00:00:36 | Epoch: 4/100 | 1000/23507 (4.25%) | Loss: 1.0437\n",
      "00:01:12 | Epoch: 4/100 | 2000/23507 (8.51%) | Loss: 1.0386\n",
      "00:01:49 | Epoch: 4/100 | 3000/23507 (12.76%) | Loss: 1.0376\n",
      "00:02:25 | Epoch: 4/100 | 4000/23507 (17.02%) | Loss: 1.0409\n",
      "00:03:01 | Epoch: 4/100 | 5000/23507 (21.27%) | Loss: 1.0466\n",
      "00:03:38 | Epoch: 4/100 | 6000/23507 (25.52%) | Loss: 1.0478\n",
      "00:04:14 | Epoch: 4/100 | 7000/23507 (29.78%) | Loss: 1.0111\n",
      "00:04:50 | Epoch: 4/100 | 8000/23507 (34.03%) | Loss: 1.0296\n",
      "00:05:27 | Epoch: 4/100 | 9000/23507 (38.29%) | Loss: 1.0351\n",
      "00:06:03 | Epoch: 4/100 | 10000/23507 (42.54%) | Loss: 1.0118\n",
      "00:06:39 | Epoch: 4/100 | 11000/23507 (46.79%) | Loss: 1.0205\n",
      "00:07:16 | Epoch: 4/100 | 12000/23507 (51.05%) | Loss: 1.0375\n",
      "00:07:52 | Epoch: 4/100 | 13000/23507 (55.30%) | Loss: 1.0154\n",
      "00:08:28 | Epoch: 4/100 | 14000/23507 (59.56%) | Loss: 1.0403\n",
      "00:09:05 | Epoch: 4/100 | 15000/23507 (63.81%) | Loss: 1.0223\n",
      "00:09:41 | Epoch: 4/100 | 16000/23507 (68.06%) | Loss: 1.0334\n",
      "00:10:18 | Epoch: 4/100 | 17000/23507 (72.32%) | Loss: 1.0483\n",
      "00:10:53 | Epoch: 4/100 | 18000/23507 (76.57%) | Loss: 1.0180\n",
      "00:11:28 | Epoch: 4/100 | 19000/23507 (80.83%) | Loss: 1.0267\n",
      "00:12:04 | Epoch: 4/100 | 20000/23507 (85.08%) | Loss: 1.0211\n",
      "00:12:39 | Epoch: 4/100 | 21000/23507 (89.34%) | Loss: 1.0106\n",
      "00:13:14 | Epoch: 4/100 | 22000/23507 (93.59%) | Loss: 1.0213\n",
      "00:13:50 | Epoch: 4/100 | 23000/23507 (97.84%) | Loss: 1.0180\n",
      "Epoch completed in 14.1 min\n",
      "Loss: 1.0293 | Genotype loss: 0.8178 | Phenotype loss: 0.2115\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.9467 | Genotype loss: 0.7420 | Phenotype loss: 0.2046\n",
      "Phenotype accuracy: 91.55% | Phenotype isolate accuracy: 86.13%\n",
      "Genotype accuracy: 80.71% | Genotype isolate accuracy: 72.17%\n",
      "Elapsed time: 01:37:04\n",
      "Epoch 5/100\n",
      "00:00:35 | Epoch: 5/100 | 1000/23507 (4.25%) | Loss: 1.0048\n",
      "00:01:10 | Epoch: 5/100 | 2000/23507 (8.51%) | Loss: 1.0038\n",
      "00:01:46 | Epoch: 5/100 | 3000/23507 (12.76%) | Loss: 1.0092\n",
      "00:02:21 | Epoch: 5/100 | 4000/23507 (17.02%) | Loss: 0.9817\n",
      "00:02:56 | Epoch: 5/100 | 5000/23507 (21.27%) | Loss: 1.0016\n",
      "00:03:32 | Epoch: 5/100 | 6000/23507 (25.52%) | Loss: 1.0028\n",
      "00:04:07 | Epoch: 5/100 | 7000/23507 (29.78%) | Loss: 1.0145\n",
      "00:04:42 | Epoch: 5/100 | 8000/23507 (34.03%) | Loss: 0.9908\n",
      "00:05:18 | Epoch: 5/100 | 9000/23507 (38.29%) | Loss: 0.9886\n",
      "00:05:53 | Epoch: 5/100 | 10000/23507 (42.54%) | Loss: 0.9987\n",
      "00:06:28 | Epoch: 5/100 | 11000/23507 (46.79%) | Loss: 1.0009\n",
      "00:07:03 | Epoch: 5/100 | 12000/23507 (51.05%) | Loss: 1.0177\n",
      "00:07:39 | Epoch: 5/100 | 13000/23507 (55.30%) | Loss: 0.9963\n",
      "00:08:14 | Epoch: 5/100 | 14000/23507 (59.56%) | Loss: 0.9935\n",
      "00:08:49 | Epoch: 5/100 | 15000/23507 (63.81%) | Loss: 0.9896\n",
      "00:09:25 | Epoch: 5/100 | 16000/23507 (68.06%) | Loss: 1.0149\n",
      "00:10:00 | Epoch: 5/100 | 17000/23507 (72.32%) | Loss: 0.9917\n",
      "00:10:35 | Epoch: 5/100 | 18000/23507 (76.57%) | Loss: 0.9675\n",
      "00:11:10 | Epoch: 5/100 | 19000/23507 (80.83%) | Loss: 0.9892\n",
      "00:11:45 | Epoch: 5/100 | 20000/23507 (85.08%) | Loss: 0.9868\n",
      "00:12:21 | Epoch: 5/100 | 21000/23507 (89.34%) | Loss: 0.9849\n",
      "00:12:56 | Epoch: 5/100 | 22000/23507 (93.59%) | Loss: 0.9911\n",
      "00:13:31 | Epoch: 5/100 | 23000/23507 (97.84%) | Loss: 0.9777\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.9950 | Genotype loss: 0.7865 | Phenotype loss: 0.2085\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.9218 | Genotype loss: 0.7185 | Phenotype loss: 0.2033\n",
      "Phenotype accuracy: 91.63% | Phenotype isolate accuracy: 86.29%\n",
      "Genotype accuracy: 81.28% | Genotype isolate accuracy: 72.85%\n",
      "Elapsed time: 02:00:21\n",
      "Epoch 6/100\n",
      "00:00:37 | Epoch: 6/100 | 1000/23507 (4.25%) | Loss: 0.9825\n",
      "00:01:13 | Epoch: 6/100 | 2000/23507 (8.51%) | Loss: 0.9857\n",
      "00:01:50 | Epoch: 6/100 | 3000/23507 (12.76%) | Loss: 0.9520\n",
      "00:02:27 | Epoch: 6/100 | 4000/23507 (17.02%) | Loss: 0.9883\n",
      "00:03:03 | Epoch: 6/100 | 5000/23507 (21.27%) | Loss: 0.9712\n",
      "00:03:40 | Epoch: 6/100 | 6000/23507 (25.52%) | Loss: 0.9988\n",
      "00:04:17 | Epoch: 6/100 | 7000/23507 (29.78%) | Loss: 0.9904\n",
      "00:04:54 | Epoch: 6/100 | 8000/23507 (34.03%) | Loss: 0.9649\n",
      "00:05:30 | Epoch: 6/100 | 9000/23507 (38.29%) | Loss: 0.9755\n",
      "00:06:07 | Epoch: 6/100 | 10000/23507 (42.54%) | Loss: 0.9563\n",
      "00:06:44 | Epoch: 6/100 | 11000/23507 (46.79%) | Loss: 0.9842\n",
      "00:07:21 | Epoch: 6/100 | 12000/23507 (51.05%) | Loss: 0.9756\n",
      "00:07:58 | Epoch: 6/100 | 13000/23507 (55.30%) | Loss: 0.9751\n",
      "00:08:35 | Epoch: 6/100 | 14000/23507 (59.56%) | Loss: 0.9692\n",
      "00:09:11 | Epoch: 6/100 | 15000/23507 (63.81%) | Loss: 0.9623\n",
      "00:09:48 | Epoch: 6/100 | 16000/23507 (68.06%) | Loss: 0.9592\n",
      "00:10:25 | Epoch: 6/100 | 17000/23507 (72.32%) | Loss: 0.9709\n",
      "00:11:02 | Epoch: 6/100 | 18000/23507 (76.57%) | Loss: 0.9450\n",
      "00:11:39 | Epoch: 6/100 | 19000/23507 (80.83%) | Loss: 0.9757\n",
      "00:12:15 | Epoch: 6/100 | 20000/23507 (85.08%) | Loss: 0.9540\n",
      "00:12:52 | Epoch: 6/100 | 21000/23507 (89.34%) | Loss: 0.9574\n",
      "00:13:29 | Epoch: 6/100 | 22000/23507 (93.59%) | Loss: 0.9801\n",
      "00:14:05 | Epoch: 6/100 | 23000/23507 (97.84%) | Loss: 0.9666\n",
      "Epoch completed in 14.4 min\n",
      "Loss: 0.9714 | Genotype loss: 0.7641 | Phenotype loss: 0.2073\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8969 | Genotype loss: 0.6965 | Phenotype loss: 0.2004\n",
      "Phenotype accuracy: 91.70% | Phenotype isolate accuracy: 86.36%\n",
      "Genotype accuracy: 81.72% | Genotype isolate accuracy: 73.49%\n",
      "Elapsed time: 02:24:32\n",
      "Epoch 7/100\n",
      "00:00:35 | Epoch: 7/100 | 1000/23507 (4.25%) | Loss: 0.9612\n",
      "00:01:10 | Epoch: 7/100 | 2000/23507 (8.51%) | Loss: 0.9589\n",
      "00:01:45 | Epoch: 7/100 | 3000/23507 (12.76%) | Loss: 0.9475\n",
      "00:02:20 | Epoch: 7/100 | 4000/23507 (17.02%) | Loss: 0.9690\n",
      "00:02:55 | Epoch: 7/100 | 5000/23507 (21.27%) | Loss: 0.9732\n",
      "00:03:30 | Epoch: 7/100 | 6000/23507 (25.52%) | Loss: 0.9491\n",
      "00:04:05 | Epoch: 7/100 | 7000/23507 (29.78%) | Loss: 0.9678\n",
      "00:04:40 | Epoch: 7/100 | 8000/23507 (34.03%) | Loss: 0.9634\n",
      "00:05:15 | Epoch: 7/100 | 9000/23507 (38.29%) | Loss: 0.9554\n",
      "00:05:50 | Epoch: 7/100 | 10000/23507 (42.54%) | Loss: 0.9645\n",
      "00:06:25 | Epoch: 7/100 | 11000/23507 (46.79%) | Loss: 0.9586\n",
      "00:07:00 | Epoch: 7/100 | 12000/23507 (51.05%) | Loss: 0.9462\n",
      "00:07:35 | Epoch: 7/100 | 13000/23507 (55.30%) | Loss: 0.9545\n",
      "00:08:10 | Epoch: 7/100 | 14000/23507 (59.56%) | Loss: 0.9407\n",
      "00:08:45 | Epoch: 7/100 | 15000/23507 (63.81%) | Loss: 0.9541\n",
      "00:09:20 | Epoch: 7/100 | 16000/23507 (68.06%) | Loss: 0.9639\n",
      "00:09:55 | Epoch: 7/100 | 17000/23507 (72.32%) | Loss: 0.9595\n",
      "00:10:30 | Epoch: 7/100 | 18000/23507 (76.57%) | Loss: 0.9498\n",
      "00:11:05 | Epoch: 7/100 | 19000/23507 (80.83%) | Loss: 0.9423\n",
      "00:11:40 | Epoch: 7/100 | 20000/23507 (85.08%) | Loss: 0.9223\n",
      "00:12:15 | Epoch: 7/100 | 21000/23507 (89.34%) | Loss: 0.9391\n",
      "00:12:50 | Epoch: 7/100 | 22000/23507 (93.59%) | Loss: 0.9549\n",
      "00:13:25 | Epoch: 7/100 | 23000/23507 (97.84%) | Loss: 0.9426\n",
      "Epoch completed in 13.7 min\n",
      "Loss: 0.9536 | Genotype loss: 0.7481 | Phenotype loss: 0.2056\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8853 | Genotype loss: 0.6844 | Phenotype loss: 0.2009\n",
      "Phenotype accuracy: 91.71% | Phenotype isolate accuracy: 86.38%\n",
      "Genotype accuracy: 82.10% | Genotype isolate accuracy: 73.90%\n",
      "Elapsed time: 02:48:29\n",
      "Epoch 8/100\n",
      "00:00:35 | Epoch: 8/100 | 1000/23507 (4.25%) | Loss: 0.9241\n",
      "00:01:10 | Epoch: 8/100 | 2000/23507 (8.51%) | Loss: 0.9365\n",
      "00:01:45 | Epoch: 8/100 | 3000/23507 (12.76%) | Loss: 0.9434\n",
      "00:02:20 | Epoch: 8/100 | 4000/23507 (17.02%) | Loss: 0.9581\n",
      "00:02:55 | Epoch: 8/100 | 5000/23507 (21.27%) | Loss: 0.9428\n",
      "00:03:30 | Epoch: 8/100 | 6000/23507 (25.52%) | Loss: 0.9591\n",
      "00:04:05 | Epoch: 8/100 | 7000/23507 (29.78%) | Loss: 0.9410\n",
      "00:04:40 | Epoch: 8/100 | 8000/23507 (34.03%) | Loss: 0.9458\n",
      "00:05:15 | Epoch: 8/100 | 9000/23507 (38.29%) | Loss: 0.9308\n",
      "00:05:51 | Epoch: 8/100 | 10000/23507 (42.54%) | Loss: 0.9328\n",
      "00:06:26 | Epoch: 8/100 | 11000/23507 (46.79%) | Loss: 0.9489\n",
      "00:07:01 | Epoch: 8/100 | 12000/23507 (51.05%) | Loss: 0.9346\n",
      "00:07:36 | Epoch: 8/100 | 13000/23507 (55.30%) | Loss: 0.9309\n",
      "00:08:11 | Epoch: 8/100 | 14000/23507 (59.56%) | Loss: 0.9332\n",
      "00:08:46 | Epoch: 8/100 | 15000/23507 (63.81%) | Loss: 0.9412\n",
      "00:09:21 | Epoch: 8/100 | 16000/23507 (68.06%) | Loss: 0.9156\n",
      "00:09:56 | Epoch: 8/100 | 17000/23507 (72.32%) | Loss: 0.9264\n",
      "00:10:31 | Epoch: 8/100 | 18000/23507 (76.57%) | Loss: 0.9243\n",
      "00:11:06 | Epoch: 8/100 | 19000/23507 (80.83%) | Loss: 0.9233\n",
      "00:11:41 | Epoch: 8/100 | 20000/23507 (85.08%) | Loss: 0.9287\n",
      "00:12:16 | Epoch: 8/100 | 21000/23507 (89.34%) | Loss: 0.9369\n",
      "00:12:51 | Epoch: 8/100 | 22000/23507 (93.59%) | Loss: 0.8987\n",
      "00:13:26 | Epoch: 8/100 | 23000/23507 (97.84%) | Loss: 0.9314\n",
      "Epoch completed in 13.7 min\n",
      "Loss: 0.9353 | Genotype loss: 0.7305 | Phenotype loss: 0.2047\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8681 | Genotype loss: 0.6695 | Phenotype loss: 0.1986\n",
      "Phenotype accuracy: 91.73% | Phenotype isolate accuracy: 86.39%\n",
      "Genotype accuracy: 82.57% | Genotype isolate accuracy: 74.47%\n",
      "Elapsed time: 03:12:09\n",
      "Epoch 9/100\n",
      "00:00:35 | Epoch: 9/100 | 1000/23507 (4.25%) | Loss: 0.9429\n",
      "00:01:11 | Epoch: 9/100 | 2000/23507 (8.51%) | Loss: 0.9268\n",
      "00:01:47 | Epoch: 9/100 | 3000/23507 (12.76%) | Loss: 0.9224\n",
      "00:02:23 | Epoch: 9/100 | 4000/23507 (17.02%) | Loss: 0.9128\n",
      "00:02:59 | Epoch: 9/100 | 5000/23507 (21.27%) | Loss: 0.9369\n",
      "00:03:35 | Epoch: 9/100 | 6000/23507 (25.52%) | Loss: 0.9369\n",
      "00:04:10 | Epoch: 9/100 | 7000/23507 (29.78%) | Loss: 0.8983\n",
      "00:04:46 | Epoch: 9/100 | 8000/23507 (34.03%) | Loss: 0.9187\n",
      "00:05:22 | Epoch: 9/100 | 9000/23507 (38.29%) | Loss: 0.9252\n",
      "00:05:58 | Epoch: 9/100 | 10000/23507 (42.54%) | Loss: 0.9315\n",
      "00:06:35 | Epoch: 9/100 | 11000/23507 (46.79%) | Loss: 0.9139\n",
      "00:07:11 | Epoch: 9/100 | 12000/23507 (51.05%) | Loss: 0.9297\n",
      "00:07:47 | Epoch: 9/100 | 13000/23507 (55.30%) | Loss: 0.9310\n",
      "00:08:23 | Epoch: 9/100 | 14000/23507 (59.56%) | Loss: 0.9421\n",
      "00:08:59 | Epoch: 9/100 | 15000/23507 (63.81%) | Loss: 0.9513\n",
      "00:09:36 | Epoch: 9/100 | 16000/23507 (68.06%) | Loss: 0.9276\n",
      "00:10:12 | Epoch: 9/100 | 17000/23507 (72.32%) | Loss: 0.8958\n",
      "00:10:48 | Epoch: 9/100 | 18000/23507 (76.57%) | Loss: 0.9266\n",
      "00:11:24 | Epoch: 9/100 | 19000/23507 (80.83%) | Loss: 0.9156\n",
      "00:12:00 | Epoch: 9/100 | 20000/23507 (85.08%) | Loss: 0.9109\n",
      "00:12:36 | Epoch: 9/100 | 21000/23507 (89.34%) | Loss: 0.8940\n",
      "00:13:13 | Epoch: 9/100 | 22000/23507 (93.59%) | Loss: 0.9076\n",
      "00:13:49 | Epoch: 9/100 | 23000/23507 (97.84%) | Loss: 0.9348\n",
      "Epoch completed in 14.1 min\n",
      "Loss: 0.9233 | Genotype loss: 0.7202 | Phenotype loss: 0.2030\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8630 | Genotype loss: 0.6651 | Phenotype loss: 0.1980\n",
      "Phenotype accuracy: 91.71% | Phenotype isolate accuracy: 86.42%\n",
      "Genotype accuracy: 82.74% | Genotype isolate accuracy: 74.67%\n",
      "Elapsed time: 03:36:09\n",
      "Epoch 10/100\n",
      "00:00:35 | Epoch: 10/100 | 1000/23507 (4.25%) | Loss: 0.9133\n",
      "00:01:10 | Epoch: 10/100 | 2000/23507 (8.51%) | Loss: 0.9190\n",
      "00:01:45 | Epoch: 10/100 | 3000/23507 (12.76%) | Loss: 0.9175\n",
      "00:02:19 | Epoch: 10/100 | 4000/23507 (17.02%) | Loss: 0.9168\n",
      "00:02:54 | Epoch: 10/100 | 5000/23507 (21.27%) | Loss: 0.9139\n",
      "00:03:29 | Epoch: 10/100 | 6000/23507 (25.52%) | Loss: 0.9078\n",
      "00:04:03 | Epoch: 10/100 | 7000/23507 (29.78%) | Loss: 0.9040\n",
      "00:04:38 | Epoch: 10/100 | 8000/23507 (34.03%) | Loss: 0.9192\n",
      "00:05:13 | Epoch: 10/100 | 9000/23507 (38.29%) | Loss: 0.9185\n",
      "00:05:48 | Epoch: 10/100 | 10000/23507 (42.54%) | Loss: 0.9291\n",
      "00:06:22 | Epoch: 10/100 | 11000/23507 (46.79%) | Loss: 0.9270\n",
      "00:06:57 | Epoch: 10/100 | 12000/23507 (51.05%) | Loss: 0.9060\n",
      "00:07:32 | Epoch: 10/100 | 13000/23507 (55.30%) | Loss: 0.9203\n",
      "00:08:08 | Epoch: 10/100 | 14000/23507 (59.56%) | Loss: 0.8884\n",
      "00:08:43 | Epoch: 10/100 | 15000/23507 (63.81%) | Loss: 0.9191\n",
      "00:09:18 | Epoch: 10/100 | 16000/23507 (68.06%) | Loss: 0.9158\n",
      "00:09:53 | Epoch: 10/100 | 17000/23507 (72.32%) | Loss: 0.8865\n",
      "00:10:28 | Epoch: 10/100 | 18000/23507 (76.57%) | Loss: 0.9094\n",
      "00:11:03 | Epoch: 10/100 | 19000/23507 (80.83%) | Loss: 0.9002\n",
      "00:11:38 | Epoch: 10/100 | 20000/23507 (85.08%) | Loss: 0.9144\n",
      "00:12:13 | Epoch: 10/100 | 21000/23507 (89.34%) | Loss: 0.9215\n",
      "00:12:48 | Epoch: 10/100 | 22000/23507 (93.59%) | Loss: 0.9237\n",
      "00:13:24 | Epoch: 10/100 | 23000/23507 (97.84%) | Loss: 0.9115\n",
      "Epoch completed in 13.7 min\n",
      "Loss: 0.9139 | Genotype loss: 0.7113 | Phenotype loss: 0.2026\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8544 | Genotype loss: 0.6568 | Phenotype loss: 0.1976\n",
      "Phenotype accuracy: 91.79% | Phenotype isolate accuracy: 86.56%\n",
      "Genotype accuracy: 82.87% | Genotype isolate accuracy: 74.97%\n",
      "Elapsed time: 04:00:09\n",
      "Epoch 11/100\n",
      "00:00:35 | Epoch: 11/100 | 1000/23507 (4.25%) | Loss: 0.9090\n",
      "00:01:10 | Epoch: 11/100 | 2000/23507 (8.51%) | Loss: 0.8978\n",
      "00:01:45 | Epoch: 11/100 | 3000/23507 (12.76%) | Loss: 0.9033\n",
      "00:02:20 | Epoch: 11/100 | 4000/23507 (17.02%) | Loss: 0.8792\n",
      "00:02:56 | Epoch: 11/100 | 5000/23507 (21.27%) | Loss: 0.9029\n",
      "00:03:31 | Epoch: 11/100 | 6000/23507 (25.52%) | Loss: 0.8900\n",
      "00:04:06 | Epoch: 11/100 | 7000/23507 (29.78%) | Loss: 0.8965\n",
      "00:04:41 | Epoch: 11/100 | 8000/23507 (34.03%) | Loss: 0.9101\n",
      "00:05:16 | Epoch: 11/100 | 9000/23507 (38.29%) | Loss: 0.8977\n",
      "00:05:52 | Epoch: 11/100 | 10000/23507 (42.54%) | Loss: 0.9238\n",
      "00:06:27 | Epoch: 11/100 | 11000/23507 (46.79%) | Loss: 0.8889\n",
      "00:07:02 | Epoch: 11/100 | 12000/23507 (51.05%) | Loss: 0.9111\n",
      "00:07:37 | Epoch: 11/100 | 13000/23507 (55.30%) | Loss: 0.8927\n",
      "00:08:12 | Epoch: 11/100 | 14000/23507 (59.56%) | Loss: 0.9087\n",
      "00:08:48 | Epoch: 11/100 | 15000/23507 (63.81%) | Loss: 0.8962\n",
      "00:09:23 | Epoch: 11/100 | 16000/23507 (68.06%) | Loss: 0.9159\n",
      "00:09:58 | Epoch: 11/100 | 17000/23507 (72.32%) | Loss: 0.8993\n",
      "00:10:33 | Epoch: 11/100 | 18000/23507 (76.57%) | Loss: 0.8963\n",
      "00:11:09 | Epoch: 11/100 | 19000/23507 (80.83%) | Loss: 0.9074\n",
      "00:11:44 | Epoch: 11/100 | 20000/23507 (85.08%) | Loss: 0.8953\n",
      "00:12:19 | Epoch: 11/100 | 21000/23507 (89.34%) | Loss: 0.9104\n",
      "00:12:54 | Epoch: 11/100 | 22000/23507 (93.59%) | Loss: 0.8904\n",
      "00:13:29 | Epoch: 11/100 | 23000/23507 (97.84%) | Loss: 0.8941\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.9005 | Genotype loss: 0.6981 | Phenotype loss: 0.2024\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8509 | Genotype loss: 0.6539 | Phenotype loss: 0.1970\n",
      "Phenotype accuracy: 91.77% | Phenotype isolate accuracy: 86.45%\n",
      "Genotype accuracy: 83.08% | Genotype isolate accuracy: 75.16%\n",
      "Elapsed time: 04:24:14\n",
      "Epoch 12/100\n",
      "00:00:35 | Epoch: 12/100 | 1000/23507 (4.25%) | Loss: 0.8934\n",
      "00:01:10 | Epoch: 12/100 | 2000/23507 (8.51%) | Loss: 0.8953\n",
      "00:01:45 | Epoch: 12/100 | 3000/23507 (12.76%) | Loss: 0.8838\n",
      "00:02:20 | Epoch: 12/100 | 4000/23507 (17.02%) | Loss: 0.9010\n",
      "00:02:56 | Epoch: 12/100 | 5000/23507 (21.27%) | Loss: 0.9154\n",
      "00:03:31 | Epoch: 12/100 | 6000/23507 (25.52%) | Loss: 0.8985\n",
      "00:04:06 | Epoch: 12/100 | 7000/23507 (29.78%) | Loss: 0.8824\n",
      "00:04:41 | Epoch: 12/100 | 8000/23507 (34.03%) | Loss: 0.9061\n",
      "00:05:16 | Epoch: 12/100 | 9000/23507 (38.29%) | Loss: 0.8842\n",
      "00:05:51 | Epoch: 12/100 | 10000/23507 (42.54%) | Loss: 0.8945\n",
      "00:06:27 | Epoch: 12/100 | 11000/23507 (46.79%) | Loss: 0.8853\n",
      "00:07:02 | Epoch: 12/100 | 12000/23507 (51.05%) | Loss: 0.8995\n",
      "00:07:37 | Epoch: 12/100 | 13000/23507 (55.30%) | Loss: 0.8889\n",
      "00:08:12 | Epoch: 12/100 | 14000/23507 (59.56%) | Loss: 0.9041\n",
      "00:08:47 | Epoch: 12/100 | 15000/23507 (63.81%) | Loss: 0.9010\n",
      "00:09:23 | Epoch: 12/100 | 16000/23507 (68.06%) | Loss: 0.8966\n",
      "00:09:58 | Epoch: 12/100 | 17000/23507 (72.32%) | Loss: 0.8983\n",
      "00:10:33 | Epoch: 12/100 | 18000/23507 (76.57%) | Loss: 0.8863\n",
      "00:11:08 | Epoch: 12/100 | 19000/23507 (80.83%) | Loss: 0.9064\n",
      "00:11:44 | Epoch: 12/100 | 20000/23507 (85.08%) | Loss: 0.9311\n",
      "00:12:19 | Epoch: 12/100 | 21000/23507 (89.34%) | Loss: 0.8921\n",
      "00:12:54 | Epoch: 12/100 | 22000/23507 (93.59%) | Loss: 0.8750\n",
      "00:13:29 | Epoch: 12/100 | 23000/23507 (97.84%) | Loss: 0.8859\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8956 | Genotype loss: 0.6946 | Phenotype loss: 0.2011\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8428 | Genotype loss: 0.6463 | Phenotype loss: 0.1964\n",
      "Phenotype accuracy: 91.81% | Phenotype isolate accuracy: 86.52%\n",
      "Genotype accuracy: 83.24% | Genotype isolate accuracy: 75.32%\n",
      "Elapsed time: 04:48:17\n",
      "Epoch 13/100\n",
      "00:00:35 | Epoch: 13/100 | 1000/23507 (4.25%) | Loss: 0.8889\n",
      "00:01:10 | Epoch: 13/100 | 2000/23507 (8.51%) | Loss: 0.8727\n",
      "00:01:46 | Epoch: 13/100 | 3000/23507 (12.76%) | Loss: 0.8674\n",
      "00:02:21 | Epoch: 13/100 | 4000/23507 (17.02%) | Loss: 0.8854\n",
      "00:02:56 | Epoch: 13/100 | 5000/23507 (21.27%) | Loss: 0.8810\n",
      "00:03:32 | Epoch: 13/100 | 6000/23507 (25.52%) | Loss: 0.8891\n",
      "00:04:07 | Epoch: 13/100 | 7000/23507 (29.78%) | Loss: 0.8933\n",
      "00:04:42 | Epoch: 13/100 | 8000/23507 (34.03%) | Loss: 0.8927\n",
      "00:05:18 | Epoch: 13/100 | 9000/23507 (38.29%) | Loss: 0.8968\n",
      "00:05:53 | Epoch: 13/100 | 10000/23507 (42.54%) | Loss: 0.9002\n",
      "00:06:28 | Epoch: 13/100 | 11000/23507 (46.79%) | Loss: 0.8938\n",
      "00:07:04 | Epoch: 13/100 | 12000/23507 (51.05%) | Loss: 0.8808\n",
      "00:07:39 | Epoch: 13/100 | 13000/23507 (55.30%) | Loss: 0.8906\n",
      "00:08:14 | Epoch: 13/100 | 14000/23507 (59.56%) | Loss: 0.8671\n",
      "00:08:50 | Epoch: 13/100 | 15000/23507 (63.81%) | Loss: 0.8819\n",
      "00:09:25 | Epoch: 13/100 | 16000/23507 (68.06%) | Loss: 0.8820\n",
      "00:10:00 | Epoch: 13/100 | 17000/23507 (72.32%) | Loss: 0.9104\n",
      "00:10:35 | Epoch: 13/100 | 18000/23507 (76.57%) | Loss: 0.8881\n",
      "00:11:11 | Epoch: 13/100 | 19000/23507 (80.83%) | Loss: 0.8984\n",
      "00:11:46 | Epoch: 13/100 | 20000/23507 (85.08%) | Loss: 0.8962\n",
      "00:12:21 | Epoch: 13/100 | 21000/23507 (89.34%) | Loss: 0.8722\n",
      "00:12:56 | Epoch: 13/100 | 22000/23507 (93.59%) | Loss: 0.8705\n",
      "00:13:31 | Epoch: 13/100 | 23000/23507 (97.84%) | Loss: 0.8817\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8861 | Genotype loss: 0.6851 | Phenotype loss: 0.2011\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8367 | Genotype loss: 0.6405 | Phenotype loss: 0.1962\n",
      "Phenotype accuracy: 91.79% | Phenotype isolate accuracy: 86.47%\n",
      "Genotype accuracy: 83.40% | Genotype isolate accuracy: 75.39%\n",
      "Elapsed time: 05:12:21\n",
      "Epoch 14/100\n",
      "00:00:35 | Epoch: 14/100 | 1000/23507 (4.25%) | Loss: 0.8978\n",
      "00:01:10 | Epoch: 14/100 | 2000/23507 (8.51%) | Loss: 0.8936\n",
      "00:01:45 | Epoch: 14/100 | 3000/23507 (12.76%) | Loss: 0.8819\n",
      "00:02:20 | Epoch: 14/100 | 4000/23507 (17.02%) | Loss: 0.8698\n",
      "00:02:55 | Epoch: 14/100 | 5000/23507 (21.27%) | Loss: 0.8824\n",
      "00:03:30 | Epoch: 14/100 | 6000/23507 (25.52%) | Loss: 0.8863\n",
      "00:04:06 | Epoch: 14/100 | 7000/23507 (29.78%) | Loss: 0.8839\n",
      "00:04:41 | Epoch: 14/100 | 8000/23507 (34.03%) | Loss: 0.8644\n",
      "00:05:16 | Epoch: 14/100 | 9000/23507 (38.29%) | Loss: 0.8675\n",
      "00:05:51 | Epoch: 14/100 | 10000/23507 (42.54%) | Loss: 0.8832\n",
      "00:06:26 | Epoch: 14/100 | 11000/23507 (46.79%) | Loss: 0.8655\n",
      "00:07:01 | Epoch: 14/100 | 12000/23507 (51.05%) | Loss: 0.9034\n",
      "00:07:36 | Epoch: 14/100 | 13000/23507 (55.30%) | Loss: 0.8908\n",
      "00:08:11 | Epoch: 14/100 | 14000/23507 (59.56%) | Loss: 0.8909\n",
      "00:08:46 | Epoch: 14/100 | 15000/23507 (63.81%) | Loss: 0.8894\n",
      "00:09:21 | Epoch: 14/100 | 16000/23507 (68.06%) | Loss: 0.8745\n",
      "00:09:56 | Epoch: 14/100 | 17000/23507 (72.32%) | Loss: 0.8802\n",
      "00:10:31 | Epoch: 14/100 | 18000/23507 (76.57%) | Loss: 0.8731\n",
      "00:11:06 | Epoch: 14/100 | 19000/23507 (80.83%) | Loss: 0.8783\n",
      "00:11:42 | Epoch: 14/100 | 20000/23507 (85.08%) | Loss: 0.8823\n",
      "00:12:17 | Epoch: 14/100 | 21000/23507 (89.34%) | Loss: 0.8673\n",
      "00:12:52 | Epoch: 14/100 | 22000/23507 (93.59%) | Loss: 0.9016\n",
      "00:13:27 | Epoch: 14/100 | 23000/23507 (97.84%) | Loss: 0.8771\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8826 | Genotype loss: 0.6816 | Phenotype loss: 0.2010\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8271 | Genotype loss: 0.6318 | Phenotype loss: 0.1952\n",
      "Phenotype accuracy: 91.75% | Phenotype isolate accuracy: 86.32%\n",
      "Genotype accuracy: 83.53% | Genotype isolate accuracy: 75.63%\n",
      "Elapsed time: 05:36:21\n",
      "Epoch 15/100\n",
      "00:00:35 | Epoch: 15/100 | 1000/23507 (4.25%) | Loss: 0.8693\n",
      "00:01:10 | Epoch: 15/100 | 2000/23507 (8.51%) | Loss: 0.8510\n",
      "00:01:45 | Epoch: 15/100 | 3000/23507 (12.76%) | Loss: 0.8629\n",
      "00:02:21 | Epoch: 15/100 | 4000/23507 (17.02%) | Loss: 0.8894\n",
      "00:02:56 | Epoch: 15/100 | 5000/23507 (21.27%) | Loss: 0.8650\n",
      "00:03:31 | Epoch: 15/100 | 6000/23507 (25.52%) | Loss: 0.8603\n",
      "00:04:06 | Epoch: 15/100 | 7000/23507 (29.78%) | Loss: 0.8852\n",
      "00:04:42 | Epoch: 15/100 | 8000/23507 (34.03%) | Loss: 0.8952\n",
      "00:05:17 | Epoch: 15/100 | 9000/23507 (38.29%) | Loss: 0.8609\n",
      "00:05:52 | Epoch: 15/100 | 10000/23507 (42.54%) | Loss: 0.8540\n",
      "00:06:27 | Epoch: 15/100 | 11000/23507 (46.79%) | Loss: 0.8716\n",
      "00:07:03 | Epoch: 15/100 | 12000/23507 (51.05%) | Loss: 0.8826\n",
      "00:07:38 | Epoch: 15/100 | 13000/23507 (55.30%) | Loss: 0.8615\n",
      "00:08:13 | Epoch: 15/100 | 14000/23507 (59.56%) | Loss: 0.8593\n",
      "00:08:48 | Epoch: 15/100 | 15000/23507 (63.81%) | Loss: 0.8766\n",
      "00:09:23 | Epoch: 15/100 | 16000/23507 (68.06%) | Loss: 0.8857\n",
      "00:09:57 | Epoch: 15/100 | 17000/23507 (72.32%) | Loss: 0.8708\n",
      "00:10:32 | Epoch: 15/100 | 18000/23507 (76.57%) | Loss: 0.8542\n",
      "00:11:07 | Epoch: 15/100 | 19000/23507 (80.83%) | Loss: 0.8810\n",
      "00:11:42 | Epoch: 15/100 | 20000/23507 (85.08%) | Loss: 0.8729\n",
      "00:12:17 | Epoch: 15/100 | 21000/23507 (89.34%) | Loss: 0.8660\n",
      "00:12:52 | Epoch: 15/100 | 22000/23507 (93.59%) | Loss: 0.8655\n",
      "00:13:27 | Epoch: 15/100 | 23000/23507 (97.84%) | Loss: 0.8517\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8697 | Genotype loss: 0.6688 | Phenotype loss: 0.2010\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8228 | Genotype loss: 0.6279 | Phenotype loss: 0.1949\n",
      "Phenotype accuracy: 91.83% | Phenotype isolate accuracy: 86.51%\n",
      "Genotype accuracy: 83.58% | Genotype isolate accuracy: 75.72%\n",
      "Elapsed time: 06:00:19\n",
      "Epoch 16/100\n",
      "00:00:35 | Epoch: 16/100 | 1000/23507 (4.25%) | Loss: 0.8603\n",
      "00:01:10 | Epoch: 16/100 | 2000/23507 (8.51%) | Loss: 0.8635\n",
      "00:01:45 | Epoch: 16/100 | 3000/23507 (12.76%) | Loss: 0.8685\n",
      "00:02:20 | Epoch: 16/100 | 4000/23507 (17.02%) | Loss: 0.8880\n",
      "00:02:55 | Epoch: 16/100 | 5000/23507 (21.27%) | Loss: 0.8574\n",
      "00:03:31 | Epoch: 16/100 | 6000/23507 (25.52%) | Loss: 0.8636\n",
      "00:04:06 | Epoch: 16/100 | 7000/23507 (29.78%) | Loss: 0.8670\n",
      "00:04:41 | Epoch: 16/100 | 8000/23507 (34.03%) | Loss: 0.8755\n",
      "00:05:16 | Epoch: 16/100 | 9000/23507 (38.29%) | Loss: 0.8697\n",
      "00:05:51 | Epoch: 16/100 | 10000/23507 (42.54%) | Loss: 0.8715\n",
      "00:06:27 | Epoch: 16/100 | 11000/23507 (46.79%) | Loss: 0.8629\n",
      "00:07:02 | Epoch: 16/100 | 12000/23507 (51.05%) | Loss: 0.8804\n",
      "00:07:37 | Epoch: 16/100 | 13000/23507 (55.30%) | Loss: 0.8617\n",
      "00:08:13 | Epoch: 16/100 | 14000/23507 (59.56%) | Loss: 0.8665\n",
      "00:08:48 | Epoch: 16/100 | 15000/23507 (63.81%) | Loss: 0.8551\n",
      "00:09:23 | Epoch: 16/100 | 16000/23507 (68.06%) | Loss: 0.8929\n",
      "00:09:59 | Epoch: 16/100 | 17000/23507 (72.32%) | Loss: 0.8581\n",
      "00:10:34 | Epoch: 16/100 | 18000/23507 (76.57%) | Loss: 0.8516\n",
      "00:11:09 | Epoch: 16/100 | 19000/23507 (80.83%) | Loss: 0.8693\n",
      "00:11:44 | Epoch: 16/100 | 20000/23507 (85.08%) | Loss: 0.8903\n",
      "00:12:20 | Epoch: 16/100 | 21000/23507 (89.34%) | Loss: 0.8651\n",
      "00:12:55 | Epoch: 16/100 | 22000/23507 (93.59%) | Loss: 0.8593\n",
      "00:13:30 | Epoch: 16/100 | 23000/23507 (97.84%) | Loss: 0.8663\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8675 | Genotype loss: 0.6679 | Phenotype loss: 0.1996\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8181 | Genotype loss: 0.6229 | Phenotype loss: 0.1951\n",
      "Phenotype accuracy: 91.83% | Phenotype isolate accuracy: 86.62%\n",
      "Genotype accuracy: 83.78% | Genotype isolate accuracy: 76.00%\n",
      "Elapsed time: 06:24:21\n",
      "Epoch 17/100\n",
      "00:00:35 | Epoch: 17/100 | 1000/23507 (4.25%) | Loss: 0.8522\n",
      "00:01:10 | Epoch: 17/100 | 2000/23507 (8.51%) | Loss: 0.8654\n",
      "00:01:45 | Epoch: 17/100 | 3000/23507 (12.76%) | Loss: 0.8671\n",
      "00:02:21 | Epoch: 17/100 | 4000/23507 (17.02%) | Loss: 0.8670\n",
      "00:02:56 | Epoch: 17/100 | 5000/23507 (21.27%) | Loss: 0.8684\n",
      "00:03:31 | Epoch: 17/100 | 6000/23507 (25.52%) | Loss: 0.8468\n",
      "00:04:06 | Epoch: 17/100 | 7000/23507 (29.78%) | Loss: 0.8523\n",
      "00:04:42 | Epoch: 17/100 | 8000/23507 (34.03%) | Loss: 0.8684\n",
      "00:05:17 | Epoch: 17/100 | 9000/23507 (38.29%) | Loss: 0.8756\n",
      "00:05:52 | Epoch: 17/100 | 10000/23507 (42.54%) | Loss: 0.8435\n",
      "00:06:27 | Epoch: 17/100 | 11000/23507 (46.79%) | Loss: 0.8624\n",
      "00:07:03 | Epoch: 17/100 | 12000/23507 (51.05%) | Loss: 0.8740\n",
      "00:07:38 | Epoch: 17/100 | 13000/23507 (55.30%) | Loss: 0.8629\n",
      "00:08:13 | Epoch: 17/100 | 14000/23507 (59.56%) | Loss: 0.8552\n",
      "00:08:48 | Epoch: 17/100 | 15000/23507 (63.81%) | Loss: 0.8618\n",
      "00:09:24 | Epoch: 17/100 | 16000/23507 (68.06%) | Loss: 0.8596\n",
      "00:09:59 | Epoch: 17/100 | 17000/23507 (72.32%) | Loss: 0.8778\n",
      "00:10:34 | Epoch: 17/100 | 18000/23507 (76.57%) | Loss: 0.8642\n",
      "00:11:10 | Epoch: 17/100 | 19000/23507 (80.83%) | Loss: 0.8613\n",
      "00:11:45 | Epoch: 17/100 | 20000/23507 (85.08%) | Loss: 0.8519\n",
      "00:12:21 | Epoch: 17/100 | 21000/23507 (89.34%) | Loss: 0.8694\n",
      "00:12:56 | Epoch: 17/100 | 22000/23507 (93.59%) | Loss: 0.8553\n",
      "00:13:31 | Epoch: 17/100 | 23000/23507 (97.84%) | Loss: 0.8678\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8625 | Genotype loss: 0.6632 | Phenotype loss: 0.1994\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8104 | Genotype loss: 0.6148 | Phenotype loss: 0.1957\n",
      "Phenotype accuracy: 91.78% | Phenotype isolate accuracy: 86.43%\n",
      "Genotype accuracy: 83.91% | Genotype isolate accuracy: 76.11%\n",
      "Elapsed time: 06:48:26\n",
      "Epoch 18/100\n",
      "00:00:35 | Epoch: 18/100 | 1000/23507 (4.25%) | Loss: 0.8689\n",
      "00:01:10 | Epoch: 18/100 | 2000/23507 (8.51%) | Loss: 0.8534\n",
      "00:01:46 | Epoch: 18/100 | 3000/23507 (12.76%) | Loss: 0.8716\n",
      "00:02:21 | Epoch: 18/100 | 4000/23507 (17.02%) | Loss: 0.8517\n",
      "00:02:56 | Epoch: 18/100 | 5000/23507 (21.27%) | Loss: 0.8823\n",
      "00:03:32 | Epoch: 18/100 | 6000/23507 (25.52%) | Loss: 0.8610\n",
      "00:04:07 | Epoch: 18/100 | 7000/23507 (29.78%) | Loss: 0.8702\n",
      "00:04:42 | Epoch: 18/100 | 8000/23507 (34.03%) | Loss: 0.8747\n",
      "00:05:17 | Epoch: 18/100 | 9000/23507 (38.29%) | Loss: 0.8729\n",
      "00:05:53 | Epoch: 18/100 | 10000/23507 (42.54%) | Loss: 0.8284\n",
      "00:06:28 | Epoch: 18/100 | 11000/23507 (46.79%) | Loss: 0.8734\n",
      "00:07:03 | Epoch: 18/100 | 12000/23507 (51.05%) | Loss: 0.8507\n",
      "00:07:39 | Epoch: 18/100 | 13000/23507 (55.30%) | Loss: 0.8571\n",
      "00:08:14 | Epoch: 18/100 | 14000/23507 (59.56%) | Loss: 0.8650\n",
      "00:08:49 | Epoch: 18/100 | 15000/23507 (63.81%) | Loss: 0.8381\n",
      "00:09:24 | Epoch: 18/100 | 16000/23507 (68.06%) | Loss: 0.8712\n",
      "00:10:00 | Epoch: 18/100 | 17000/23507 (72.32%) | Loss: 0.8435\n",
      "00:10:35 | Epoch: 18/100 | 18000/23507 (76.57%) | Loss: 0.8413\n",
      "00:11:10 | Epoch: 18/100 | 19000/23507 (80.83%) | Loss: 0.8612\n",
      "00:11:45 | Epoch: 18/100 | 20000/23507 (85.08%) | Loss: 0.8483\n",
      "00:12:21 | Epoch: 18/100 | 21000/23507 (89.34%) | Loss: 0.8496\n",
      "00:12:56 | Epoch: 18/100 | 22000/23507 (93.59%) | Loss: 0.8597\n",
      "00:13:31 | Epoch: 18/100 | 23000/23507 (97.84%) | Loss: 0.8581\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8587 | Genotype loss: 0.6599 | Phenotype loss: 0.1988\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8114 | Genotype loss: 0.6160 | Phenotype loss: 0.1954\n",
      "Phenotype accuracy: 91.75% | Phenotype isolate accuracy: 86.51%\n",
      "Genotype accuracy: 83.97% | Genotype isolate accuracy: 76.19%\n",
      "Elapsed time: 07:12:33\n",
      "Epoch 19/100\n",
      "00:00:35 | Epoch: 19/100 | 1000/23507 (4.25%) | Loss: 0.8599\n",
      "00:01:10 | Epoch: 19/100 | 2000/23507 (8.51%) | Loss: 0.8446\n",
      "00:01:45 | Epoch: 19/100 | 3000/23507 (12.76%) | Loss: 0.8376\n",
      "00:02:20 | Epoch: 19/100 | 4000/23507 (17.02%) | Loss: 0.8559\n",
      "00:02:55 | Epoch: 19/100 | 5000/23507 (21.27%) | Loss: 0.8432\n",
      "00:03:31 | Epoch: 19/100 | 6000/23507 (25.52%) | Loss: 0.8581\n",
      "00:04:06 | Epoch: 19/100 | 7000/23507 (29.78%) | Loss: 0.8761\n",
      "00:04:41 | Epoch: 19/100 | 8000/23507 (34.03%) | Loss: 0.8424\n",
      "00:05:16 | Epoch: 19/100 | 9000/23507 (38.29%) | Loss: 0.8647\n",
      "00:05:51 | Epoch: 19/100 | 10000/23507 (42.54%) | Loss: 0.8571\n",
      "00:06:27 | Epoch: 19/100 | 11000/23507 (46.79%) | Loss: 0.8633\n",
      "00:07:02 | Epoch: 19/100 | 12000/23507 (51.05%) | Loss: 0.8545\n",
      "00:07:37 | Epoch: 19/100 | 13000/23507 (55.30%) | Loss: 0.8520\n",
      "00:08:12 | Epoch: 19/100 | 14000/23507 (59.56%) | Loss: 0.8586\n",
      "00:08:47 | Epoch: 19/100 | 15000/23507 (63.81%) | Loss: 0.8574\n",
      "00:09:23 | Epoch: 19/100 | 16000/23507 (68.06%) | Loss: 0.8739\n",
      "00:09:58 | Epoch: 19/100 | 17000/23507 (72.32%) | Loss: 0.8541\n",
      "00:10:33 | Epoch: 19/100 | 18000/23507 (76.57%) | Loss: 0.8220\n",
      "00:11:08 | Epoch: 19/100 | 19000/23507 (80.83%) | Loss: 0.8441\n",
      "00:11:43 | Epoch: 19/100 | 20000/23507 (85.08%) | Loss: 0.8398\n",
      "00:12:19 | Epoch: 19/100 | 21000/23507 (89.34%) | Loss: 0.8507\n",
      "00:12:54 | Epoch: 19/100 | 22000/23507 (93.59%) | Loss: 0.8622\n",
      "00:13:29 | Epoch: 19/100 | 23000/23507 (97.84%) | Loss: 0.8421\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8528 | Genotype loss: 0.6538 | Phenotype loss: 0.1990\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8080 | Genotype loss: 0.6146 | Phenotype loss: 0.1934\n",
      "Phenotype accuracy: 91.83% | Phenotype isolate accuracy: 86.58%\n",
      "Genotype accuracy: 84.01% | Genotype isolate accuracy: 76.31%\n",
      "Elapsed time: 07:36:40\n",
      "Epoch 20/100\n",
      "00:00:35 | Epoch: 20/100 | 1000/23507 (4.25%) | Loss: 0.8423\n",
      "00:01:10 | Epoch: 20/100 | 2000/23507 (8.51%) | Loss: 0.8443\n",
      "00:01:45 | Epoch: 20/100 | 3000/23507 (12.76%) | Loss: 0.8379\n",
      "00:02:20 | Epoch: 20/100 | 4000/23507 (17.02%) | Loss: 0.8559\n",
      "00:02:55 | Epoch: 20/100 | 5000/23507 (21.27%) | Loss: 0.8418\n",
      "00:03:30 | Epoch: 20/100 | 6000/23507 (25.52%) | Loss: 0.8633\n",
      "00:04:05 | Epoch: 20/100 | 7000/23507 (29.78%) | Loss: 0.8464\n",
      "00:04:40 | Epoch: 20/100 | 8000/23507 (34.03%) | Loss: 0.8529\n",
      "00:05:15 | Epoch: 20/100 | 9000/23507 (38.29%) | Loss: 0.8189\n",
      "00:05:50 | Epoch: 20/100 | 10000/23507 (42.54%) | Loss: 0.8532\n",
      "00:06:25 | Epoch: 20/100 | 11000/23507 (46.79%) | Loss: 0.8507\n",
      "00:07:00 | Epoch: 20/100 | 12000/23507 (51.05%) | Loss: 0.8466\n",
      "00:07:36 | Epoch: 20/100 | 13000/23507 (55.30%) | Loss: 0.8362\n",
      "00:08:11 | Epoch: 20/100 | 14000/23507 (59.56%) | Loss: 0.8395\n",
      "00:08:46 | Epoch: 20/100 | 15000/23507 (63.81%) | Loss: 0.8537\n",
      "00:09:21 | Epoch: 20/100 | 16000/23507 (68.06%) | Loss: 0.8407\n",
      "00:09:56 | Epoch: 20/100 | 17000/23507 (72.32%) | Loss: 0.8453\n",
      "00:10:31 | Epoch: 20/100 | 18000/23507 (76.57%) | Loss: 0.8700\n",
      "00:11:06 | Epoch: 20/100 | 19000/23507 (80.83%) | Loss: 0.8327\n",
      "00:11:42 | Epoch: 20/100 | 20000/23507 (85.08%) | Loss: 0.8455\n",
      "00:12:17 | Epoch: 20/100 | 21000/23507 (89.34%) | Loss: 0.8608\n",
      "00:12:52 | Epoch: 20/100 | 22000/23507 (93.59%) | Loss: 0.8507\n",
      "00:13:27 | Epoch: 20/100 | 23000/23507 (97.84%) | Loss: 0.8379\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8462 | Genotype loss: 0.6481 | Phenotype loss: 0.1981\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8055 | Genotype loss: 0.6119 | Phenotype loss: 0.1936\n",
      "Phenotype accuracy: 91.86% | Phenotype isolate accuracy: 86.64%\n",
      "Genotype accuracy: 84.00% | Genotype isolate accuracy: 75.94%\n",
      "Elapsed time: 08:00:41\n",
      "Epoch 21/100\n",
      "00:00:35 | Epoch: 21/100 | 1000/23507 (4.25%) | Loss: 0.8461\n",
      "00:01:10 | Epoch: 21/100 | 2000/23507 (8.51%) | Loss: 0.8319\n",
      "00:01:45 | Epoch: 21/100 | 3000/23507 (12.76%) | Loss: 0.8553\n",
      "00:02:20 | Epoch: 21/100 | 4000/23507 (17.02%) | Loss: 0.8434\n",
      "00:02:55 | Epoch: 21/100 | 5000/23507 (21.27%) | Loss: 0.8424\n",
      "00:03:30 | Epoch: 21/100 | 6000/23507 (25.52%) | Loss: 0.8373\n",
      "00:04:06 | Epoch: 21/100 | 7000/23507 (29.78%) | Loss: 0.8498\n",
      "00:04:41 | Epoch: 21/100 | 8000/23507 (34.03%) | Loss: 0.8585\n",
      "00:05:16 | Epoch: 21/100 | 9000/23507 (38.29%) | Loss: 0.8489\n",
      "00:05:51 | Epoch: 21/100 | 10000/23507 (42.54%) | Loss: 0.8398\n",
      "00:06:26 | Epoch: 21/100 | 11000/23507 (46.79%) | Loss: 0.8537\n",
      "00:07:02 | Epoch: 21/100 | 12000/23507 (51.05%) | Loss: 0.8461\n",
      "00:07:37 | Epoch: 21/100 | 13000/23507 (55.30%) | Loss: 0.8547\n",
      "00:08:12 | Epoch: 21/100 | 14000/23507 (59.56%) | Loss: 0.8469\n",
      "00:08:47 | Epoch: 21/100 | 15000/23507 (63.81%) | Loss: 0.8540\n",
      "00:09:22 | Epoch: 21/100 | 16000/23507 (68.06%) | Loss: 0.8403\n",
      "00:09:58 | Epoch: 21/100 | 17000/23507 (72.32%) | Loss: 0.8322\n",
      "00:10:33 | Epoch: 21/100 | 18000/23507 (76.57%) | Loss: 0.8632\n",
      "00:11:08 | Epoch: 21/100 | 19000/23507 (80.83%) | Loss: 0.8584\n",
      "00:11:43 | Epoch: 21/100 | 20000/23507 (85.08%) | Loss: 0.8425\n",
      "00:12:18 | Epoch: 21/100 | 21000/23507 (89.34%) | Loss: 0.8479\n",
      "00:12:53 | Epoch: 21/100 | 22000/23507 (93.59%) | Loss: 0.8438\n",
      "00:13:28 | Epoch: 21/100 | 23000/23507 (97.84%) | Loss: 0.8455\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8472 | Genotype loss: 0.6486 | Phenotype loss: 0.1986\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.8087 | Genotype loss: 0.6142 | Phenotype loss: 0.1945\n",
      "Phenotype accuracy: 91.82% | Phenotype isolate accuracy: 86.49%\n",
      "Genotype accuracy: 84.10% | Genotype isolate accuracy: 76.32%\n",
      "Elapsed time: 08:24:51\n",
      "Epoch 22/100\n",
      "00:00:35 | Epoch: 22/100 | 1000/23507 (4.25%) | Loss: 0.8487\n",
      "00:01:10 | Epoch: 22/100 | 2000/23507 (8.51%) | Loss: 0.8480\n",
      "00:01:46 | Epoch: 22/100 | 3000/23507 (12.76%) | Loss: 0.8272\n",
      "00:02:21 | Epoch: 22/100 | 4000/23507 (17.02%) | Loss: 0.8304\n",
      "00:02:57 | Epoch: 22/100 | 5000/23507 (21.27%) | Loss: 0.8345\n",
      "00:03:32 | Epoch: 22/100 | 6000/23507 (25.52%) | Loss: 0.8388\n",
      "00:04:07 | Epoch: 22/100 | 7000/23507 (29.78%) | Loss: 0.8513\n",
      "00:04:42 | Epoch: 22/100 | 8000/23507 (34.03%) | Loss: 0.8205\n",
      "00:05:18 | Epoch: 22/100 | 9000/23507 (38.29%) | Loss: 0.8418\n",
      "00:05:53 | Epoch: 22/100 | 10000/23507 (42.54%) | Loss: 0.8325\n",
      "00:06:28 | Epoch: 22/100 | 11000/23507 (46.79%) | Loss: 0.8643\n",
      "00:07:03 | Epoch: 22/100 | 12000/23507 (51.05%) | Loss: 0.8476\n",
      "00:07:38 | Epoch: 22/100 | 13000/23507 (55.30%) | Loss: 0.8385\n",
      "00:08:13 | Epoch: 22/100 | 14000/23507 (59.56%) | Loss: 0.8461\n",
      "00:08:48 | Epoch: 22/100 | 15000/23507 (63.81%) | Loss: 0.8414\n",
      "00:09:24 | Epoch: 22/100 | 16000/23507 (68.06%) | Loss: 0.8537\n",
      "00:09:59 | Epoch: 22/100 | 17000/23507 (72.32%) | Loss: 0.8570\n",
      "00:10:34 | Epoch: 22/100 | 18000/23507 (76.57%) | Loss: 0.8404\n",
      "00:11:09 | Epoch: 22/100 | 19000/23507 (80.83%) | Loss: 0.8403\n",
      "00:11:44 | Epoch: 22/100 | 20000/23507 (85.08%) | Loss: 0.8415\n",
      "00:12:19 | Epoch: 22/100 | 21000/23507 (89.34%) | Loss: 0.8493\n",
      "00:12:55 | Epoch: 22/100 | 22000/23507 (93.59%) | Loss: 0.8598\n",
      "00:13:30 | Epoch: 22/100 | 23000/23507 (97.84%) | Loss: 0.8701\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8444 | Genotype loss: 0.6468 | Phenotype loss: 0.1977\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7984 | Genotype loss: 0.6051 | Phenotype loss: 0.1933\n",
      "Phenotype accuracy: 91.84% | Phenotype isolate accuracy: 86.67%\n",
      "Genotype accuracy: 84.09% | Genotype isolate accuracy: 76.19%\n",
      "Elapsed time: 08:48:51\n",
      "Epoch 23/100\n",
      "00:00:35 | Epoch: 23/100 | 1000/23507 (4.25%) | Loss: 0.8538\n",
      "00:01:10 | Epoch: 23/100 | 2000/23507 (8.51%) | Loss: 0.8516\n",
      "00:01:45 | Epoch: 23/100 | 3000/23507 (12.76%) | Loss: 0.8409\n",
      "00:02:20 | Epoch: 23/100 | 4000/23507 (17.02%) | Loss: 0.8434\n",
      "00:02:55 | Epoch: 23/100 | 5000/23507 (21.27%) | Loss: 0.8400\n",
      "00:03:30 | Epoch: 23/100 | 6000/23507 (25.52%) | Loss: 0.8289\n",
      "00:04:06 | Epoch: 23/100 | 7000/23507 (29.78%) | Loss: 0.8421\n",
      "00:04:41 | Epoch: 23/100 | 8000/23507 (34.03%) | Loss: 0.8449\n",
      "00:05:16 | Epoch: 23/100 | 9000/23507 (38.29%) | Loss: 0.8194\n",
      "00:05:52 | Epoch: 23/100 | 10000/23507 (42.54%) | Loss: 0.8241\n",
      "00:06:27 | Epoch: 23/100 | 11000/23507 (46.79%) | Loss: 0.8401\n",
      "00:07:02 | Epoch: 23/100 | 12000/23507 (51.05%) | Loss: 0.8237\n",
      "00:07:37 | Epoch: 23/100 | 13000/23507 (55.30%) | Loss: 0.8245\n",
      "00:08:12 | Epoch: 23/100 | 14000/23507 (59.56%) | Loss: 0.8469\n",
      "00:08:47 | Epoch: 23/100 | 15000/23507 (63.81%) | Loss: 0.8468\n",
      "00:09:23 | Epoch: 23/100 | 16000/23507 (68.06%) | Loss: 0.8487\n",
      "00:09:58 | Epoch: 23/100 | 17000/23507 (72.32%) | Loss: 0.8263\n",
      "00:10:33 | Epoch: 23/100 | 18000/23507 (76.57%) | Loss: 0.8251\n",
      "00:11:08 | Epoch: 23/100 | 19000/23507 (80.83%) | Loss: 0.8454\n",
      "00:11:44 | Epoch: 23/100 | 20000/23507 (85.08%) | Loss: 0.8516\n",
      "00:12:19 | Epoch: 23/100 | 21000/23507 (89.34%) | Loss: 0.8546\n",
      "00:12:54 | Epoch: 23/100 | 22000/23507 (93.59%) | Loss: 0.8404\n",
      "00:13:30 | Epoch: 23/100 | 23000/23507 (97.84%) | Loss: 0.8526\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8399 | Genotype loss: 0.6416 | Phenotype loss: 0.1983\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7956 | Genotype loss: 0.6024 | Phenotype loss: 0.1931\n",
      "Phenotype accuracy: 91.89% | Phenotype isolate accuracy: 86.63%\n",
      "Genotype accuracy: 84.34% | Genotype isolate accuracy: 76.72%\n",
      "Elapsed time: 09:12:56\n",
      "Epoch 24/100\n",
      "00:00:35 | Epoch: 24/100 | 1000/23507 (4.25%) | Loss: 0.8274\n",
      "00:01:10 | Epoch: 24/100 | 2000/23507 (8.51%) | Loss: 0.8121\n",
      "00:01:46 | Epoch: 24/100 | 3000/23507 (12.76%) | Loss: 0.8458\n",
      "00:02:21 | Epoch: 24/100 | 4000/23507 (17.02%) | Loss: 0.8446\n",
      "00:02:56 | Epoch: 24/100 | 5000/23507 (21.27%) | Loss: 0.8396\n",
      "00:03:31 | Epoch: 24/100 | 6000/23507 (25.52%) | Loss: 0.8382\n",
      "00:04:07 | Epoch: 24/100 | 7000/23507 (29.78%) | Loss: 0.8172\n",
      "00:04:42 | Epoch: 24/100 | 8000/23507 (34.03%) | Loss: 0.8371\n",
      "00:05:17 | Epoch: 24/100 | 9000/23507 (38.29%) | Loss: 0.8436\n",
      "00:05:53 | Epoch: 24/100 | 10000/23507 (42.54%) | Loss: 0.8472\n",
      "00:06:28 | Epoch: 24/100 | 11000/23507 (46.79%) | Loss: 0.8563\n",
      "00:07:04 | Epoch: 24/100 | 12000/23507 (51.05%) | Loss: 0.8404\n",
      "00:07:39 | Epoch: 24/100 | 13000/23507 (55.30%) | Loss: 0.8380\n",
      "00:08:14 | Epoch: 24/100 | 14000/23507 (59.56%) | Loss: 0.8363\n",
      "00:08:49 | Epoch: 24/100 | 15000/23507 (63.81%) | Loss: 0.8312\n",
      "00:09:25 | Epoch: 24/100 | 16000/23507 (68.06%) | Loss: 0.8267\n",
      "00:10:00 | Epoch: 24/100 | 17000/23507 (72.32%) | Loss: 0.8373\n",
      "00:10:35 | Epoch: 24/100 | 18000/23507 (76.57%) | Loss: 0.8394\n",
      "00:11:10 | Epoch: 24/100 | 19000/23507 (80.83%) | Loss: 0.8378\n",
      "00:11:46 | Epoch: 24/100 | 20000/23507 (85.08%) | Loss: 0.8317\n",
      "00:12:21 | Epoch: 24/100 | 21000/23507 (89.34%) | Loss: 0.8448\n",
      "00:12:56 | Epoch: 24/100 | 22000/23507 (93.59%) | Loss: 0.8595\n",
      "00:13:31 | Epoch: 24/100 | 23000/23507 (97.84%) | Loss: 0.8452\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8381 | Genotype loss: 0.6408 | Phenotype loss: 0.1974\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7941 | Genotype loss: 0.6004 | Phenotype loss: 0.1937\n",
      "Phenotype accuracy: 91.88% | Phenotype isolate accuracy: 86.61%\n",
      "Genotype accuracy: 84.34% | Genotype isolate accuracy: 76.72%\n",
      "Elapsed time: 09:36:58\n",
      "Epoch 25/100\n",
      "00:00:35 | Epoch: 25/100 | 1000/23507 (4.25%) | Loss: 0.8281\n",
      "00:01:10 | Epoch: 25/100 | 2000/23507 (8.51%) | Loss: 0.8025\n",
      "00:01:46 | Epoch: 25/100 | 3000/23507 (12.76%) | Loss: 0.8133\n",
      "00:02:21 | Epoch: 25/100 | 4000/23507 (17.02%) | Loss: 0.8324\n",
      "00:02:56 | Epoch: 25/100 | 5000/23507 (21.27%) | Loss: 0.8573\n",
      "00:03:32 | Epoch: 25/100 | 6000/23507 (25.52%) | Loss: 0.8470\n",
      "00:04:07 | Epoch: 25/100 | 7000/23507 (29.78%) | Loss: 0.8472\n",
      "00:04:43 | Epoch: 25/100 | 8000/23507 (34.03%) | Loss: 0.8321\n",
      "00:05:18 | Epoch: 25/100 | 9000/23507 (38.29%) | Loss: 0.8499\n",
      "00:05:53 | Epoch: 25/100 | 10000/23507 (42.54%) | Loss: 0.8406\n",
      "00:06:28 | Epoch: 25/100 | 11000/23507 (46.79%) | Loss: 0.8359\n",
      "00:07:03 | Epoch: 25/100 | 12000/23507 (51.05%) | Loss: 0.8281\n",
      "00:07:39 | Epoch: 25/100 | 13000/23507 (55.30%) | Loss: 0.8328\n",
      "00:08:14 | Epoch: 25/100 | 14000/23507 (59.56%) | Loss: 0.8216\n",
      "00:08:49 | Epoch: 25/100 | 15000/23507 (63.81%) | Loss: 0.8127\n",
      "00:09:24 | Epoch: 25/100 | 16000/23507 (68.06%) | Loss: 0.8165\n",
      "00:10:00 | Epoch: 25/100 | 17000/23507 (72.32%) | Loss: 0.8437\n",
      "00:10:35 | Epoch: 25/100 | 18000/23507 (76.57%) | Loss: 0.8456\n",
      "00:11:10 | Epoch: 25/100 | 19000/23507 (80.83%) | Loss: 0.8365\n",
      "00:11:46 | Epoch: 25/100 | 20000/23507 (85.08%) | Loss: 0.8242\n",
      "00:12:21 | Epoch: 25/100 | 21000/23507 (89.34%) | Loss: 0.8214\n",
      "00:12:57 | Epoch: 25/100 | 22000/23507 (93.59%) | Loss: 0.8280\n",
      "00:13:32 | Epoch: 25/100 | 23000/23507 (97.84%) | Loss: 0.8163\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8315 | Genotype loss: 0.6340 | Phenotype loss: 0.1975\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7885 | Genotype loss: 0.5960 | Phenotype loss: 0.1925\n",
      "Phenotype accuracy: 91.84% | Phenotype isolate accuracy: 86.63%\n",
      "Genotype accuracy: 84.46% | Genotype isolate accuracy: 76.73%\n",
      "Elapsed time: 10:01:04\n",
      "Epoch 26/100\n",
      "00:00:35 | Epoch: 26/100 | 1000/23507 (4.25%) | Loss: 0.8313\n",
      "00:01:10 | Epoch: 26/100 | 2000/23507 (8.51%) | Loss: 0.8298\n",
      "00:01:46 | Epoch: 26/100 | 3000/23507 (12.76%) | Loss: 0.8169\n",
      "00:02:21 | Epoch: 26/100 | 4000/23507 (17.02%) | Loss: 0.8136\n",
      "00:02:56 | Epoch: 26/100 | 5000/23507 (21.27%) | Loss: 0.8279\n",
      "00:03:31 | Epoch: 26/100 | 6000/23507 (25.52%) | Loss: 0.8149\n",
      "00:04:07 | Epoch: 26/100 | 7000/23507 (29.78%) | Loss: 0.8520\n",
      "00:04:42 | Epoch: 26/100 | 8000/23507 (34.03%) | Loss: 0.8032\n",
      "00:05:17 | Epoch: 26/100 | 9000/23507 (38.29%) | Loss: 0.8375\n",
      "00:05:53 | Epoch: 26/100 | 10000/23507 (42.54%) | Loss: 0.8409\n",
      "00:06:28 | Epoch: 26/100 | 11000/23507 (46.79%) | Loss: 0.8485\n",
      "00:07:03 | Epoch: 26/100 | 12000/23507 (51.05%) | Loss: 0.8434\n",
      "00:07:38 | Epoch: 26/100 | 13000/23507 (55.30%) | Loss: 0.8265\n",
      "00:08:14 | Epoch: 26/100 | 14000/23507 (59.56%) | Loss: 0.8381\n",
      "00:08:49 | Epoch: 26/100 | 15000/23507 (63.81%) | Loss: 0.8518\n",
      "00:09:24 | Epoch: 26/100 | 16000/23507 (68.06%) | Loss: 0.8324\n",
      "00:09:59 | Epoch: 26/100 | 17000/23507 (72.32%) | Loss: 0.8268\n",
      "00:10:35 | Epoch: 26/100 | 18000/23507 (76.57%) | Loss: 0.8347\n",
      "00:11:10 | Epoch: 26/100 | 19000/23507 (80.83%) | Loss: 0.8211\n",
      "00:11:45 | Epoch: 26/100 | 20000/23507 (85.08%) | Loss: 0.8185\n",
      "00:12:21 | Epoch: 26/100 | 21000/23507 (89.34%) | Loss: 0.8421\n",
      "00:12:56 | Epoch: 26/100 | 22000/23507 (93.59%) | Loss: 0.8253\n",
      "00:13:31 | Epoch: 26/100 | 23000/23507 (97.84%) | Loss: 0.8224\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8306 | Genotype loss: 0.6334 | Phenotype loss: 0.1972\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7885 | Genotype loss: 0.5948 | Phenotype loss: 0.1937\n",
      "Phenotype accuracy: 91.84% | Phenotype isolate accuracy: 86.61%\n",
      "Genotype accuracy: 84.50% | Genotype isolate accuracy: 76.85%\n",
      "Elapsed time: 10:25:06\n",
      "Epoch 27/100\n",
      "00:00:35 | Epoch: 27/100 | 1000/23507 (4.25%) | Loss: 0.8080\n",
      "00:01:10 | Epoch: 27/100 | 2000/23507 (8.51%) | Loss: 0.8311\n",
      "00:01:45 | Epoch: 27/100 | 3000/23507 (12.76%) | Loss: 0.8199\n",
      "00:02:20 | Epoch: 27/100 | 4000/23507 (17.02%) | Loss: 0.8218\n",
      "00:02:56 | Epoch: 27/100 | 5000/23507 (21.27%) | Loss: 0.8118\n",
      "00:03:31 | Epoch: 27/100 | 6000/23507 (25.52%) | Loss: 0.8471\n",
      "00:04:06 | Epoch: 27/100 | 7000/23507 (29.78%) | Loss: 0.8110\n",
      "00:04:41 | Epoch: 27/100 | 8000/23507 (34.03%) | Loss: 0.8225\n",
      "00:05:16 | Epoch: 27/100 | 9000/23507 (38.29%) | Loss: 0.8190\n",
      "00:05:52 | Epoch: 27/100 | 10000/23507 (42.54%) | Loss: 0.8352\n",
      "00:06:27 | Epoch: 27/100 | 11000/23507 (46.79%) | Loss: 0.8334\n",
      "00:07:02 | Epoch: 27/100 | 12000/23507 (51.05%) | Loss: 0.8433\n",
      "00:07:37 | Epoch: 27/100 | 13000/23507 (55.30%) | Loss: 0.8530\n",
      "00:08:12 | Epoch: 27/100 | 14000/23507 (59.56%) | Loss: 0.8246\n",
      "00:08:48 | Epoch: 27/100 | 15000/23507 (63.81%) | Loss: 0.8193\n",
      "00:09:23 | Epoch: 27/100 | 16000/23507 (68.06%) | Loss: 0.8137\n",
      "00:09:58 | Epoch: 27/100 | 17000/23507 (72.32%) | Loss: 0.8179\n",
      "00:10:33 | Epoch: 27/100 | 18000/23507 (76.57%) | Loss: 0.8165\n",
      "00:11:09 | Epoch: 27/100 | 19000/23507 (80.83%) | Loss: 0.8423\n",
      "00:11:44 | Epoch: 27/100 | 20000/23507 (85.08%) | Loss: 0.8530\n",
      "00:12:19 | Epoch: 27/100 | 21000/23507 (89.34%) | Loss: 0.8345\n",
      "00:12:54 | Epoch: 27/100 | 22000/23507 (93.59%) | Loss: 0.8323\n",
      "00:13:30 | Epoch: 27/100 | 23000/23507 (97.84%) | Loss: 0.8495\n",
      "Epoch completed in 13.8 min\n",
      "Loss: 0.8287 | Genotype loss: 0.6311 | Phenotype loss: 0.1976\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7884 | Genotype loss: 0.5950 | Phenotype loss: 0.1934\n",
      "Phenotype accuracy: 91.88% | Phenotype isolate accuracy: 86.70%\n",
      "Genotype accuracy: 84.44% | Genotype isolate accuracy: 76.84%\n",
      "Elapsed time: 10:49:32\n",
      "Epoch 28/100\n",
      "00:00:38 | Epoch: 28/100 | 1000/23507 (4.25%) | Loss: 0.8314\n",
      "00:01:16 | Epoch: 28/100 | 2000/23507 (8.51%) | Loss: 0.8121\n",
      "00:01:54 | Epoch: 28/100 | 3000/23507 (12.76%) | Loss: 0.8271\n",
      "00:02:32 | Epoch: 28/100 | 4000/23507 (17.02%) | Loss: 0.8293\n",
      "00:03:10 | Epoch: 28/100 | 5000/23507 (21.27%) | Loss: 0.8205\n",
      "00:03:48 | Epoch: 28/100 | 6000/23507 (25.52%) | Loss: 0.8365\n",
      "00:04:26 | Epoch: 28/100 | 7000/23507 (29.78%) | Loss: 0.8321\n",
      "00:05:04 | Epoch: 28/100 | 8000/23507 (34.03%) | Loss: 0.8189\n",
      "00:05:42 | Epoch: 28/100 | 9000/23507 (38.29%) | Loss: 0.8087\n",
      "00:06:20 | Epoch: 28/100 | 10000/23507 (42.54%) | Loss: 0.8274\n",
      "00:06:58 | Epoch: 28/100 | 11000/23507 (46.79%) | Loss: 0.8103\n",
      "00:07:35 | Epoch: 28/100 | 12000/23507 (51.05%) | Loss: 0.8292\n",
      "00:08:13 | Epoch: 28/100 | 13000/23507 (55.30%) | Loss: 0.8266\n",
      "00:08:51 | Epoch: 28/100 | 14000/23507 (59.56%) | Loss: 0.8194\n",
      "00:09:29 | Epoch: 28/100 | 15000/23507 (63.81%) | Loss: 0.8103\n",
      "00:10:07 | Epoch: 28/100 | 16000/23507 (68.06%) | Loss: 0.8362\n",
      "00:10:47 | Epoch: 28/100 | 17000/23507 (72.32%) | Loss: 0.8234\n",
      "00:11:26 | Epoch: 28/100 | 18000/23507 (76.57%) | Loss: 0.8226\n",
      "00:12:04 | Epoch: 28/100 | 19000/23507 (80.83%) | Loss: 0.8520\n",
      "00:12:43 | Epoch: 28/100 | 20000/23507 (85.08%) | Loss: 0.8270\n",
      "00:13:21 | Epoch: 28/100 | 21000/23507 (89.34%) | Loss: 0.8311\n",
      "00:14:00 | Epoch: 28/100 | 22000/23507 (93.59%) | Loss: 0.8089\n",
      "00:14:38 | Epoch: 28/100 | 23000/23507 (97.84%) | Loss: 0.8269\n",
      "Epoch completed in 15.0 min\n",
      "Loss: 0.8248 | Genotype loss: 0.6281 | Phenotype loss: 0.1967\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7853 | Genotype loss: 0.5931 | Phenotype loss: 0.1922\n",
      "Phenotype accuracy: 91.88% | Phenotype isolate accuracy: 86.67%\n",
      "Genotype accuracy: 84.67% | Genotype isolate accuracy: 77.04%\n",
      "Elapsed time: 11:14:29\n",
      "Epoch 29/100\n",
      "00:00:38 | Epoch: 29/100 | 1000/23507 (4.25%) | Loss: 0.8172\n",
      "00:01:17 | Epoch: 29/100 | 2000/23507 (8.51%) | Loss: 0.8235\n",
      "00:01:55 | Epoch: 29/100 | 3000/23507 (12.76%) | Loss: 0.8133\n",
      "00:02:33 | Epoch: 29/100 | 4000/23507 (17.02%) | Loss: 0.8060\n",
      "00:03:12 | Epoch: 29/100 | 5000/23507 (21.27%) | Loss: 0.8126\n",
      "00:03:50 | Epoch: 29/100 | 6000/23507 (25.52%) | Loss: 0.8322\n",
      "00:04:28 | Epoch: 29/100 | 7000/23507 (29.78%) | Loss: 0.8274\n",
      "00:05:06 | Epoch: 29/100 | 8000/23507 (34.03%) | Loss: 0.8069\n",
      "00:05:45 | Epoch: 29/100 | 9000/23507 (38.29%) | Loss: 0.8113\n",
      "00:06:23 | Epoch: 29/100 | 10000/23507 (42.54%) | Loss: 0.8182\n",
      "00:07:02 | Epoch: 29/100 | 11000/23507 (46.79%) | Loss: 0.8226\n",
      "00:07:40 | Epoch: 29/100 | 12000/23507 (51.05%) | Loss: 0.8448\n",
      "00:08:19 | Epoch: 29/100 | 13000/23507 (55.30%) | Loss: 0.8383\n",
      "00:08:57 | Epoch: 29/100 | 14000/23507 (59.56%) | Loss: 0.8226\n",
      "00:09:35 | Epoch: 29/100 | 15000/23507 (63.81%) | Loss: 0.8259\n",
      "00:10:14 | Epoch: 29/100 | 16000/23507 (68.06%) | Loss: 0.8228\n",
      "00:10:53 | Epoch: 29/100 | 17000/23507 (72.32%) | Loss: 0.8159\n",
      "00:11:31 | Epoch: 29/100 | 18000/23507 (76.57%) | Loss: 0.8292\n",
      "00:12:09 | Epoch: 29/100 | 19000/23507 (80.83%) | Loss: 0.8251\n",
      "00:12:48 | Epoch: 29/100 | 20000/23507 (85.08%) | Loss: 0.8276\n",
      "00:13:26 | Epoch: 29/100 | 21000/23507 (89.34%) | Loss: 0.7978\n",
      "00:14:05 | Epoch: 29/100 | 22000/23507 (93.59%) | Loss: 0.8125\n",
      "00:14:43 | Epoch: 29/100 | 23000/23507 (97.84%) | Loss: 0.8321\n",
      "Epoch completed in 15.1 min\n",
      "Loss: 0.8210 | Genotype loss: 0.6241 | Phenotype loss: 0.1969\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7886 | Genotype loss: 0.5967 | Phenotype loss: 0.1919\n",
      "Phenotype accuracy: 91.89% | Phenotype isolate accuracy: 86.62%\n",
      "Genotype accuracy: 84.60% | Genotype isolate accuracy: 77.03%\n",
      "Elapsed time: 11:39:13\n",
      "Epoch 30/100\n",
      "00:00:38 | Epoch: 30/100 | 1000/23507 (4.25%) | Loss: 0.8144\n",
      "00:01:16 | Epoch: 30/100 | 2000/23507 (8.51%) | Loss: 0.8133\n",
      "00:01:54 | Epoch: 30/100 | 3000/23507 (12.76%) | Loss: 0.7974\n",
      "00:02:31 | Epoch: 30/100 | 4000/23507 (17.02%) | Loss: 0.8406\n",
      "00:03:09 | Epoch: 30/100 | 5000/23507 (21.27%) | Loss: 0.8158\n",
      "00:03:47 | Epoch: 30/100 | 6000/23507 (25.52%) | Loss: 0.8316\n",
      "00:04:25 | Epoch: 30/100 | 7000/23507 (29.78%) | Loss: 0.8327\n",
      "00:05:03 | Epoch: 30/100 | 8000/23507 (34.03%) | Loss: 0.8197\n",
      "00:05:41 | Epoch: 30/100 | 9000/23507 (38.29%) | Loss: 0.8179\n",
      "00:06:21 | Epoch: 30/100 | 10000/23507 (42.54%) | Loss: 0.8243\n",
      "00:07:01 | Epoch: 30/100 | 11000/23507 (46.79%) | Loss: 0.8372\n",
      "00:07:39 | Epoch: 30/100 | 12000/23507 (51.05%) | Loss: 0.8217\n",
      "00:08:18 | Epoch: 30/100 | 13000/23507 (55.30%) | Loss: 0.7996\n",
      "00:08:56 | Epoch: 30/100 | 14000/23507 (59.56%) | Loss: 0.8133\n",
      "00:09:33 | Epoch: 30/100 | 15000/23507 (63.81%) | Loss: 0.8135\n",
      "00:10:10 | Epoch: 30/100 | 16000/23507 (68.06%) | Loss: 0.8297\n",
      "00:10:49 | Epoch: 30/100 | 17000/23507 (72.32%) | Loss: 0.8112\n",
      "00:11:27 | Epoch: 30/100 | 18000/23507 (76.57%) | Loss: 0.8241\n",
      "00:12:05 | Epoch: 30/100 | 19000/23507 (80.83%) | Loss: 0.8049\n",
      "00:12:42 | Epoch: 30/100 | 20000/23507 (85.08%) | Loss: 0.8225\n",
      "00:13:20 | Epoch: 30/100 | 21000/23507 (89.34%) | Loss: 0.8073\n",
      "00:13:59 | Epoch: 30/100 | 22000/23507 (93.59%) | Loss: 0.8195\n",
      "00:14:46 | Epoch: 30/100 | 23000/23507 (97.84%) | Loss: 0.8400\n",
      "Epoch completed in 15.2 min\n",
      "Loss: 0.8198 | Genotype loss: 0.6234 | Phenotype loss: 0.1963\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7856 | Genotype loss: 0.5936 | Phenotype loss: 0.1920\n",
      "Phenotype accuracy: 91.89% | Phenotype isolate accuracy: 86.68%\n",
      "Genotype accuracy: 84.62% | Genotype isolate accuracy: 76.91%\n",
      "Elapsed time: 12:03:21\n",
      "Epoch 31/100\n",
      "00:00:37 | Epoch: 31/100 | 1000/23507 (4.25%) | Loss: 0.8134\n",
      "00:01:16 | Epoch: 31/100 | 2000/23507 (8.51%) | Loss: 0.8117\n",
      "00:01:55 | Epoch: 31/100 | 3000/23507 (12.76%) | Loss: 0.8092\n",
      "00:02:34 | Epoch: 31/100 | 4000/23507 (17.02%) | Loss: 0.8224\n",
      "00:03:39 | Epoch: 31/100 | 5000/23507 (21.27%) | Loss: 0.8249\n",
      "00:04:25 | Epoch: 31/100 | 6000/23507 (25.52%) | Loss: 0.7890\n",
      "00:05:06 | Epoch: 31/100 | 7000/23507 (29.78%) | Loss: 0.8085\n",
      "00:05:47 | Epoch: 31/100 | 8000/23507 (34.03%) | Loss: 0.8326\n",
      "00:06:26 | Epoch: 31/100 | 9000/23507 (38.29%) | Loss: 0.8147\n",
      "00:07:06 | Epoch: 31/100 | 10000/23507 (42.54%) | Loss: 0.8212\n",
      "00:07:46 | Epoch: 31/100 | 11000/23507 (46.79%) | Loss: 0.8204\n",
      "00:08:26 | Epoch: 31/100 | 12000/23507 (51.05%) | Loss: 0.8146\n",
      "00:09:07 | Epoch: 31/100 | 13000/23507 (55.30%) | Loss: 0.8235\n",
      "00:09:47 | Epoch: 31/100 | 14000/23507 (59.56%) | Loss: 0.8164\n",
      "00:10:28 | Epoch: 31/100 | 15000/23507 (63.81%) | Loss: 0.8152\n",
      "00:11:07 | Epoch: 31/100 | 16000/23507 (68.06%) | Loss: 0.8143\n",
      "00:11:45 | Epoch: 31/100 | 17000/23507 (72.32%) | Loss: 0.8243\n",
      "00:12:23 | Epoch: 31/100 | 18000/23507 (76.57%) | Loss: 0.8398\n",
      "00:13:01 | Epoch: 31/100 | 19000/23507 (80.83%) | Loss: 0.8106\n",
      "00:13:39 | Epoch: 31/100 | 20000/23507 (85.08%) | Loss: 0.8234\n",
      "00:14:17 | Epoch: 31/100 | 21000/23507 (89.34%) | Loss: 0.8018\n",
      "00:14:55 | Epoch: 31/100 | 22000/23507 (93.59%) | Loss: 0.8432\n",
      "00:15:32 | Epoch: 31/100 | 23000/23507 (97.84%) | Loss: 0.8451\n",
      "Epoch completed in 15.9 min\n",
      "Loss: 0.8192 | Genotype loss: 0.6233 | Phenotype loss: 0.1959\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7781 | Genotype loss: 0.5865 | Phenotype loss: 0.1916\n",
      "Phenotype accuracy: 91.86% | Phenotype isolate accuracy: 86.62%\n",
      "Genotype accuracy: 84.73% | Genotype isolate accuracy: 77.15%\n",
      "Elapsed time: 12:28:10\n",
      "Epoch 32/100\n",
      "00:00:38 | Epoch: 32/100 | 1000/23507 (4.25%) | Loss: 0.8099\n",
      "00:01:17 | Epoch: 32/100 | 2000/23507 (8.51%) | Loss: 0.8153\n",
      "00:01:56 | Epoch: 32/100 | 3000/23507 (12.76%) | Loss: 0.8121\n",
      "00:02:36 | Epoch: 32/100 | 4000/23507 (17.02%) | Loss: 0.7995\n",
      "00:03:15 | Epoch: 32/100 | 5000/23507 (21.27%) | Loss: 0.8003\n",
      "00:03:52 | Epoch: 32/100 | 6000/23507 (25.52%) | Loss: 0.8092\n",
      "00:04:30 | Epoch: 32/100 | 7000/23507 (29.78%) | Loss: 0.8078\n",
      "00:05:06 | Epoch: 32/100 | 8000/23507 (34.03%) | Loss: 0.8067\n",
      "00:05:44 | Epoch: 32/100 | 9000/23507 (38.29%) | Loss: 0.8115\n",
      "00:06:21 | Epoch: 32/100 | 10000/23507 (42.54%) | Loss: 0.8228\n",
      "00:06:59 | Epoch: 32/100 | 11000/23507 (46.79%) | Loss: 0.8309\n",
      "00:07:37 | Epoch: 32/100 | 12000/23507 (51.05%) | Loss: 0.8204\n",
      "00:08:15 | Epoch: 32/100 | 13000/23507 (55.30%) | Loss: 0.8145\n",
      "00:08:53 | Epoch: 32/100 | 14000/23507 (59.56%) | Loss: 0.8009\n",
      "00:09:30 | Epoch: 32/100 | 15000/23507 (63.81%) | Loss: 0.8350\n",
      "00:10:06 | Epoch: 32/100 | 16000/23507 (68.06%) | Loss: 0.8271\n",
      "00:10:44 | Epoch: 32/100 | 17000/23507 (72.32%) | Loss: 0.8315\n",
      "00:11:21 | Epoch: 32/100 | 18000/23507 (76.57%) | Loss: 0.8143\n",
      "00:11:57 | Epoch: 32/100 | 19000/23507 (80.83%) | Loss: 0.8016\n",
      "00:12:34 | Epoch: 32/100 | 20000/23507 (85.08%) | Loss: 0.8062\n",
      "00:13:11 | Epoch: 32/100 | 21000/23507 (89.34%) | Loss: 0.8093\n",
      "00:13:48 | Epoch: 32/100 | 22000/23507 (93.59%) | Loss: 0.8166\n",
      "00:14:26 | Epoch: 32/100 | 23000/23507 (97.84%) | Loss: 0.8287\n",
      "Epoch completed in 14.8 min\n",
      "Loss: 0.8145 | Genotype loss: 0.6179 | Phenotype loss: 0.1966\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7809 | Genotype loss: 0.5899 | Phenotype loss: 0.1910\n",
      "Phenotype accuracy: 91.89% | Phenotype isolate accuracy: 86.69%\n",
      "Genotype accuracy: 84.56% | Genotype isolate accuracy: 76.85%\n",
      "Elapsed time: 12:51:38\n",
      "Epoch 33/100\n",
      "00:00:39 | Epoch: 33/100 | 1000/23507 (4.25%) | Loss: 0.7951\n",
      "00:01:18 | Epoch: 33/100 | 2000/23507 (8.51%) | Loss: 0.8002\n",
      "00:01:56 | Epoch: 33/100 | 3000/23507 (12.76%) | Loss: 0.8144\n",
      "00:02:34 | Epoch: 33/100 | 4000/23507 (17.02%) | Loss: 0.7983\n",
      "00:03:12 | Epoch: 33/100 | 5000/23507 (21.27%) | Loss: 0.8179\n",
      "00:03:50 | Epoch: 33/100 | 6000/23507 (25.52%) | Loss: 0.8027\n",
      "00:04:28 | Epoch: 33/100 | 7000/23507 (29.78%) | Loss: 0.8324\n",
      "00:05:05 | Epoch: 33/100 | 8000/23507 (34.03%) | Loss: 0.8016\n",
      "00:05:41 | Epoch: 33/100 | 9000/23507 (38.29%) | Loss: 0.8048\n",
      "00:06:18 | Epoch: 33/100 | 10000/23507 (42.54%) | Loss: 0.8124\n",
      "00:06:55 | Epoch: 33/100 | 11000/23507 (46.79%) | Loss: 0.8077\n",
      "00:07:33 | Epoch: 33/100 | 12000/23507 (51.05%) | Loss: 0.8223\n",
      "00:08:11 | Epoch: 33/100 | 13000/23507 (55.30%) | Loss: 0.8167\n",
      "00:08:47 | Epoch: 33/100 | 14000/23507 (59.56%) | Loss: 0.7971\n",
      "00:09:24 | Epoch: 33/100 | 15000/23507 (63.81%) | Loss: 0.8266\n",
      "00:10:02 | Epoch: 33/100 | 16000/23507 (68.06%) | Loss: 0.8259\n",
      "00:10:40 | Epoch: 33/100 | 17000/23507 (72.32%) | Loss: 0.8195\n",
      "00:11:20 | Epoch: 33/100 | 18000/23507 (76.57%) | Loss: 0.8204\n",
      "00:11:58 | Epoch: 33/100 | 19000/23507 (80.83%) | Loss: 0.8246\n",
      "00:12:36 | Epoch: 33/100 | 20000/23507 (85.08%) | Loss: 0.8227\n",
      "00:13:13 | Epoch: 33/100 | 21000/23507 (89.34%) | Loss: 0.8328\n",
      "00:13:50 | Epoch: 33/100 | 22000/23507 (93.59%) | Loss: 0.8097\n",
      "00:14:28 | Epoch: 33/100 | 23000/23507 (97.84%) | Loss: 0.8102\n",
      "Epoch completed in 14.8 min\n",
      "Loss: 0.8136 | Genotype loss: 0.6178 | Phenotype loss: 0.1958\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7776 | Genotype loss: 0.5857 | Phenotype loss: 0.1919\n",
      "Phenotype accuracy: 91.88% | Phenotype isolate accuracy: 86.60%\n",
      "Genotype accuracy: 84.88% | Genotype isolate accuracy: 77.26%\n",
      "Elapsed time: 13:15:15\n",
      "Epoch 34/100\n",
      "00:00:37 | Epoch: 34/100 | 1000/23507 (4.25%) | Loss: 0.8300\n",
      "00:01:16 | Epoch: 34/100 | 2000/23507 (8.51%) | Loss: 0.8210\n",
      "00:01:55 | Epoch: 34/100 | 3000/23507 (12.76%) | Loss: 0.7991\n",
      "00:02:33 | Epoch: 34/100 | 4000/23507 (17.02%) | Loss: 0.8148\n",
      "00:03:11 | Epoch: 34/100 | 5000/23507 (21.27%) | Loss: 0.8163\n",
      "00:03:48 | Epoch: 34/100 | 6000/23507 (25.52%) | Loss: 0.7944\n",
      "00:04:26 | Epoch: 34/100 | 7000/23507 (29.78%) | Loss: 0.8235\n",
      "00:05:05 | Epoch: 34/100 | 8000/23507 (34.03%) | Loss: 0.8398\n",
      "00:05:43 | Epoch: 34/100 | 9000/23507 (38.29%) | Loss: 0.7896\n",
      "00:06:21 | Epoch: 34/100 | 10000/23507 (42.54%) | Loss: 0.8252\n",
      "00:06:57 | Epoch: 34/100 | 11000/23507 (46.79%) | Loss: 0.8236\n",
      "00:07:34 | Epoch: 34/100 | 12000/23507 (51.05%) | Loss: 0.8241\n",
      "00:08:11 | Epoch: 34/100 | 13000/23507 (55.30%) | Loss: 0.8272\n",
      "00:08:47 | Epoch: 34/100 | 14000/23507 (59.56%) | Loss: 0.8101\n",
      "00:09:24 | Epoch: 34/100 | 15000/23507 (63.81%) | Loss: 0.8158\n",
      "00:10:01 | Epoch: 34/100 | 16000/23507 (68.06%) | Loss: 0.7983\n",
      "00:10:38 | Epoch: 34/100 | 17000/23507 (72.32%) | Loss: 0.8251\n",
      "00:11:17 | Epoch: 34/100 | 18000/23507 (76.57%) | Loss: 0.8189\n",
      "00:11:54 | Epoch: 34/100 | 19000/23507 (80.83%) | Loss: 0.8104\n",
      "00:12:33 | Epoch: 34/100 | 20000/23507 (85.08%) | Loss: 0.8296\n",
      "00:13:11 | Epoch: 34/100 | 21000/23507 (89.34%) | Loss: 0.8128\n",
      "00:13:48 | Epoch: 34/100 | 22000/23507 (93.59%) | Loss: 0.8159\n",
      "00:14:27 | Epoch: 34/100 | 23000/23507 (97.84%) | Loss: 0.8230\n",
      "Epoch completed in 14.8 min\n",
      "Loss: 0.8166 | Genotype loss: 0.6195 | Phenotype loss: 0.1971\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7780 | Genotype loss: 0.5864 | Phenotype loss: 0.1916\n",
      "Phenotype accuracy: 91.89% | Phenotype isolate accuracy: 86.69%\n",
      "Genotype accuracy: 84.91% | Genotype isolate accuracy: 77.29%\n",
      "Elapsed time: 13:39:06\n",
      "Epoch 35/100\n",
      "00:00:36 | Epoch: 35/100 | 1000/23507 (4.25%) | Loss: 0.8142\n",
      "00:01:13 | Epoch: 35/100 | 2000/23507 (8.51%) | Loss: 0.8031\n",
      "00:01:50 | Epoch: 35/100 | 3000/23507 (12.76%) | Loss: 0.8008\n",
      "00:02:27 | Epoch: 35/100 | 4000/23507 (17.02%) | Loss: 0.8175\n",
      "00:03:03 | Epoch: 35/100 | 5000/23507 (21.27%) | Loss: 0.8013\n",
      "00:03:40 | Epoch: 35/100 | 6000/23507 (25.52%) | Loss: 0.8176\n",
      "00:04:17 | Epoch: 35/100 | 7000/23507 (29.78%) | Loss: 0.8179\n",
      "00:04:52 | Epoch: 35/100 | 8000/23507 (34.03%) | Loss: 0.8038\n",
      "00:05:29 | Epoch: 35/100 | 9000/23507 (38.29%) | Loss: 0.8196\n",
      "00:06:06 | Epoch: 35/100 | 10000/23507 (42.54%) | Loss: 0.8089\n",
      "00:06:41 | Epoch: 35/100 | 11000/23507 (46.79%) | Loss: 0.8207\n",
      "00:07:17 | Epoch: 35/100 | 12000/23507 (51.05%) | Loss: 0.7868\n",
      "00:07:53 | Epoch: 35/100 | 13000/23507 (55.30%) | Loss: 0.8073\n",
      "00:08:30 | Epoch: 35/100 | 14000/23507 (59.56%) | Loss: 0.8166\n",
      "00:09:08 | Epoch: 35/100 | 15000/23507 (63.81%) | Loss: 0.8285\n",
      "00:09:46 | Epoch: 35/100 | 16000/23507 (68.06%) | Loss: 0.7887\n",
      "00:10:21 | Epoch: 35/100 | 17000/23507 (72.32%) | Loss: 0.8128\n",
      "00:10:58 | Epoch: 35/100 | 18000/23507 (76.57%) | Loss: 0.8338\n",
      "00:11:34 | Epoch: 35/100 | 19000/23507 (80.83%) | Loss: 0.8436\n",
      "00:12:12 | Epoch: 35/100 | 20000/23507 (85.08%) | Loss: 0.7894\n",
      "00:12:47 | Epoch: 35/100 | 21000/23507 (89.34%) | Loss: 0.7978\n",
      "00:13:24 | Epoch: 35/100 | 22000/23507 (93.59%) | Loss: 0.8230\n",
      "00:14:01 | Epoch: 35/100 | 23000/23507 (97.84%) | Loss: 0.8068\n",
      "Epoch completed in 14.3 min\n",
      "Loss: 0.8111 | Genotype loss: 0.6148 | Phenotype loss: 0.1963\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.7718 | Genotype loss: 0.5791 | Phenotype loss: 0.1927\n",
      "Phenotype accuracy: 91.88% | Phenotype isolate accuracy: 86.67%\n",
      "Genotype accuracy: 84.86% | Genotype isolate accuracy: 77.07%\n",
      "Elapsed time: 14:02:31\n",
      "Epoch 36/100\n",
      "00:00:39 | Epoch: 36/100 | 1000/23507 (4.25%) | Loss: 0.8056\n",
      "00:01:17 | Epoch: 36/100 | 2000/23507 (8.51%) | Loss: 0.8129\n",
      "00:01:52 | Epoch: 36/100 | 3000/23507 (12.76%) | Loss: 0.8275\n",
      "00:02:28 | Epoch: 36/100 | 4000/23507 (17.02%) | Loss: 0.8031\n",
      "00:03:06 | Epoch: 36/100 | 5000/23507 (21.27%) | Loss: 0.8077\n"
     ]
    }
   ],
   "source": [
    "from utils import get_multimodal_split_indices, export_results\n",
    "from multimodal.models import BERT\n",
    "\n",
    "specials = config['specials']\n",
    "pad_token = specials['PAD']\n",
    "pad_idx = list(specials.values()).index(pad_token) # pass to model for embedding\n",
    "\n",
    "ds_geno = ds_NCBI[ds_NCBI['num_ab'] == 0].reset_index(drop=True)\n",
    "ds_geno.fillna(pad_token, inplace=True)\n",
    "ds_pheno = ds_TESSy.copy()\n",
    "ds_pheno['country'] = ds_pheno['country'].map(config['data']['TESSy']['country_code_to_name'])\n",
    "\n",
    "# ds_geno = ds_geno.iloc[:20000]\n",
    "# ds_pheno = ds_pheno.iloc[:100000]\n",
    "\n",
    "antibiotics = list(set(data_dict['antibiotics']['abbr_to_names'].keys()) - set(data_dict['exclude_antibiotics']))\n",
    "vocab = construct_MM_vocab(\n",
    "    df_geno=ds_NCBI,\n",
    "    df_pheno=ds_pheno,\n",
    "    antibiotics=antibiotics,\n",
    "    specials=specials,\n",
    ")\n",
    "vocab_size = len(vocab)\n",
    "config['name'] = 'pt_test'\n",
    "if config['name']:\n",
    "        results_dir = Path(os.path.join(base_dir / \"results\" / \"MM\", config['name']))\n",
    "else:\n",
    "    time_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    results_dir = Path(os.path.join(base_dir / \"results\" / \"MM\", \"experiment_\" + str(time_str)))\n",
    "\n",
    "os.environ['WANDB_MODE'] = config['wandb_mode']\n",
    "if config['max_seq_len'] == 'auto':\n",
    "    max_seq_len = int(max((ds_NCBI['num_genotypes'] + ds_NCBI['num_ab']).max() + 3, ds_pheno['num_ab'].max() + 5))\n",
    "\n",
    "train_indices, val_indices = get_multimodal_split_indices(\n",
    "    [ds_geno.shape[0], ds_pheno.shape[0]], \n",
    "    val_share=config['val_share'], \n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "ds_pt_train = MMPretrainDataset(\n",
    "    ds_geno=ds_geno.iloc[train_indices[0]],\n",
    "    ds_pheno=ds_pheno.iloc[train_indices[1]],\n",
    "    vocab=vocab,\n",
    "    antibiotics=antibiotics,\n",
    "    specials=specials,\n",
    "    max_seq_len=max_seq_len,\n",
    "    mask_prob_geno=config['mask_prob_geno'],\n",
    "    mask_prob_pheno=config['mask_prob_pheno'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "ds_pt_val = MMPretrainDataset(\n",
    "    ds_geno=ds_geno.iloc[val_indices[0]],\n",
    "    ds_pheno=ds_pheno.iloc[val_indices[1]],\n",
    "    vocab=vocab,\n",
    "    antibiotics=antibiotics,\n",
    "    specials=specials,\n",
    "    max_seq_len=max_seq_len,\n",
    "    mask_prob_geno=config['mask_prob_geno'],\n",
    "    mask_prob_pheno=config['mask_prob_pheno'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "bert = BERT(config, vocab_size, max_seq_len, len(antibiotics), pad_idx=pad_idx).to(device)\n",
    "trainer = MMBertPreTrainer(\n",
    "    config=config,\n",
    "    model=bert,\n",
    "    antibiotics=antibiotics,\n",
    "    train_set=ds_pt_train,\n",
    "    val_set=ds_pt_val,\n",
    "    results_dir=results_dir,\n",
    ")\n",
    "trainer.print_model_summary()\n",
    "trainer.print_trainer_summary()\n",
    "pt_results = trainer()\n",
    "export_results(pt_results, results_dir / 'pt_results.pkl')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMFinetuneDataset(Dataset):\n",
    "    # df column names\n",
    "    # ORIGINAL_SEQUENCE = 'original_sequence'\n",
    "    INDICES_MASKED = 'indices_masked' # input to BERT, token indices of the masked sequence\n",
    "    # TARGET_INDICES = 'target_indices' # target indices of the masked sequence ## USE IF GENOTYPES ARE ALSO MASKED ##\n",
    "    TARGET_RESISTANCES = 'target_resistances' # resistance of the target antibiotics, what we want to predict\n",
    "    TOKEN_TYPES = 'token_types' # # 0 for patient info, 1 for genotype, 2 for phenotype\n",
    "    # if sequences are included\n",
    "    MASKED_SEQUENCE = 'masked_sequence'\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df_MM: pd.DataFrame, \n",
    "        vocab,\n",
    "        antibiotics: list,\n",
    "        specials: dict,\n",
    "        max_seq_len: int,\n",
    "        mask_prob: float,\n",
    "        num_known_ab: int,\n",
    "        random_state: int = 42,\n",
    "        include_sequences: bool = False\n",
    "    ):\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        self.ds_MM = df_MM.reset_index(drop=True)\n",
    "        assert all(self.ds_MM['num_ab'] > 0), \"Dataset contains isolates without phenotypes\"\n",
    "        self.num_samples = self.ds_MM.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics)\n",
    "        self.ab_to_idx = {ab: idx for idx, ab in enumerate(self.antibiotics)}\n",
    "        self.enc_res = {'S': 0, 'R': 1}\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.CLS, self.PAD, self.MASK, self.UNK = specials.values()\n",
    "        \n",
    "        self.mask_prob = mask_prob\n",
    "        self.num_known_ab = num_known_ab\n",
    "        assert not (self.num_known_ab and self.mask_prob), \"Cannot specify both num_known_ab and mask_prob\"\n",
    "        \n",
    "        self.include_sequences = include_sequences\n",
    "        if self.include_sequences:\n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_RESISTANCES, self.TOKEN_TYPES,\n",
    "                            self.MASKED_SEQUENCE]\n",
    "        else:\n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_RESISTANCES, self.TOKEN_TYPES]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        \n",
    "        input = torch.tensor(item[self.INDICES_MASKED], dtype=torch.long, device=device)\n",
    "        target_res = torch.tensor(item[self.TARGET_RESISTANCES], dtype=torch.float32, device=device)\n",
    "        token_types = torch.tensor(item[self.TOKEN_TYPES], dtype=torch.long, device=device)\n",
    "        attn_mask = (input != self.vocab[self.PAD]).unsqueeze(0).unsqueeze(1) # one dim for batch, one for heads\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            masked_sequence = item[self.MASKED_SEQUENCE]\n",
    "            return input, target_res, token_types, attn_mask, masked_sequence\n",
    "        else:\n",
    "            return input, target_res, token_types, attn_mask   \n",
    "    \n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        geno_sequences = deepcopy(self.ds_MM['genotypes'].tolist())\n",
    "        pheno_sequences = deepcopy(self.ds_MM['phenotypes'].tolist())\n",
    "        years = self.ds_MM['year'].astype(str).tolist()\n",
    "        countries = self.ds_MM['country'].tolist()\n",
    "        \n",
    "        masked_pheno_sequences, target_resistances = self._mask_pheno_sequences(pheno_sequences)\n",
    "        pheno_token_types = [[2]*len(seq) for seq in masked_pheno_sequences]\n",
    "        \n",
    "        geno_token_types = [[1]*len(seq) for seq in geno_sequences]\n",
    "        seq_starts = [[self.CLS, years[i], countries[i]] for i in range(self.num_samples)]\n",
    "        \n",
    "        # combine sequences and pad\n",
    "        masked_sequences = [seq_starts[i] + geno_sequences[i] + masked_pheno_sequences[i] for i in range(self.num_samples)]\n",
    "        masked_sequences = [seq + [self.PAD]*(self.max_seq_len - len(seq)) for seq in masked_sequences]\n",
    "        indices_masked = [self.vocab.lookup_indices(seq) for seq in masked_sequences]\n",
    "        \n",
    "        token_types = [[0]*3 + geno_token_types[i] + pheno_token_types[i] for i in range(self.num_samples)]\n",
    "        token_types = [seq + [2]*(self.max_seq_len - len(seq)) for seq in token_types]\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            rows = zip(indices_masked, target_resistances, token_types, masked_sequences)\n",
    "        else:\n",
    "            rows = zip(indices_masked, target_resistances, token_types)\n",
    "        self.df = pd.DataFrame(rows, columns=self.columns)\n",
    "         \n",
    "    \n",
    "    def _mask_pheno_sequences(self, pheno_sequences):\n",
    "        masked_pheno_sequences = list()\n",
    "        target_resistances = list()\n",
    "\n",
    "        if self.mask_prob:\n",
    "            for pheno_seq in pheno_sequences:\n",
    "                seq_len = len(pheno_seq)\n",
    "                token_mask = np.random.rand(seq_len) < self.mask_prob\n",
    "                target_res = [-1]*self.num_ab\n",
    "                if not token_mask.any():\n",
    "                    idx = np.random.randint(seq_len)\n",
    "                    ab, res = pheno_seq[idx].split('_')\n",
    "                    target_res[self.ab_to_idx[ab]] = self.enc_res[res]  \n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8:\n",
    "                        pheno_seq[idx] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        pheno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size)) \n",
    "                else:\n",
    "                    for idx in token_mask.nonzero()[0]:\n",
    "                        ab, res = pheno_seq[idx].split('_')\n",
    "                        target_res[self.ab_to_idx[ab]] = self.enc_res[res]\n",
    "                        r = np.random.rand()\n",
    "                        if r < 0.8:\n",
    "                            pheno_seq[idx] = self.MASK\n",
    "                        elif r < 0.9:\n",
    "                            pheno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "                masked_pheno_sequences.append(pheno_seq)\n",
    "                target_resistances.append(target_res)\n",
    "        else:\n",
    "            for pheno_seq in pheno_sequences:\n",
    "                seq_len = len(pheno_seq)\n",
    "                target_res = [-1]*self.num_ab\n",
    "                indices = np.random.choice(seq_len, self.num_known_ab, replace=False)\n",
    "                for idx in indices:\n",
    "                    ab, res = pheno_seq[idx].split('_')\n",
    "                    target_res[self.ab_to_idx[ab]] = self.enc_res[res]\n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8:\n",
    "                        pheno_seq[idx] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        pheno_seq[idx] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "                masked_pheno_sequences.append(pheno_seq)\n",
    "                target_resistances.append(target_res)    \n",
    "        return masked_pheno_sequences, target_resistances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMBertFineTuner():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        model,\n",
    "        antibiotics: list,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        results_dir: Path\n",
    "    ):\n",
    "        super(MMBertFineTuner, self).__init__()\n",
    "        \n",
    "        config_ft = config[\"fine_tune\"]\n",
    "        self.random_state = config_ft['random_state']\n",
    "        np.random.seed(self.random_state)\n",
    "        torch.manual_seed(self.random_state)\n",
    "        torch.cuda.manual_seed(self.random_state)\n",
    "        \n",
    "        self.model = model\n",
    "        self.project_name = config_ft[\"project_name\"]\n",
    "        self.wandb_name = config_ft[\"name\"] if config_ft[\"name\"] else datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics) \n",
    "        \n",
    "        self.train_set, self.train_size = train_set, len(train_set)\n",
    "        self.val_set, self.val_size = val_set, len(val_set) \n",
    "        assert round(self.val_size / (self.train_size + self.val_size), 2) == config_ft[\"val_share\"], \"Validation set size does not match intended val_share\"\n",
    "        self.val_share, self.train_share = config_ft[\"val_share\"], 1 - config_ft[\"val_share\"]\n",
    "        self.batch_size = config_ft[\"batch_size\"]\n",
    "        self.num_batches = round(self.train_size / self.batch_size)\n",
    "        self.vocab = self.train_set.vocab\n",
    "         \n",
    "        self.lr = config_ft[\"lr\"]\n",
    "        self.weight_decay = config_ft[\"weight_decay\"]\n",
    "        self.epochs = config_ft[\"epochs\"]\n",
    "        self.patience = config_ft[\"early_stopping_patience\"]\n",
    "        self.save_model_ = config_ft[\"save_model\"]\n",
    "        \n",
    "        self.mask_prob = self.train_set.mask_prob\n",
    "        self.num_known_ab = self.train_set.num_known_ab\n",
    "        \n",
    "        self.ab_criterions = [nn.BCEWithLogitsLoss().to(device) for _ in range(self.num_ab)] # the list is so that we can introduce individual weights\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.scheduler = None\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.9)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.98)\n",
    "                 \n",
    "        self.current_epoch = 0\n",
    "        self.report_every = config_ft[\"report_every\"] \n",
    "        self.print_progress_every = config_ft[\"print_progress_every\"]\n",
    "        self._splitter_size = 70\n",
    "        self.results_dir = results_dir\n",
    "        if self.results_dir:\n",
    "            self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "    def print_model_summary(self):        \n",
    "        print(\"Model summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Is pre-trained: {'Yes' if self.model.is_pretrained else 'No'}\")\n",
    "        print(f\"Embedding dim: {self.model.emb_dim}\")\n",
    "        print(f\"Feed-forward dim: {self.model.ff_dim}\")\n",
    "        print(f\"Hidden dim: {self.model.hidden_dim}\")\n",
    "        print(f\"Number of heads: {self.model.num_heads}\")\n",
    "        print(f\"Number of encoder layers: {self.model.num_layers}\")\n",
    "        print(f\"Dropout probability: {self.model.dropout_prob:.0%}\")\n",
    "        print(f\"Max sequence length: {self.model.max_seq_len}\")\n",
    "        print(f\"Vocab size: {len(self.vocab):,}\")\n",
    "        print(f\"Number of parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "    \n",
    "    def print_trainer_summary(self):\n",
    "        print(\"Trainer summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        if device.type == \"cuda\":\n",
    "            print(f\"Device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "        else:\n",
    "            print(f\"Device: {device}\")        \n",
    "        print(f\"Training dataset size: {self.train_size:,}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Number of batches: {self.num_batches:,}\")\n",
    "        print(f\"Number of antibiotics: {self.num_ab}\")\n",
    "        print(f\"Antibiotics: {self.antibiotics}\")\n",
    "        print(f\"CV split: {self.train_share:.0%} train | {self.val_share:.0%} val\")\n",
    "        if self.mask_prob:\n",
    "            print(f\"Mask probability: {self.mask_prob:.0%}\")\n",
    "        if self.num_known_ab:\n",
    "            print(f\"Number of known antibiotics: {self.num_known_ab}\")\n",
    "        print(f\"Number of epochs: {self.epochs}\")\n",
    "        print(f\"Early stopping patience: {self.patience}\")\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Weight decay: {self.weight_decay}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "    \n",
    "    def __call__(self):      \n",
    "        if not self.model.pheno_only:\n",
    "            self.model.pheno_only = True\n",
    "        self.wandb_run = self._init_wandb()\n",
    "        self.val_set.prepare_dataset()\n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf') \n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            train_loss = self.train(self.current_epoch) # returns loss, averaged over batches\n",
    "            self.losses.append(train_loss)\n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min | Loss: {train_loss:.4f}\")\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader, self.val_set)\n",
    "            s = f\"Val loss: {val_results['loss']:.4f}\"\n",
    "            s += f\" | Accuracy {val_results['acc']:.2%} | Isolate accuracy {val_results['iso_acc']:.2%}\"\n",
    "            print(s)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            print(\"=\"*self._splitter_size)\n",
    "            self._update_val_lists(val_results)\n",
    "            self._report_epoch_results()\n",
    "            early_stop = self.early_stopping()\n",
    "            if early_stop:\n",
    "                print(f\"Early stopping at epoch {self.current_epoch+1} with validation loss {self.val_losses[-1]:.4f}\")\n",
    "                print(f\"Validation stats at best epoch ({self.best_epoch+1}):\")\n",
    "                s = f\"Loss: {self.val_losses[self.best_epoch]:.4f}\" \n",
    "                s += f\" | Accuracy: {self.val_accs[self.best_epoch]:.2%}\"\n",
    "                s += f\" | Isolate accuracy: {self.val_iso_accs[self.best_epoch]:.2%}\"\n",
    "                print(s)\n",
    "                self.wandb_run.log({\n",
    "                    \"Losses/final_val_loss\": self.best_val_loss, \n",
    "                    \"Accuracies/final_val_acc\": self.val_accs[self.best_epoch],\n",
    "                    \"Accuracies/final_val_iso_acc\": self.val_iso_accs[self.best_epoch],\n",
    "                    \"final_epoch\": self.best_epoch+1\n",
    "                })\n",
    "                self.model.load_state_dict(self.best_model_state) \n",
    "                self.current_epoch = self.best_epoch\n",
    "                break\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "        if not early_stop:    \n",
    "            self.wandb_run.log({\n",
    "                    \"Losses/final_val_loss\": self.best_val_loss, \n",
    "                    \"Accuracies/final_val_acc\": self.val_accs[-1],\n",
    "                    \"Accuracies/final_val_iso_acc\": self.val_iso_accs[-1],\n",
    "                    \"final_epoch\": self.current_epoch+1\n",
    "                })\n",
    "        self.model.is_pretrained = True\n",
    "        if self.save_model_:\n",
    "            self.save_model(self.results_dir / \"model_state.pt\") \n",
    "        train_time = (time.time() - start_time)/60\n",
    "        self.wandb_run.log({\"Training time (min)\": train_time})\n",
    "        disp_time = f\"{train_time//60:.0f}h {train_time % 60:.1f} min\" if train_time > 60 else f\"{train_time:.1f} min\"\n",
    "        print(f\"Training completed in {disp_time}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        if not early_stop:\n",
    "            print(\"Final validation stats:\")\n",
    "            s = f\"Loss: {self.val_losses[-1]:.4f}\"\n",
    "            s = f\" | Accuracy: {self.val_accs[-1]:.2%}\"\n",
    "            s += f\" | Isolate accuracy: {self.val_iso_accs[-1]:.2%}\"\n",
    "            print(s)\n",
    "        \n",
    "        results = {\n",
    "            \"best epoch\": self.best_epoch,\n",
    "            \"train_losses\": self.losses,\n",
    "            \"val_losses\": self.val_losses,\n",
    "            \"val_accs\": self.val_accs,\n",
    "            \"val_iso_accs\": self.val_iso_accs,\n",
    "            \"train_time\": train_time,\n",
    "            \"val_iso_stats\": self.val_iso_stats,\n",
    "            \"val_ab_stats\": self.val_ab_stats\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss, reporting_loss, printing_loss = 0, 0, 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            batch_index = i + 1\n",
    "            self.optimizer.zero_grad() # zero out gradients\n",
    "            \n",
    "            input, target_res, token_types, attn_mask = batch \n",
    "            # input, target_indices, target_res, token_types, attn_mask, masked_sequences = batch   \n",
    "            pred_logits = self.model(input, token_types, attn_mask) # get predictions for all antibiotics\n",
    "            ab_mask = target_res != -1 # (batch_size, num_ab), True if antibiotic is masked, False otherwise\n",
    "            \n",
    "            ab_indices = ab_mask.any(dim=0).nonzero().squeeze(-1).tolist() # list of indices of antibiotics present in the batch\n",
    "            losses = list()\n",
    "            for j in ab_indices: \n",
    "                mask = ab_mask[:, j] # (batch_size,), indicates which samples contain the antibiotic masked\n",
    "                # isolate the predictions and targets for the antibiotic\n",
    "                ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                ab_loss = self.ab_criterions[j](ab_pred_logits, ab_targets)\n",
    "                losses.append(ab_loss)\n",
    "            loss = sum(losses) / len(losses) # average loss over antibiotics\n",
    "            epoch_loss += loss\n",
    "            reporting_loss += loss\n",
    "            printing_loss += loss\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step() \n",
    "            if batch_index % self.report_every == 0:\n",
    "                self._report_loss_results(batch_index, reporting_loss)\n",
    "                reporting_loss = 0 \n",
    "                \n",
    "            if batch_index % self.print_progress_every == 0:\n",
    "                time_elapsed = time.gmtime(time.time() - time_ref) \n",
    "                self._print_loss_summary(time_elapsed, batch_index, printing_loss) \n",
    "                printing_loss = 0  \n",
    "        avg_epoch_loss = epoch_loss / self.num_batches\n",
    "        return avg_epoch_loss \n",
    "    \n",
    "    \n",
    "    def early_stopping(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "        \n",
    "            \n",
    "    def evaluate(self, loader: DataLoader, ds_obj):\n",
    "        self.model.eval()\n",
    "        # prepare evaluation statistics dataframes\n",
    "        ab_stats, iso_stats = self._init_eval_stats(ds_obj)\n",
    "        with torch.no_grad(): \n",
    "            ## Antibiotic tracking ##\n",
    "            ab_num = np.zeros((self.num_ab, 2)) # tracks the occurence for each antibiotic & resistance\n",
    "            ab_num_preds = np.zeros_like(ab_num) # tracks the number of predictions for each antibiotic & resistance\n",
    "            ab_num_correct = np.zeros_like(ab_num) # tracks the number of correct predictions for each antibiotic & resistance\n",
    "            ## General tracking ##\n",
    "            loss = 0\n",
    "            for i, batch in enumerate(loader):                \n",
    "                input, target_res, token_types, attn_mask = batch   \n",
    "                 \n",
    "                pred_logits = self.model(input, token_types, attn_mask) # get predictions for all antibiotics\n",
    "                pred_res = torch.where(pred_logits > 0, torch.ones_like(pred_logits), torch.zeros_like(pred_logits)) # logits -> 0/1 (S/R)\n",
    "                        \n",
    "                ab_mask = target_res >= 0 # (batch_size, num_ab), True if antibiotic is masked, False otherwise\n",
    "                iso_stats = self._update_pheno_stats(i, pred_res, target_res, ab_mask, iso_stats)\n",
    "                \n",
    "                ab_indices = ab_mask.any(dim=0).nonzero().squeeze(-1).tolist() # list of indices of antibiotics present in the batch\n",
    "                losses = list()\n",
    "                for j in ab_indices: \n",
    "                    mask = ab_mask[:, j] # (batch_size,)\n",
    "                    \n",
    "                    # isolate the predictions and targets for the antibiotic\n",
    "                    ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                    ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                    num_R = ab_targets.sum().item()\n",
    "                    num_S = ab_targets.shape[0] - num_R\n",
    "                    ab_num[j, :] += [num_S, num_R]\n",
    "                    \n",
    "                    ab_loss = self.ab_criterions[j](ab_pred_logits, ab_targets)\n",
    "                    losses.append(ab_loss)\n",
    "                    \n",
    "                    ab_pred_res = pred_res[mask, j]\n",
    "                    ab_num_correct[j, :] += self._get_num_correct(ab_pred_res, ab_targets)    \n",
    "                    ab_num_preds[j, :] += self._get_num_preds(ab_pred_res)\n",
    "                loss += sum(losses) / len(losses) # average loss over antibiotics\n",
    "                    \n",
    "        avg_loss = loss / len(loader)\n",
    "        \n",
    "        ab_stats = self._update_ab_eval_stats(ab_stats, ab_num, ab_num_preds, ab_num_correct)\n",
    "        iso_stats = self._calculate_iso_stats(iso_stats)\n",
    "        \n",
    "        acc = iso_stats['num_correct'].sum() / iso_stats['num_masked'].sum()\n",
    "        iso_acc = iso_stats['all_correct'].sum() / iso_stats.shape[0]\n",
    "\n",
    "        results = {\n",
    "            \"loss\": avg_loss, \n",
    "            \"acc\": acc,\n",
    "            \"iso_acc\": iso_acc,\n",
    "            \"ab_stats\": ab_stats,\n",
    "            \"iso_stats\": iso_stats,\n",
    "        }\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def _init_result_lists(self):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "        self.val_iso_accs = []\n",
    "        self.val_ab_stats = []\n",
    "        self.val_iso_stats = []\n",
    "        \n",
    "        \n",
    "    def _update_val_lists(self, results: dict):\n",
    "        self.val_losses.append(results[\"loss\"])\n",
    "        self.val_accs.append(results[\"acc\"])\n",
    "        self.val_iso_accs.append(results[\"iso_acc\"])\n",
    "        self.val_ab_stats.append(results[\"ab_stats\"])\n",
    "        self.val_iso_stats.append(results[\"iso_stats\"])\n",
    "    \n",
    "    \n",
    "    def _init_eval_stats(self, ds_obj):\n",
    "        ab_stats = pd.DataFrame(columns=[\n",
    "            'antibiotic', 'num_tot', 'num_S', 'num_R', 'num_pred_S', 'num_pred_R', \n",
    "            'num_correct', 'num_correct_S', 'num_correct_R',\n",
    "            'accuracy', 'sensitivity', 'specificity', 'precision', 'F1'\n",
    "        ])\n",
    "        ab_stats['antibiotic'] = self.antibiotics\n",
    "        ab_stats['num_tot'], ab_stats['num_S'], ab_stats['num_R'] = 0, 0, 0\n",
    "        ab_stats['num_pred_S'], ab_stats['num_pred_R'] = 0, 0\n",
    "        ab_stats['num_correct'], ab_stats['num_correct_S'], ab_stats['num_correct_R'] = 0, 0, 0\n",
    "        \n",
    "        iso_stats = ds_obj.ds_MM.drop(columns=['genotypes', 'phenotypes'])\n",
    "        iso_stats['num_masked'], iso_stats['num_masked_S'], iso_stats['num_masked_R'] = 0, 0, 0\n",
    "        iso_stats['num_correct'], iso_stats['correct_S'], iso_stats['correct_R'] = 0, 0, 0\n",
    "        iso_stats['sensitivity'], iso_stats['specificity'], iso_stats['accuracy'] = 0, 0, 0\n",
    "        iso_stats['all_correct'] = False  \n",
    "        return ab_stats, iso_stats\n",
    "    \n",
    "    \n",
    "    def _update_ab_eval_stats(self, ab_stats: pd.DataFrame, num, num_preds, num_correct):\n",
    "        for j in range(self.num_ab): \n",
    "            ab_stats.loc[j, 'num_tot'] = num[j, :].sum()\n",
    "            ab_stats.loc[j, 'num_S'], ab_stats.loc[j, 'num_R'] = num[j, 0], num[j, 1]\n",
    "            ab_stats.loc[j, 'num_pred_S'], ab_stats.loc[j, 'num_pred_R'] = num_preds[j, 0], num_preds[j, 1]\n",
    "            ab_stats.loc[j, 'num_correct'] = num_correct[j, :].sum()\n",
    "            ab_stats.loc[j, 'num_correct_S'], ab_stats.loc[j, 'num_correct_R'] = num_correct[j, 0], num_correct[j, 1]\n",
    "        ab_stats['accuracy'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct']/row['num_tot'] if row['num_tot'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['sensitivity'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct_R']/row['num_R'] if row['num_R'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['specificity'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct_S']/row['num_S'] if row['num_S'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['precision'] = ab_stats.apply(\n",
    "            lambda row: row['num_correct_R']/row['num_pred_R'] if row['num_pred_R'] > 0 else np.nan, axis=1)\n",
    "        ab_stats['F1'] = ab_stats.apply(\n",
    "            lambda row: 2*row['precision']*row['sensitivity']/(row['precision']+row['sensitivity']) \n",
    "            if row['precision'] > 0 and row['sensitivity'] > 0 else np.nan, axis=1)\n",
    "        return ab_stats\n",
    "    \n",
    "    \n",
    "    def _get_num_correct(self, pred_res: torch.Tensor, target_res: torch.Tensor):\n",
    "        eq = torch.eq(pred_res, target_res)\n",
    "        num_correct_S = eq[target_res == 0].sum().item()\n",
    "        num_correct_R = eq[target_res == 1].sum().item()\n",
    "        return [num_correct_S, num_correct_R]\n",
    "    \n",
    "    \n",
    "    def _get_num_preds(self, pred_res: torch.Tensor):\n",
    "        num_pred_S = (pred_res == 0).sum().item()\n",
    "        num_pred_R = (pred_res == 1).sum().item()\n",
    "        return [num_pred_S, num_pred_R]\n",
    "    \n",
    "    \n",
    "    def _update_pheno_stats(self, batch_idx, pred_res: torch.Tensor, target_res: torch.Tensor, \n",
    "                          ab_mask: torch.Tensor, iso_stats: pd.DataFrame):\n",
    "        for i in range(pred_res.shape[0]): \n",
    "            iso_ab_mask = ab_mask[i]\n",
    "            df_idx = batch_idx * self.batch_size + i # index of the isolate in the combined dataset\n",
    "            \n",
    "            # counts\n",
    "            num_masked_tot = iso_ab_mask.sum().item()\n",
    "            num_masked_R = target_res[i][iso_ab_mask].sum().item()\n",
    "            num_masked_S = num_masked_tot - num_masked_R\n",
    "            \n",
    "            # statistics            \n",
    "            iso_target_res = target_res[i][iso_ab_mask]\n",
    "            eq = torch.eq(pred_res[i][iso_ab_mask], iso_target_res)\n",
    "            num_correct_R = eq[iso_target_res == 1].sum().item()\n",
    "            num_correct_S = eq[iso_target_res == 0].sum().item()\n",
    "            num_correct = num_correct_S + num_correct_R\n",
    "            all_correct = eq.all().item()\n",
    "            \n",
    "            data = {\n",
    "                'num_masked': num_masked_tot, 'num_masked_S': num_masked_S, 'num_masked_R': num_masked_R, \n",
    "                'num_correct': num_correct, 'correct_S': num_correct_S, 'correct_R': num_correct_R,\n",
    "                'all_correct': all_correct\n",
    "            }\n",
    "            iso_stats.loc[df_idx, data.keys()] = data.values()\n",
    "                          \n",
    "        return iso_stats\n",
    "    \n",
    "    def _calculate_iso_stats(self, iso_stats: pd.DataFrame): \n",
    "        iso_stats['accuracy'] = iso_stats['num_correct'] / iso_stats['num_masked']\n",
    "        iso_stats['sensitivity'] = iso_stats.apply(\n",
    "            lambda row: row['correct_R']/row['num_masked_R'] if row['num_masked_R'] > 0 else np.nan, axis=1\n",
    "        )\n",
    "        iso_stats['specificity'] = iso_stats.apply(\n",
    "            lambda row: row['correct_S']/row['num_masked_S'] if row['num_masked_S'] > 0 else np.nan, axis=1\n",
    "        )\n",
    "        \n",
    "        return iso_stats\n",
    "        \n",
    "     \n",
    "    def _init_wandb(self):\n",
    "        print(\"Initializing wandb...\")\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"hidden_dim\": self.model.hidden_dim,\n",
    "                \"num_layers\": self.model.num_layers,\n",
    "                \"num_heads\": self.model.num_heads,\n",
    "                \"emb_dim\": self.model.emb_dim,\n",
    "                'ff_dim': self.model.ff_dim,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"mask_prob\": self.mask_prob,\n",
    "                \"num_known_ab\": self.num_known_ab,\n",
    "                \"max_seq_len\": self.model.max_seq_len,\n",
    "                \"vocab_size\": len(self.vocab),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "                \"num_antibiotics\": self.num_ab,\n",
    "                \"antibiotics\": self.antibiotics,\n",
    "                \"train_size\": self.train_size,\n",
    "                \"random_state\": self.random_state,\n",
    "                'val_share': self.val_share,\n",
    "                \"val_size\": self.val_size,\n",
    "                \"is_pretrained\": self.model.is_pretrained,\n",
    "                # \"early_stopping_patience\": self.patience,\n",
    "                # \"dropout_prob\": self.model.dropout_prob,\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/live_loss\", step_metric=\"batch\")\n",
    "        self.wandb_run.define_metric(\"Losses/train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_iso_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_iso_acc\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "     \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"Losses/train_loss\": self.losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            # \"Losses/val_geno_loss\": self.val_geno_losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "            \"Accuracies/val_iso_acc\": self.val_iso_accs[-1],\n",
    "            # \"Accuracies/val_geno_acc\": self.val_geno_accs[-1],\n",
    "            # \"Accuracies/val_geno_iso_acc\": self.val_geno_iso_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)\n",
    "    \n",
    "        \n",
    "    def _report_loss_results(self, batch_index, tot_loss):\n",
    "        avg_loss = tot_loss / self.report_every\n",
    "        \n",
    "        global_step = self.current_epoch * self.num_batches + batch_index # global step, total #batches seen\n",
    "        self.wandb_run.log({\"batch\": global_step, \"Losses/live_loss\": avg_loss})\n",
    "    \n",
    "        \n",
    "    def _print_loss_summary(self, time_elapsed, batch_index, tot_loss):\n",
    "        progress = batch_index / self.num_batches\n",
    "        mlm_loss = tot_loss / self.print_progress_every\n",
    "          \n",
    "        s = f\"{time.strftime('%H:%M:%S', time_elapsed)}\" \n",
    "        s += f\" | Epoch: {self.current_epoch+1}/{self.epochs} | {batch_index}/{self.num_batches} ({progress:.2%}) | \"\\\n",
    "                f\"Loss: {mlm_loss:.4f}\"\n",
    "        print(s)\n",
    "    \n",
    "    \n",
    "    def save_model(self, savepath: Path):\n",
    "        print(type(self.best_model_state))\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def load_model(self, savepath: Path):\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        self.model.to(device)\n",
    "        print(\"Model loaded\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fine-tuning BERT on the multimodal dataset \n",
      "\n",
      "======================================================================\n",
      "Loading model from c:\\Users\\jespe\\Documents\\GitHub_local\\ARFusion\\results\\MM\\test_run_with_ft\\model_state.pt\n",
      "Model loaded\n",
      "======================================================================\n",
      "Model summary:\n",
      "======================================================================\n",
      "Is pre-trained: No\n",
      "Embedding dim: 256\n",
      "Feed-forward dim: 256\n",
      "Hidden dim: 256\n",
      "Number of heads: 4\n",
      "Number of encoder layers: 6\n",
      "Dropout probability: 10%\n",
      "Max sequence length: 40\n",
      "Vocab size: 1,546\n",
      "Number of parameters: 3,176,202\n",
      "======================================================================\n",
      "Trainer summary:\n",
      "======================================================================\n",
      "Device: cuda (NVIDIA GeForce RTX 3080)\n",
      "Training dataset size: 5,473\n",
      "Batch size: 16\n",
      "Number of batches: 342\n",
      "Number of antibiotics: 18\n",
      "Antibiotics: ['CIP', 'NAL', 'MFX', 'AMC', 'NOR', 'AMP', 'CTX', 'TOB', 'NET', 'CRO', 'FEP', 'CAZ', 'OFX', 'TZP', 'AMX', 'GEN', 'LVX', 'PIP']\n",
      "CV split: 85% train | 15% val\n",
      "Mask probability (phenotypes): 25%\n",
      "Number of epochs: 100\n",
      "Early stopping patience: 3\n",
      "Learning rate: 1e-05\n",
      "Weight decay: 0.01\n",
      "======================================================================\n",
      "Initializing wandb...\n",
      "Epoch 1/100\n",
      "Epoch completed in 0.1 min | Loss: 0.4290\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.3226 | Accuracy 86.69% | Isolate accuracy 79.40%\n",
      "Elapsed time: 00:00:11\n",
      "======================================================================\n",
      "Epoch 2/100\n",
      "Epoch completed in 0.1 min | Loss: 0.3088\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.2707 | Accuracy 89.80% | Isolate accuracy 83.64%\n",
      "Elapsed time: 00:00:21\n",
      "======================================================================\n",
      "Epoch 3/100\n",
      "Epoch completed in 0.1 min | Loss: 0.2643\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.2500 | Accuracy 90.49% | Isolate accuracy 84.37%\n",
      "Elapsed time: 00:00:32\n",
      "======================================================================\n",
      "Epoch 4/100\n",
      "Epoch completed in 0.1 min | Loss: 0.2491\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.2311 | Accuracy 91.07% | Isolate accuracy 84.99%\n",
      "Elapsed time: 00:00:44\n",
      "======================================================================\n",
      "Epoch 5/100\n",
      "Epoch completed in 0.1 min | Loss: 0.2367\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.2183 | Accuracy 91.99% | Isolate accuracy 86.23%\n",
      "Elapsed time: 00:00:55\n",
      "======================================================================\n",
      "Epoch 6/100\n",
      "Epoch completed in 0.1 min | Loss: 0.2214\n",
      "Evaluating on validation set...\n",
      "Val loss: 0.2077 | Accuracy 92.74% | Isolate accuracy 87.58%\n",
      "Elapsed time: 00:01:06\n",
      "======================================================================\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m tuner\u001b[38;5;241m.\u001b[39mprint_model_summary()\n\u001b[0;32m     47\u001b[0m tuner\u001b[38;5;241m.\u001b[39mprint_trainer_summary()\n\u001b[1;32m---> 48\u001b[0m ft_results \u001b[38;5;241m=\u001b[39m tuner()\n",
      "Cell \u001b[1;32mIn[12], line 110\u001b[0m, in \u001b[0;36mMMBertFineTuner.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    109\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 110\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch) \u001b[38;5;66;03m# returns loss, averaged over batches\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mepoch_start_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 204\u001b[0m, in \u001b[0;36mMMBertFineTuner.train\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    201\u001b[0m reporting_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    202\u001b[0m printing_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m--> 204\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep() \n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jespe\\miniconda3\\envs\\ARFusion\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jespe\\miniconda3\\envs\\ARFusion\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import get_split_indices, export_results\n",
    "\n",
    "config_ft = config['fine_tune']\n",
    "print(\"\\n Fine-tuning BERT on the multimodal dataset \\n\")\n",
    "assert config_ft['ds_path'], \"Please specify the path to the pre-processed NCBI dataset\"\n",
    "ds_NCBI = pd.read_pickle(config_ft['ds_path'])\n",
    "ds_MM = ds_NCBI[ds_NCBI['num_ab'] > 0].reset_index(drop=True)\n",
    "pad_token = specials['PAD']\n",
    "ds_MM.fillna(pad_token, inplace=True)\n",
    "pad_idx = vocab[pad_token]\n",
    "\n",
    "os.environ['WANDB_MODE'] = config['wandb_mode']\n",
    "\n",
    "train_indices, val_indices = get_split_indices(\n",
    "    ds_MM.shape[0], \n",
    "    val_share=config_ft['val_share'], \n",
    "    random_state=config['random_state']\n",
    ")\n",
    "ds_ft_train = MMFinetuneDataset(\n",
    "    df_MM=ds_MM.iloc[train_indices],\n",
    "    vocab=vocab,\n",
    "    antibiotics=antibiotics,\n",
    "    specials=specials,\n",
    "    max_seq_len=max_seq_len,\n",
    "    mask_prob=config_ft['mask_prob'],\n",
    "    num_known_ab=config_ft['num_known_ab'],\n",
    "    random_state=config_ft['random_state']\n",
    ")\n",
    "ds_ft_val = MMFinetuneDataset(\n",
    "    df_MM=ds_MM.iloc[val_indices],\n",
    "    vocab=vocab,\n",
    "    antibiotics=antibiotics,\n",
    "    specials=specials,\n",
    "    max_seq_len=max_seq_len,\n",
    "    mask_prob=config_ft['mask_prob'],\n",
    "    num_known_ab=config_ft['num_known_ab'],\n",
    "    random_state=config_ft['random_state']\n",
    ")\n",
    "# set bert in pheno_only mode\n",
    "bert = BERT(config, vocab_size, max_seq_len, len(antibiotics), pad_idx, pheno_only=True)\n",
    "    \n",
    "tuner = MMBertFineTuner(\n",
    "    config=config,\n",
    "    model=bert,\n",
    "    antibiotics=antibiotics,\n",
    "    train_set=ds_ft_train,\n",
    "    val_set=ds_ft_val,\n",
    "    results_dir=results_dir,\n",
    ")\n",
    "tuner.load_model(config_ft['model_path'])\n",
    "tuner.print_model_summary()\n",
    "tuner.print_trainer_summary()\n",
    "ft_results = tuner()\n",
    "export_results(ft_results, results_dir / 'pt_results.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARFusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
