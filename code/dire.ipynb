{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# user-defined modules\n",
    "from trainers import BertMLMTrainer\n",
    "\n",
    "# user-defined functions\n",
    "from construct_vocab import construct_pheno_vocab\n",
    "from utils import get_split_indices\n",
    "from data_preprocessing import preprocess_TESSy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080\n",
      "base directory: C:\\Users\\jespe\\Documents\\GitHub_local\\ARFusion\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "os.environ['WANDB_MODE'] = 'disabled' # 'dryrun' or 'run' or 'offline' or 'disabled' or 'online'\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Using CPU\") \n",
    "    \n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "RESULTS_DIR = Path(os.path.join(BASE_DIR / \"results\" / \"temp\"))\n",
    "os.chdir(BASE_DIR)\n",
    "print(\"base directory:\", BASE_DIR)\n",
    "\n",
    "config_path = BASE_DIR / \"config_pheno.yaml\"\n",
    "with open(config_path, \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.emb_dim) \n",
    "        # self.token_type_emb = nn.Embedding(self.vocab_size, self.emb_dim) \n",
    "        self.position_emb = nn.Embedding(self.vocab_size, self.emb_dim) \n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        # input_tensor: (batch_size, seq_len)\n",
    "        # token_type_ids: (batch_size, seq_len)\n",
    "        # position_ids: (batch_size, seq_len)\n",
    "        \n",
    "        seq_len = input_tensor.size(-1)\n",
    "        \n",
    "        pos_tensor = self.numeric_position(seq_len, input_tensor)\n",
    "        # token_type not relevant for unimodal data\n",
    "        # token_type_tensor = torch.zeros_like(input_tensor).to(device) # (batch_size, seq_len)\n",
    "        # token_type_tensor[:, (seq_len//2 + 1):] = 1 # here, we assume that the sentence is split in half\n",
    "        \n",
    "        token_emb = self.token_emb(input_tensor) # (batch_size, seq_len, emb_dim)\n",
    "        # token_type_emb = self.token_type_emb(token_type_tensor) # (batch_size, seq_len, emb_dim)\n",
    "        position_emb = self.position_emb(pos_tensor) # (batch_size, seq_len, emb_dim)\n",
    "        \n",
    "        # emb = token_emb + token_type_emb + position_emb\n",
    "        emb = token_emb + position_emb\n",
    "        emb = self.dropout(emb)\n",
    "        emb = self.layer_norm(emb) \n",
    "        return emb\n",
    "                \n",
    "    def numeric_position(self, dim, input_tensor): # input_tensor: (batch_size, seq_len)\n",
    "        # dim is the length of the sequence\n",
    "        position_ids = torch.arange(dim, dtype=torch.long, device=device) # create tensor of [0, 1, 2, ..., dim-1]\n",
    "        return position_ids.expand_as(input_tensor) # expand to (batch_size, seq_len)\n",
    "    \n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.head_dim = self.emb_dim // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.emb_dim, f\"Embedding dimension must be divisible by number of heads, got {self.emb_dim} and {self.num_heads}\"\n",
    "        \n",
    "        self.q = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.k = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.v = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "    \n",
    "    def forward(self, input_emb: torch.Tensor, attn_mask:torch.Tensor = None):\n",
    "        B, L, D = input_emb.size() # (L=batch_size, L=seq_len, D=emb_dim)\n",
    "        \n",
    "        # project input embeddings to query, key, value, then split into num_heads, reducing the embedding dimension\n",
    "        query = self.q(input_emb).view(B, L, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, L, head_dim)\n",
    "        key = self.k(input_emb).view(B, L, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, L, head_dim)\n",
    "        value = self.v(input_emb).view(B, L, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, L, head_dim)\n",
    "        \n",
    "        scale_factor = query.size(-1) ** 0.5\n",
    "        attn_scores = torch.matmul(query, key.transpose(-1, -2)) / scale_factor # (B, num_heads, L, L)\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill_(~attn_mask, -1e9) if attn_mask is not None else attn_scores \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1) # (B, num_heads, L, L)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn = torch.matmul(attn_weights, value) # (B, num_heads, L, head_dim)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, L, D) # (B, L, num_heads, head_dim) -> (B, L, D), concatenate the heads\n",
    "        \n",
    "        return attn\n",
    "        \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.hidden_dim),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, self.emb_dim),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        \n",
    "    def forward(self, input_emb: torch.Tensor, attn_mask: torch.Tensor = None):\n",
    "        x = input_emb\n",
    "        attn = self.attention(x, attn_mask)\n",
    "        x = x + attn\n",
    "        x = self.layer_norm(x)\n",
    "        res = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = x + res\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PhenoBERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, vocab_size, antibiotics):\n",
    "        super(PhenoBERT, self).__init__()\n",
    "                \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = None # Can be set later\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.embedding = JointEmbedding(config, vocab_size)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(config) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.token_prediction_layer = nn.Linear(self.emb_dim, self.vocab_size) # MLM task\n",
    "        self.softmax = nn.LogSoftmax(dim=-1) # log softmax improves numerical stability, we use NLLLoss later\n",
    "        if antibiotics:\n",
    "            self.classification_layer = [AbPredictor(self.emb_dim).to(device) for _ in range(len(antibiotics))] # classification task\n",
    "        \n",
    "    def forward(self, input_tensor, attn_mask): \n",
    "        embedded = self.embedding(input_tensor)\n",
    "        for layer in self.encoder:\n",
    "            embedded = layer(embedded, attn_mask)\n",
    "        encoded = embedded # ouput of the BERT Encoder\n",
    "        \n",
    "        if self.classification_layer: # ASSUMES MLM AND CLASSIFICATION ARE NOT DONE AT THE SAME TIME\n",
    "            cls_token = encoded[:, 0, :] # (batch_size, emb_dim)\n",
    "            predictions = torch.cat([net(cls_token) for net in self.classification_layer], dim=1) # (batch_size, num_ab)\n",
    "            return predictions\n",
    "        else:\n",
    "            token_prediction = self.token_prediction_layer(encoded) # (batch_size, seq_len, vocab_size)\n",
    "            return self.softmax(token_prediction)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "class AbPredictor(nn.Module): # predicts resistance or susceptibility for an antibiotic\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AbPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, 1), # binary classification\n",
    "        )\n",
    "        # self.classifiers = nn.ModuleList(\n",
    "        #     [nn.Sequential(\n",
    "        #         nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.LayerNorm(self.hidden_dim),\n",
    "        #         nn.Linear(self.hidden_dim, 2), # one value for R and one for S\n",
    "        #     ) for _ in range(self.num_ab)]\n",
    "        # )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X is the CLS token of the BERT model\n",
    "        return self.classifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class PhenotypeDataset(Dataset):      \n",
    "    # df column names\n",
    "    INDICES_MASKED = 'indices_masked' # input to BERT, token indices of the masked sequence\n",
    "    TARGET_RESISTANCES = 'target_resistances' # resistance of the target antibiotics, what we want to predict\n",
    "    TOKEN_MASK = 'token_mask' # True if token is masked, False otherwise\n",
    "    AB_MASK = 'ab_mask' # True if antibiotic is masked, False otherwise\n",
    "    # # if original text is included\n",
    "    ORIGINAL_SEQUENCE = 'original_sequence'\n",
    "    MASKED_SEQUENCE = 'masked_sequence'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ds: pd.DataFrame,\n",
    "                 vocab,\n",
    "                 antibiotics: list,\n",
    "                 specials: dict,\n",
    "                 max_seq_len: int,\n",
    "                 base_dir: Path,\n",
    "                 include_sequences: bool = False,\n",
    "                 random_state: int = 42,\n",
    "                 ):\n",
    "        \n",
    "        os.chdir(base_dir)\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        self.ds = ds.reset_index(drop=True) \n",
    "        self.num_samples = self.ds.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics)\n",
    "        self.ab_to_idx = {ab: i for i, ab in enumerate(self.antibiotics)}\n",
    "        self.enc_res = {'S': 0, 'R': 1}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.CLS = specials['CLS']\n",
    "        self.PAD = specials['PAD']\n",
    "        self.MASK = specials['MASK']\n",
    "        self.UNK = specials['UNK']\n",
    "        self.special_tokens = specials.values()\n",
    "        self.max_seq_len = max_seq_len\n",
    "           \n",
    "        self.include_sequences = include_sequences\n",
    "        if self.include_sequences:\n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_RESISTANCES, self.TOKEN_MASK, self.AB_MASK,\n",
    "                            self.ORIGINAL_SEQUENCE, self.MASKED_SEQUENCE]\n",
    "        else: \n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_RESISTANCES, self.TOKEN_MASK, self.AB_MASK]        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        \n",
    "        input = torch.tensor(item[self.INDICES_MASKED], dtype=torch.long, device=device)\n",
    "        target_res = torch.tensor(item[self.TARGET_RESISTANCES], dtype=torch.float32, device=device)\n",
    "        token_mask = torch.tensor(item[self.TOKEN_MASK], dtype=torch.bool, device=device)\n",
    "        ab_mask = torch.tensor(item[self.AB_MASK], dtype=torch.bool, device=device)\n",
    "        attn_mask = (input != self.vocab[self.PAD]).unsqueeze(0).unsqueeze(1) # one dim for batch, one for heads\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            original_sequence = item[self.ORIGINAL_SEQUENCE]\n",
    "            masked_sequence = item[self.MASKED_SEQUENCE]\n",
    "            return input, target_res, token_mask, ab_mask, attn_mask, original_sequence, masked_sequence\n",
    "        else:\n",
    "            return input, target_res, token_mask, ab_mask, attn_mask\n",
    "\n",
    "       \n",
    "    def prepare_dataset(self, mask_prob: float = 0.15): # will be called at the start of each epoch (dynamic masking)\n",
    "        sequences, masked_sequences, target_resistances, token_masks, ab_masks = self._construct_masked_sequences(mask_prob)\n",
    "        \n",
    "        indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            rows = zip(indices_masked, target_resistances, token_masks, ab_masks, sequences, masked_sequences)\n",
    "        else:\n",
    "            rows = zip(indices_masked, target_resistances, token_masks, ab_masks)\n",
    "        self.df = pd.DataFrame(rows, columns=self.columns)\n",
    "\n",
    "    \n",
    "    def _encode_sequence(self, seq: list):\n",
    "        dict = {ab: res for ab, res in [token.split('_') for token in seq]}\n",
    "        indices = [self.ab_to_idx[ab] for ab in dict.keys()]\n",
    "        resistances = [self.enc_res[res] for res in dict.values()]\n",
    "        \n",
    "        return indices, resistances\n",
    "    \n",
    "    \n",
    "    def _construct_masked_sequences(self, mask_prob: float):  \n",
    "        # RoBERTa: 80% -> [MASK], 10% -> original token, 10% -> random token\n",
    "        self.mask_prob = mask_prob\n",
    "        sequences = deepcopy(self.ds['phenotypes'].tolist())\n",
    "        masked_sequences = list()\n",
    "        # all_target_indices = list()\n",
    "        all_target_resistances = list()\n",
    "        ab_masks = list() # will be applied to the output of the model, i.e. (batch_size, num_ab)\n",
    "        token_masks = list() # will be applied to the the sequence itself, i.e. (batch_size, seq_len)\n",
    "        for seq in deepcopy(sequences):\n",
    "            seq_len = len(seq)\n",
    "            # target_indices, target_resistances = self._encode_sequence(seq) # if we don't want to indicate masking here, we \n",
    "            # all_target_indices.append(target_indices)                       # encode the whole sequence, and use a token mask\n",
    "            # all_target_resistances.append(target_resistances)\n",
    "            \n",
    "            token_mask = [False] * seq_len # indicates which tokens in the sequence are masked, includes all tokens\n",
    "            ab_mask = [False] * self.num_ab # will indicate which antibiotics are masked, indexed in the order of self.antibiotics\n",
    "            target_resistances = [-1]*self.num_ab # -1 indicates padding, will indicate the target resistance, same indexing as ab_mask\n",
    "            tokens_masked = 0\n",
    "            for i in range(seq_len):\n",
    "                if np.random.rand() < self.mask_prob: \n",
    "                    ab, res = seq[i].split('_')\n",
    "                    ab_idx = self.ab_to_idx[ab]\n",
    "                    tokens_masked += 1\n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8: \n",
    "                        seq[i] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        j = np.random.randint(self.vocab_size-self.num_ab*2, self.vocab_size) # select random pheno token\n",
    "                        seq[i] = self.vocab.lookup_token(j)\n",
    "                    # else: do nothing, since r > 0.9 and we keep the same token\n",
    "                    token_mask[i] = True\n",
    "                    ab_mask[ab_idx] = True # indicate which antibiotic is masked at this position\n",
    "                    target_resistances[ab_idx] = self.enc_res[res] # the target resistance of the antibiotic\n",
    "            if tokens_masked == 0: # mask at least one token\n",
    "                i = np.random.randint(seq_len)\n",
    "                ab, res = seq[i].split('_')\n",
    "                ab_idx = self.ab_to_idx[ab]\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8: \n",
    "                    seq[i] = self.MASK\n",
    "                elif r < 0.9:\n",
    "                    j = np.random.randint(self.vocab_size-self.num_ab*2, self.vocab_size) # select random token, excluding specials\n",
    "                    seq[i] = self.vocab.lookup_token(j)\n",
    "                # else: do nothing, since r > 0.9 and we keep the same token\n",
    "                token_mask[i] = True\n",
    "                ab_mask[ab_idx] = True # indicate which antibiotic is masked at this position\n",
    "                target_resistances[ab_idx] = self.enc_res[res] # the target resistance of the antibiotic\n",
    "                \n",
    "            masked_sequences.append(seq)\n",
    "            token_masks.append(token_mask)\n",
    "            ab_masks.append(ab_mask)\n",
    "            all_target_resistances.append(target_resistances)\n",
    "        \n",
    "        for i in range(len(sequences)):\n",
    "            token_masks[i] = 5*[False] + token_masks[i]\n",
    "            seq_start = [self.CLS, \n",
    "                         str(self.ds['year'].iloc[i]), \n",
    "                         self.ds['country'].iloc[i], \n",
    "                         self.ds['gender'].iloc[i], \n",
    "                         str(int(self.ds['age'].iloc[i]))]\n",
    "            \n",
    "            sequences[i][:0] = seq_start\n",
    "            masked_sequences[i][:0] = seq_start\n",
    "            # all_target_indices[i][:0] = [-1]*5 \n",
    "            \n",
    "            seq_len = len(sequences[i])\n",
    "            if seq_len < self.max_seq_len:\n",
    "                sequences[i].extend([self.PAD] * (self.max_seq_len - seq_len))\n",
    "                masked_sequences[i].extend([self.PAD] * (self.max_seq_len - seq_len))\n",
    "                token_masks[i].extend([False] * (self.max_seq_len - seq_len))\n",
    "            # the antibiotic-specific lists should always be of length num_ab\n",
    "            pheno_len = len(all_target_resistances[i])\n",
    "            all_target_resistances[i].extend([-1] * (self.num_ab - pheno_len))\n",
    "            # all_target_indices[i].extend([-1] * (self.num_ab - pheno_len)) # -1 indicates padding\n",
    "            # ab_mask is defined with correct length\n",
    "                \n",
    "        return sequences, masked_sequences, all_target_resistances, token_masks, ab_masks  \n",
    "    \n",
    "    \n",
    "    def reconstruct_sequence(self, seq_from_batch):\n",
    "        tuple_len = len(seq_from_batch[0])\n",
    "        sequences = list()\n",
    "        for j in range(tuple_len):\n",
    "            sequence = list()\n",
    "            for i in range(self.max_seq_len):\n",
    "                sequence.append(seq_from_batch[i][j])\n",
    "            sequences.append(sequence)\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############################################ Trainer for MLM task ############################################\n",
    "\n",
    "def get_num_correct(pred_res: torch.Tensor, target_res: torch.Tensor):\n",
    "    num_correct = torch.eq(pred_res, target_res).sum().item()\n",
    "    return num_correct\n",
    "\n",
    "\n",
    "def get_num_correct_seq(pred_res: torch.Tensor, target_res: torch.Tensor, \n",
    "                        token_mask: torch.Tensor, ab_mask: torch.Tensor):\n",
    "    num_correct = 0\n",
    "    for i in range(pred_res.shape[0]): # for each sequence\n",
    "        sample_ab_mask = ab_mask[i]\n",
    "        sample_target_res = target_res[i][sample_ab_mask]\n",
    "        sample_pred_res = pred_res[i][sample_ab_mask]\n",
    "        eq = torch.eq(sample_pred_res, sample_target_res)\n",
    "        num_correct += eq.all().sum().item() \n",
    "    return num_correct\n",
    "\n",
    "\n",
    "class BertPhenoTrainer(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config: dict,\n",
    "                 model: PhenoBERT,\n",
    "                 antibiotics: list, # list of antibiotics in the dataset\n",
    "                 train_set,\n",
    "                 val_set,\n",
    "                 test_set, # can be None\n",
    "                 results_dir: Path = None,\n",
    "                 ):\n",
    "        super(BertPhenoTrainer, self).__init__()\n",
    "        \n",
    "        self.random_state = config[\"random_state\"]\n",
    "        torch.manual_seed(self.random_state)\n",
    "        torch.cuda.manual_seed(self.random_state)\n",
    "        \n",
    "        self.model = model\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics) \n",
    "        self.train_set, self.train_size = train_set, len(train_set)\n",
    "        self.train_size = len(self.train_set)      \n",
    "        self.model.max_seq_len = self.train_set.max_seq_len \n",
    "        self.val_set, self.val_size = val_set, len(val_set)\n",
    "        if test_set:\n",
    "            self.test_set, self.test_size = test_set, len(test_set)\n",
    "        self.split = config[\"split\"]\n",
    "        self.project_name = config[\"project_name\"]\n",
    "        self.wandb_name = config[\"name\"] if config[\"name\"] else datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "         \n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.patience = config[\"early_stopping_patience\"]\n",
    "        self.save_model = config[\"save_model\"] if config[\"save_model\"] else False\n",
    "        \n",
    "        self.do_testing = config[\"do_testing\"] if config[\"do_testing\"] else False\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        \n",
    "        self.mask_prob = config[\"mask_prob\"]\n",
    "        self.criterions = [nn.BCEWithLogitsLoss() for _ in range(self.num_ab)] # the list is so that we can introduce individual weights\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.scheduler = None\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.9)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.98)\n",
    "                 \n",
    "        self.current_epoch = 0\n",
    "        self.report_every = config[\"report_every\"] if config[\"report_every\"] else 100\n",
    "        self.print_progress_every = config[\"print_progress_every\"] if config[\"print_progress_every\"] else 1000\n",
    "        self._splitter_size = 70\n",
    "        self.results_dir = results_dir\n",
    "        os.makedirs(self.results_dir) if not os.path.exists(self.results_dir) else None\n",
    "        \n",
    "        \n",
    "    def print_model_summary(self):        \n",
    "        print(\"Model summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Embedding dim: {self.model.emb_dim}\")\n",
    "        print(f\"Hidden dim: {self.model.hidden_dim}\")\n",
    "        print(f\"Number of heads: {self.model.num_heads}\")\n",
    "        print(f\"Number of encoder layers: {self.model.num_layers}\")\n",
    "        print(f\"Max sequence length: {self.model.max_seq_len}\")\n",
    "        print(f\"Vocab size: {len(self.train_set.vocab):,}\")\n",
    "        print(f\"Number of parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "    \n",
    "    def print_trainer_summary(self):\n",
    "        print(\"Trainer summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "        print(f\"Training dataset size: {self.train_size:,}\")\n",
    "        print(f\"Number of antibiotics: {self.num_ab}\")\n",
    "        print(f\"Antibiotics: {self.antibiotics}\")\n",
    "        print(f\"Train-val-test split {self.split[0]:.0%} - {self.split[1]:.0%} - {self.split[2]:.0%}\")\n",
    "        print(f\"Will test? {'Yes' if self.do_testing else 'No'}\")\n",
    "        print(f\"Mask probability: {self.mask_prob:.0%}\")\n",
    "        print(f\"Number of epochs: {self.epochs}\")\n",
    "        print(f\"Early stopping patience: {self.patience}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Number of batches: {self.num_batches:,}\")\n",
    "        print(f\"Dropout probability: {self.model.dropout_prob:.0%}\")\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Weight decay: {self.weight_decay}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def __call__(self):      \n",
    "        self.wandb_run = self._init_wandb()\n",
    "        self.val_set.prepare_dataset(mask_prob=self.mask_prob) \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        if self.do_testing:\n",
    "            self.test_set.prepare_dataset(mask_prob=self.mask_prob) \n",
    "            self.test_loader = DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_seq_accuracies = []\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            self.model.train()\n",
    "            # Dynamic masking: New mask for training set each epoch\n",
    "            self.train_set.prepare_dataset(mask_prob=self.mask_prob)\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            loss = self.train(self.current_epoch) # returns loss, averaged over batches\n",
    "            self.losses.append(loss) \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            # print(\"Evaluating on training set...\")\n",
    "            # _, train_acc = self.evaluate(self.train_loader)\n",
    "            # self.train_accuracies.append(train_acc)\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_loss, val_acc, val_seq_acc = self.evaluate(self.val_loader)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            self.val_seq_accuracies.append(val_seq_acc)\n",
    "            self._report_epoch_results()\n",
    "            early_stop = self.early_stopping()\n",
    "            if early_stop:\n",
    "                print(f\"Early stopping at epoch {self.current_epoch+1} with validation loss {self.val_losses[-1]:.3f}\")\n",
    "                s = f\"Best validation loss {self.best_val_loss:.3f}\"\n",
    "                s += f\" | Validation accuracy {self.val_accuracies[self.best_epoch]:.2%}\"\n",
    "                s += f\" | Validation sequence accuracy {self.val_seq_accuracies[self.best_epoch]:.2%}\"\n",
    "                s += f\" at epoch {self.best_epoch+1}\"\n",
    "                print(s)\n",
    "                self.wandb_run.log({\"Losses/final_val_loss\": self.best_val_loss, \n",
    "                           \"Accuracies/final_val_acc\":self.val_accuracies[self.best_epoch],\n",
    "                           \"Accuracies/final_val_seq_acc\": self.val_seq_accuracies[self.best_epoch],\n",
    "                           \"final_epoch\": self.best_epoch+1})\n",
    "                print(\"=\"*self._splitter_size)\n",
    "                self.model.load_state_dict(self.best_model_state) \n",
    "                self.current_epoch = self.best_epoch\n",
    "                break\n",
    "            self.scheduler.step() if self.scheduler else None\n",
    "        \n",
    "        if not early_stop:    \n",
    "            self.wandb_run.log({\"Losses/final_val_loss\": self.val_losses[-1], \n",
    "                    \"Accuracies/final_val_acc\":self.val_accuracies[-1],\n",
    "                    \"Accuracies/final_val_seq_acc\": self.val_seq_accuracies[-1],\n",
    "                    \"final_epoch\": self.current_epoch+1})\n",
    "        self.save_model(self.results_dir / \"model_state.pt\") if self.save_model else None\n",
    "        train_time = (time.time() - start_time)/60\n",
    "        self.wandb_run.log({\"Training time (min)\": train_time})\n",
    "        disp_time = f\"{train_time//60:.0f}h {train_time % 60:.1f} min\" if train_time > 60 else f\"{train_time:.1f} min\"\n",
    "        print(f\"Training completed in {disp_time}\")\n",
    "        if not early_stop:\n",
    "            s = f\"Final validation loss {self.val_losses[-1]:.3f}\"\n",
    "            s += f\" | Final validation accuracy {self.val_accuracies[-1]:.2%}\"\n",
    "            s += f\" | Final validation sequence accuracy {self.val_seq_accuracies[-1]:.2%}\"\n",
    "            print(s)\n",
    "        \n",
    "        if self.do_testing:\n",
    "            print(\"Evaluating on test set...\")\n",
    "            self.test_loss, self.test_acc, test_seq_acc = self.evaluate(self.test_loader)\n",
    "            self.wandb_run.log({\"Losses/test_loss\": self.test_loss, \n",
    "                                \"Accuracies/test_acc\": self.test_acc,\n",
    "                                \"Accuracies/test_seq_acc\": test_seq_acc})\n",
    "        self._visualize_losses(savepath=self.results_dir / \"losses.png\")\n",
    "        self._visualize_accuracy(savepath=self.results_dir / \"accuracy.png\")\n",
    "        \n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        reporting_loss = 0\n",
    "        printing_loss = 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            batch_index = i + 1\n",
    "            # input, target_res, token_mask, ab_mask, attn_mask, original_seq, masked_seq = batch\n",
    "            # original_seq = self.train_set.reconstruct_sequence(original_seq)\n",
    "            # masked_seq = self.train_set.reconstruct_sequence(masked_seq)\n",
    "            input, target_res, token_mask, ab_mask, attn_mask = batch\n",
    "            \n",
    "            # print(\"original sequence:\", original_seq)\n",
    "            # print(\"masked sequence:\", masked_seq)\n",
    "            \n",
    "            # print(\"input shape:\", input.shape)\n",
    "            # print(\"input:\", input)\n",
    "            # print(\"target_res shape:\", target_res.shape)\n",
    "            # print(\"target_res:\", target_res)\n",
    "            # print(\"attn_mask shape:\", attn_mask.shape)\n",
    "            # print(\"attn_mask:\", attn_mask)\n",
    "            # print(\"token_mask shape:\", token_mask.shape)\n",
    "            # print(\"token_mask:\", token_mask)\n",
    "            # print(\"ab_mask shape:\", ab_mask.shape)\n",
    "            # print(\"ab_mask:\", ab_mask)\n",
    "            \n",
    "            self.optimizer.zero_grad() # zero out gradients\n",
    "            pred_logits = self.model(input, attn_mask) # get predictions for all antibiotics\n",
    "            \n",
    "            # select only the antibtiotics present in the samples - to calculate loss\n",
    "            losses = list()\n",
    "            for j in range(self.num_ab): # for each antibiotic\n",
    "                mask = ab_mask[:, j] # (batch_size,), indicates which samples contain the antibiotic masked\n",
    "                if mask.any(): # if there is at least one masked sample for this antibiotic\n",
    "                    # isolate the predictions and targets for the antibiotic\n",
    "                    ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                    ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                    ab_loss = self.criterions[j](ab_pred_logits, ab_targets)\n",
    "                    losses.append(ab_loss)\n",
    "            loss = sum(losses) / len(losses) # average loss over antibiotics\n",
    "            epoch_loss += loss.item() \n",
    "            reporting_loss += loss.item()\n",
    "            printing_loss += loss.item()\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step() \n",
    "            if batch_index % self.report_every == 0:\n",
    "                self._report_loss_results(batch_index, reporting_loss)\n",
    "                reporting_loss = 0 \n",
    "                \n",
    "            if batch_index % self.print_progress_every == 0:\n",
    "                time_elapsed = time.gmtime(time.time() - time_ref) \n",
    "                self._print_loss_summary(time_elapsed, batch_index, printing_loss) \n",
    "                printing_loss = 0           \n",
    "        avg_epoch_loss = epoch_loss / self.num_batches\n",
    "        return avg_epoch_loss \n",
    "    \n",
    "    def early_stopping(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "        \n",
    "            \n",
    "    def evaluate(self, loader: DataLoader, print_mode: bool = True):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            num_preds = 0\n",
    "            num_correct = 0\n",
    "            num_correct_seq = 0\n",
    "            for batch in loader:\n",
    "                input, target_res, token_mask, ab_mask, attn_mask = batch\n",
    "                pred_logits = self.model(input, attn_mask) # get predictions for all antibiotics\n",
    "                pred_res = torch.where(pred_logits > 0, torch.ones_like(pred_logits), torch.zeros_like(pred_logits))\n",
    "                \n",
    "                num_correct_seq += get_num_correct_seq(pred_res, target_res, token_mask, ab_mask)\n",
    "                batch_loss = list()\n",
    "                for j in range(self.num_ab): # for each antibiotic\n",
    "                    mask = ab_mask[:, j] \n",
    "                    if mask.any(): # if there is at least one masked sample for this antibiotic\n",
    "                        ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                        ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                        \n",
    "                        ab_loss = self.criterions[j](ab_pred_logits, ab_targets)\n",
    "                        batch_loss.append(ab_loss.item())\n",
    "                        \n",
    "                        ab_pred_res = pred_res[mask, j] # (num_masked_samples,)\n",
    "                        num_preds += ab_targets.shape[0]\n",
    "                        num_correct += get_num_correct(ab_pred_res, ab_targets)    \n",
    "                loss += sum(batch_loss) / len(batch_loss) # average loss over antibiotics\n",
    "            loss /= len(loader) # average loss over batches\n",
    "            acc = num_correct / num_preds # accuracy over all predictions\n",
    "            seq_acc = num_correct_seq / (len(loader) * self.batch_size) # accuracy over all sequences\n",
    "            \n",
    "        if print_mode:\n",
    "            print(f\"Loss: {loss:.3f} | Accuracy: {acc:.2%} | Sequence accuracy: {seq_acc:.2%}\")\n",
    "            print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        return loss, acc, seq_acc\n",
    "            \n",
    "     \n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                # \"dataset\": \"NCBI\",\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                # \"model\": \"BERT\",\n",
    "                \"hidden_dim\": self.model.hidden_dim,\n",
    "                \"num_layers\": self.model.num_layers,\n",
    "                \"num_heads\": self.model.num_heads,\n",
    "                \"emb_dim\": self.model.emb_dim,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"mask_prob\": self.mask_prob,\n",
    "                \"max_seq_len\": self.model.max_seq_len,\n",
    "                \"vocab_size\": len(self.train_set.vocab),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "                \"train_size\": self.train_size,\n",
    "                \"random_state\": self.random_state,\n",
    "                # \"val_size\": self.val_size,\n",
    "                # \"test_size\": self.test_size,\n",
    "                # \"early_stopping_patience\": self.patience,\n",
    "                # \"dropout_prob\": self.model.dropout_prob,\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/live_loss\", step_metric=\"batch\")\n",
    "        self.wandb_run.define_metric(\"Losses/train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_seq_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        \n",
    "        if self.do_testing:\n",
    "            self.wandb_run.define_metric(\"Losses/test_loss\")\n",
    "            self.wandb_run.define_metric(\"Accuracies/test_acc\")\n",
    "            self.wandb_run.define_metric(\"Accuracies/test_seq_acc\")\n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_seq_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "     \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"Losses/train_loss\": self.losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            \"Accuracies/val_acc\": self.val_accuracies[-1],\n",
    "            \"Accuracies/val_seq_acc\": self.val_seq_accuracies[-1]\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)\n",
    "    \n",
    "        \n",
    "    def _report_loss_results(self, batch_index, tot_loss):\n",
    "        avg_loss = tot_loss / self.report_every\n",
    "        \n",
    "        global_step = self.current_epoch * self.num_batches + batch_index # global step, total #batches seen\n",
    "        self.wandb_run.log({\"batch\": global_step, \"Losses/live_loss\": avg_loss})\n",
    "        # self.writer.add_scalar(\"Loss\", avg_loss, global_step=global_step)\n",
    "    \n",
    "        \n",
    "    def _print_loss_summary(self, time_elapsed, batch_index, tot_loss):\n",
    "        progress = batch_index / self.num_batches\n",
    "        mlm_loss = tot_loss / self.print_progress_every\n",
    "          \n",
    "        s = f\"{time.strftime('%H:%M:%S', time_elapsed)}\" \n",
    "        s += f\" | Epoch: {self.current_epoch+1}/{self.epochs} | {batch_index}/{self.num_batches} ({progress:.2%}) | \"\\\n",
    "                f\"Loss: {mlm_loss:.3f}\"\n",
    "        print(s)\n",
    "    \n",
    "    \n",
    "    def _visualize_losses(self, savepath: Path = None):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(self.losses)), self.losses, '-o', label='Training')\n",
    "        ax.plot(range(len(self.val_losses)), self.val_losses, '-o', label='Validation')\n",
    "        ax.axhline(y=self.test_loss, color='r', linestyle='--', label='Test') if self.do_testing else None\n",
    "        ax.set_title('MLM losses')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_xticks(range(len(self.losses))) if len(self.losses) < 10 else ax.set_xticks(range(0, len(self.losses), 5))\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        plt.savefig(savepath, dpi=300) if savepath else None\n",
    "        # self.wandb_run.log({\"Losses/losses\": wandb.log(ax)})\n",
    "        self.wandb_run.log({\"Losses/losses\": wandb.Image(ax)})\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    def _visualize_accuracy(self, savepath: Path = None):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(self.val_accuracies)), self.val_accuracies, '-o', label='Validation')\n",
    "        ax.axhline(y=self.test_acc, color='r', linestyle='--', label='Test') if self.do_testing else None\n",
    "        ax.set_title('MLM accuracy')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_xticks(range(len(self.val_accuracies))) if len(self.val_accuracies) < 10 else ax.set_xticks(range(0, len(self.val_accuracies), 5))\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        plt.savefig(savepath, dpi=300) if savepath else None\n",
    "        # self.wandb_run.log({\"Accuracies/accuracy\": wandb.log(ax)})\n",
    "        self.wandb_run.log({\"Accuracies/accuracy\": wandb.Image(ax)})\n",
    "        plt.close() \n",
    "    \n",
    "    \n",
    "    def save_model(self, savepath: Path):\n",
    "        torch.save(self.model.state_dict(), savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def load_model(self, savepath: Path):\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "        print(\"=\"*self._splitter_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Number of samples: 1,439,018\n",
      "Constructing vocabulary...\n",
      "Found 20 antibiotics: ['AMP', 'CTX', 'GEN', 'TOB', 'CIP', 'CAZ', 'CRO', 'OFX', 'AMK', 'AMX', 'LVX', 'TZP', 'AMC', 'FEP', 'COL', 'MFX', 'NOR', 'NET', 'PIP', 'NAL']\n",
      "Loading model...\n",
      "Model summary:\n",
      "======================================================================\n",
      "Embedding dim: 512\n",
      "Hidden dim: 512\n",
      "Number of heads: 4\n",
      "Number of encoder layers: 12\n",
      "Max sequence length: 22\n",
      "Vocab size: 218\n",
      "Number of parameters: 16,107,738\n",
      "======================================================================\n",
      "Trainer summary:\n",
      "======================================================================\n",
      "Device: cuda (NVIDIA GeForce RTX 3080)\n",
      "Training dataset size: 1,151,214\n",
      "Number of antibiotics: 20\n",
      "Antibiotics: ['AMP', 'CTX', 'GEN', 'TOB', 'CIP', 'CAZ', 'CRO', 'OFX', 'AMK', 'AMX', 'LVX', 'TZP', 'AMC', 'FEP', 'COL', 'MFX', 'NOR', 'NET', 'PIP', 'NAL']\n",
      "Train-val-test split 80% - 10% - 10%\n",
      "Will test? No\n",
      "Mask probability: 25%\n",
      "Number of epochs: 10\n",
      "Early stopping patience: 3\n",
      "Batch size: 32\n",
      "Number of batches: 35,975\n",
      "Dropout probability: 10%\n",
      "Learning rate: 5e-05\n",
      "Weight decay: 0.01\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjespeols\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\jespe\\Documents\\GitHub_local\\ARFusion\\wandb\\run-20231122_081727-w7rsd1uu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jespeols/Phenotype-MLM/runs/w7rsd1uu' target=\"_blank\">test_new_code_12layers</a></strong> to <a href='https://wandb.ai/jespeols/Phenotype-MLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jespeols/Phenotype-MLM' target=\"_blank\">https://wandb.ai/jespeols/Phenotype-MLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jespeols/Phenotype-MLM/runs/w7rsd1uu' target=\"_blank\">https://wandb.ai/jespeols/Phenotype-MLM/runs/w7rsd1uu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "00:02:18 | Epoch: 1/10 | 3000/35975 (8.34%) | Loss: 0.240\n",
      "00:04:26 | Epoch: 1/10 | 6000/35975 (16.68%) | Loss: 0.220\n",
      "00:06:33 | Epoch: 1/10 | 9000/35975 (25.02%) | Loss: 0.214\n",
      "00:08:41 | Epoch: 1/10 | 12000/35975 (33.36%) | Loss: 0.207\n",
      "00:10:48 | Epoch: 1/10 | 15000/35975 (41.70%) | Loss: 0.208\n",
      "00:12:55 | Epoch: 1/10 | 18000/35975 (50.03%) | Loss: 0.204\n",
      "00:15:03 | Epoch: 1/10 | 21000/35975 (58.37%) | Loss: 0.198\n",
      "00:17:11 | Epoch: 1/10 | 24000/35975 (66.71%) | Loss: 0.202\n",
      "00:19:18 | Epoch: 1/10 | 27000/35975 (75.05%) | Loss: 0.200\n",
      "00:21:26 | Epoch: 1/10 | 30000/35975 (83.39%) | Loss: 0.201\n",
      "00:23:34 | Epoch: 1/10 | 33000/35975 (91.73%) | Loss: 0.198\n",
      "Epoch completed in 26.0 min\n",
      "Elapsed time: 00:26:47\n",
      "Evaluating on validation set...\n",
      "Loss: 0.198 | Accuracy: 91.53% | Sequence accuracy: 85.54%\n",
      "======================================================================\n",
      "Epoch 2/10\n",
      "00:03:27 | Epoch: 2/10 | 3000/35975 (8.34%) | Loss: 0.194\n",
      "00:06:57 | Epoch: 2/10 | 6000/35975 (16.68%) | Loss: 0.200\n",
      "00:10:27 | Epoch: 2/10 | 9000/35975 (25.02%) | Loss: 0.196\n",
      "00:13:57 | Epoch: 2/10 | 12000/35975 (33.36%) | Loss: 0.195\n",
      "00:17:27 | Epoch: 2/10 | 15000/35975 (41.70%) | Loss: 0.196\n",
      "00:21:07 | Epoch: 2/10 | 18000/35975 (50.03%) | Loss: 0.198\n",
      "00:24:08 | Epoch: 2/10 | 21000/35975 (58.37%) | Loss: 0.194\n",
      "00:26:18 | Epoch: 2/10 | 24000/35975 (66.71%) | Loss: 0.196\n",
      "00:28:28 | Epoch: 2/10 | 27000/35975 (75.05%) | Loss: 0.195\n",
      "00:30:38 | Epoch: 2/10 | 30000/35975 (83.39%) | Loss: 0.193\n",
      "00:32:50 | Epoch: 2/10 | 33000/35975 (91.73%) | Loss: 0.194\n",
      "Epoch completed in 35.1 min\n",
      "Elapsed time: 01:05:37\n",
      "Evaluating on validation set...\n",
      "Loss: 0.192 | Accuracy: 91.61% | Sequence accuracy: 85.71%\n",
      "======================================================================\n",
      "Epoch 3/10\n",
      "00:02:14 | Epoch: 3/10 | 3000/35975 (8.34%) | Loss: 0.193\n",
      "00:04:24 | Epoch: 3/10 | 6000/35975 (16.68%) | Loss: 0.191\n",
      "00:06:32 | Epoch: 3/10 | 9000/35975 (25.02%) | Loss: 0.193\n",
      "00:08:40 | Epoch: 3/10 | 12000/35975 (33.36%) | Loss: 0.193\n",
      "00:10:47 | Epoch: 3/10 | 15000/35975 (41.70%) | Loss: 0.193\n",
      "00:13:04 | Epoch: 3/10 | 18000/35975 (50.03%) | Loss: 0.193\n",
      "00:15:16 | Epoch: 3/10 | 21000/35975 (58.37%) | Loss: 0.191\n",
      "00:17:28 | Epoch: 3/10 | 24000/35975 (66.71%) | Loss: 0.191\n",
      "00:19:40 | Epoch: 3/10 | 27000/35975 (75.05%) | Loss: 0.192\n",
      "00:21:45 | Epoch: 3/10 | 30000/35975 (83.39%) | Loss: 0.191\n",
      "00:23:56 | Epoch: 3/10 | 33000/35975 (91.73%) | Loss: 0.188\n",
      "Epoch completed in 26.2 min\n",
      "Elapsed time: 01:34:52\n",
      "Evaluating on validation set...\n",
      "Loss: 0.191 | Accuracy: 91.74% | Sequence accuracy: 85.78%\n",
      "======================================================================\n",
      "Epoch 4/10\n",
      "00:02:06 | Epoch: 4/10 | 3000/35975 (8.34%) | Loss: 0.189\n",
      "00:04:15 | Epoch: 4/10 | 6000/35975 (16.68%) | Loss: 0.192\n",
      "00:06:26 | Epoch: 4/10 | 9000/35975 (25.02%) | Loss: 0.190\n",
      "00:08:33 | Epoch: 4/10 | 12000/35975 (33.36%) | Loss: 0.190\n",
      "00:10:42 | Epoch: 4/10 | 15000/35975 (41.70%) | Loss: 0.194\n",
      "00:12:52 | Epoch: 4/10 | 18000/35975 (50.03%) | Loss: 0.192\n",
      "00:15:00 | Epoch: 4/10 | 21000/35975 (58.37%) | Loss: 0.192\n",
      "00:17:08 | Epoch: 4/10 | 24000/35975 (66.71%) | Loss: 0.190\n",
      "00:19:16 | Epoch: 4/10 | 27000/35975 (75.05%) | Loss: 0.192\n",
      "00:21:24 | Epoch: 4/10 | 30000/35975 (83.39%) | Loss: 0.190\n",
      "00:23:32 | Epoch: 4/10 | 33000/35975 (91.73%) | Loss: 0.190\n",
      "Epoch completed in 25.7 min\n",
      "Elapsed time: 02:03:30\n",
      "Evaluating on validation set...\n",
      "Loss: 0.191 | Accuracy: 91.69% | Sequence accuracy: 85.80%\n",
      "======================================================================\n",
      "Epoch 5/10\n",
      "00:02:05 | Epoch: 5/10 | 3000/35975 (8.34%) | Loss: 0.193\n",
      "00:04:12 | Epoch: 5/10 | 6000/35975 (16.68%) | Loss: 0.189\n",
      "00:06:20 | Epoch: 5/10 | 9000/35975 (25.02%) | Loss: 0.191\n",
      "00:08:31 | Epoch: 5/10 | 12000/35975 (33.36%) | Loss: 0.191\n",
      "00:10:37 | Epoch: 5/10 | 15000/35975 (41.70%) | Loss: 0.189\n",
      "00:12:47 | Epoch: 5/10 | 18000/35975 (50.03%) | Loss: 0.189\n",
      "00:14:55 | Epoch: 5/10 | 21000/35975 (58.37%) | Loss: 0.192\n",
      "00:17:05 | Epoch: 5/10 | 24000/35975 (66.71%) | Loss: 0.192\n",
      "00:19:13 | Epoch: 5/10 | 27000/35975 (75.05%) | Loss: 0.188\n",
      "00:21:22 | Epoch: 5/10 | 30000/35975 (83.39%) | Loss: 0.192\n",
      "00:23:31 | Epoch: 5/10 | 33000/35975 (91.73%) | Loss: 0.190\n",
      "Epoch completed in 25.6 min\n",
      "Elapsed time: 02:32:03\n",
      "Evaluating on validation set...\n",
      "Loss: 0.188 | Accuracy: 91.82% | Sequence accuracy: 86.00%\n",
      "======================================================================\n",
      "Epoch 6/10\n",
      "00:02:12 | Epoch: 6/10 | 3000/35975 (8.34%) | Loss: 0.190\n",
      "00:04:28 | Epoch: 6/10 | 6000/35975 (16.68%) | Loss: 0.192\n",
      "00:06:36 | Epoch: 6/10 | 9000/35975 (25.02%) | Loss: 0.190\n",
      "00:08:46 | Epoch: 6/10 | 12000/35975 (33.36%) | Loss: 0.188\n",
      "00:10:53 | Epoch: 6/10 | 15000/35975 (41.70%) | Loss: 0.193\n",
      "00:13:04 | Epoch: 6/10 | 18000/35975 (50.03%) | Loss: 0.192\n",
      "00:15:10 | Epoch: 6/10 | 21000/35975 (58.37%) | Loss: 0.189\n",
      "00:17:16 | Epoch: 6/10 | 24000/35975 (66.71%) | Loss: 0.190\n",
      "00:19:22 | Epoch: 6/10 | 27000/35975 (75.05%) | Loss: 0.192\n",
      "00:21:28 | Epoch: 6/10 | 30000/35975 (83.39%) | Loss: 0.189\n",
      "00:23:34 | Epoch: 6/10 | 33000/35975 (91.73%) | Loss: 0.190\n",
      "Epoch completed in 25.7 min\n",
      "Elapsed time: 03:00:35\n",
      "Evaluating on validation set...\n",
      "Loss: 0.190 | Accuracy: 91.74% | Sequence accuracy: 85.77%\n",
      "======================================================================\n",
      "Epoch 7/10\n",
      "00:02:08 | Epoch: 7/10 | 3000/35975 (8.34%) | Loss: 0.190\n",
      "00:04:18 | Epoch: 7/10 | 6000/35975 (16.68%) | Loss: 0.194\n",
      "00:06:29 | Epoch: 7/10 | 9000/35975 (25.02%) | Loss: 0.192\n",
      "00:08:35 | Epoch: 7/10 | 12000/35975 (33.36%) | Loss: 0.191\n",
      "00:10:42 | Epoch: 7/10 | 15000/35975 (41.70%) | Loss: 0.189\n",
      "00:12:49 | Epoch: 7/10 | 18000/35975 (50.03%) | Loss: 0.190\n",
      "00:14:55 | Epoch: 7/10 | 21000/35975 (58.37%) | Loss: 0.193\n",
      "00:17:00 | Epoch: 7/10 | 24000/35975 (66.71%) | Loss: 0.187\n",
      "00:19:06 | Epoch: 7/10 | 27000/35975 (75.05%) | Loss: 0.190\n",
      "00:21:13 | Epoch: 7/10 | 30000/35975 (83.39%) | Loss: 0.191\n",
      "00:24:10 | Epoch: 7/10 | 33000/35975 (91.73%) | Loss: 0.195\n",
      "Epoch completed in 26.3 min\n",
      "Elapsed time: 03:29:46\n",
      "Evaluating on validation set...\n",
      "Loss: 0.188 | Accuracy: 91.79% | Sequence accuracy: 85.96%\n",
      "======================================================================\n",
      "Epoch 8/10\n",
      "00:02:10 | Epoch: 8/10 | 3000/35975 (8.34%) | Loss: 0.192\n",
      "00:04:17 | Epoch: 8/10 | 6000/35975 (16.68%) | Loss: 0.188\n",
      "00:06:23 | Epoch: 8/10 | 9000/35975 (25.02%) | Loss: 0.189\n",
      "00:08:30 | Epoch: 8/10 | 12000/35975 (33.36%) | Loss: 0.192\n",
      "00:10:36 | Epoch: 8/10 | 15000/35975 (41.70%) | Loss: 0.190\n",
      "00:12:42 | Epoch: 8/10 | 18000/35975 (50.03%) | Loss: 0.191\n",
      "00:14:50 | Epoch: 8/10 | 21000/35975 (58.37%) | Loss: 0.191\n",
      "00:16:57 | Epoch: 8/10 | 24000/35975 (66.71%) | Loss: 0.191\n",
      "00:19:05 | Epoch: 8/10 | 27000/35975 (75.05%) | Loss: 0.191\n",
      "00:21:14 | Epoch: 8/10 | 30000/35975 (83.39%) | Loss: 0.191\n",
      "00:23:23 | Epoch: 8/10 | 33000/35975 (91.73%) | Loss: 0.190\n",
      "Epoch completed in 25.5 min\n",
      "Elapsed time: 03:58:11\n",
      "Evaluating on validation set...\n",
      "Loss: 0.189 | Accuracy: 91.67% | Sequence accuracy: 85.57%\n",
      "======================================================================\n",
      "Early stopping at epoch 8 with validation loss 0.189\n",
      "Best validation loss 0.188 | Validation accuracy 91.82% | Validation sequence accuracy 86.00% at epoch 5\n",
      "======================================================================\n",
      "Training completed in 4h 0.4 min\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "ds = pd.read_pickle(config['data']['load_path'])\n",
    "# ds = ds.iloc[:10000]\n",
    "num_samples = ds.shape[0]\n",
    "print(f\"Number of samples: {num_samples:,}\")\n",
    "\n",
    "specials = config['specials']\n",
    "print(\"Constructing vocabulary...\")\n",
    "savepath_vocab = BASE_DIR / \"data\" / \"pheno_vocab.pt\" if config['save_vocab'] else None\n",
    "vocab, antibiotics = construct_pheno_vocab(ds,\n",
    "                                           specials,\n",
    "                                           savepath_vocab=savepath_vocab, \n",
    "                                           separate_phenotypes=config['separate_phenotypes'])\n",
    "print(f\"Found {len(antibiotics)} antibiotics: {antibiotics}\")\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "max_phenotypes_len = ds['num_phenotypes'].max()    \n",
    "if config['max_seq_len'] == 'auto':\n",
    "    if config['separate_phenotypes']:\n",
    "        max_seq_len = 2*max_phenotypes_len + 4 + 1 # +4 for year, country, age & gender, +1 for CLS token\n",
    "    else:\n",
    "        max_seq_len = max_phenotypes_len + 4 + 1\n",
    "else:\n",
    "    max_seq_len = config['max_seq_len']\n",
    "\n",
    "train_indices, val_indices, test_indices = get_split_indices(num_samples, config['split'], \n",
    "                                                                 random_state=config['random_state'])\n",
    "train_set = PhenotypeDataset(ds.iloc[train_indices], vocab, antibiotics, specials, max_seq_len, base_dir=BASE_DIR,\n",
    "                            #  include_sequences=True\n",
    "                             )\n",
    "val_set = PhenotypeDataset(ds.iloc[val_indices], vocab, antibiotics, specials, max_seq_len, base_dir=BASE_DIR,\n",
    "                        #    include_sequences=True\n",
    "                           )\n",
    "test_set = PhenotypeDataset(ds.iloc[test_indices], vocab, antibiotics, specials, max_seq_len, base_dir=BASE_DIR,\n",
    "                            # include_sequences=True\n",
    "                            )\n",
    "\n",
    "# os.environ['WANDB_MODE'] = 'disabled' # 'dryrun' or 'run' or 'offline' or 'disabled' or 'online'\n",
    "os.environ['WANDB_MODE'] = 'online' # 'dryrun' or 'run' or 'offline' or 'disabled' or 'online'\n",
    "           \n",
    "print(\"Loading model...\")\n",
    "config['name'] = \"test_new_code_12layers\"\n",
    "config['batch_size'] = 32\n",
    "config['emb_dim'] = 512\n",
    "config['hidden_dim'] = 512\n",
    "config['num_heads'] = 4\n",
    "config['num_layers'] = 12\n",
    "config['epochs'] = 10\n",
    "config['print_progress_every'] = 3000\n",
    "bert = PhenoBERT(config, vocab_size, antibiotics).to(device)\n",
    "trainer = BertPhenoTrainer(\n",
    "    config=config,\n",
    "    model=bert,\n",
    "    train_set=train_set,\n",
    "    val_set=val_set,\n",
    "    test_set=test_set,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    antibiotics=antibiotics,\n",
    ")\n",
    "\n",
    "trainer.print_model_summary()\n",
    "trainer.print_trainer_summary()\n",
    "trainer()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARFusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
