{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# user-defined modules\n",
    "from trainers import BertMLMTrainer\n",
    "\n",
    "# user-defined functions\n",
    "from construct_vocab import construct_pheno_vocab\n",
    "from utils import get_split_indices\n",
    "from data_preprocessing import preprocess_TESSy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080\n",
      "base directory: C:\\Users\\jespe\\Documents\\GitHub_local\\ARFusion\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "os.environ['WANDB_MODE'] = 'disabled' # 'dryrun' or 'run' or 'offline' or 'disabled' or 'online'\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Using CPU\") \n",
    "    \n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "RESULTS_DIR = Path(os.path.join(BASE_DIR / \"results\" / \"temp\"))\n",
    "os.chdir(BASE_DIR)\n",
    "print(\"base directory:\", BASE_DIR)\n",
    "\n",
    "config_path = BASE_DIR / \"config_pheno.yaml\"\n",
    "with open(config_path, \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.emb_dim) \n",
    "        # self.token_type_emb = nn.Embedding(self.vocab_size, self.emb_dim) \n",
    "        self.position_emb = nn.Embedding(self.vocab_size, self.emb_dim) \n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        # input_tensor: (batch_size, seq_len)\n",
    "        # token_type_ids: (batch_size, seq_len)\n",
    "        # position_ids: (batch_size, seq_len)\n",
    "        \n",
    "        seq_len = input_tensor.size(-1)\n",
    "        \n",
    "        pos_tensor = self.numeric_position(seq_len, input_tensor)\n",
    "        # token_type not relevant for unimodal data\n",
    "        # token_type_tensor = torch.zeros_like(input_tensor).to(device) # (batch_size, seq_len)\n",
    "        # token_type_tensor[:, (seq_len//2 + 1):] = 1 # here, we assume that the sentence is split in half\n",
    "        \n",
    "        token_emb = self.token_emb(input_tensor) # (batch_size, seq_len, emb_dim)\n",
    "        # token_type_emb = self.token_type_emb(token_type_tensor) # (batch_size, seq_len, emb_dim)\n",
    "        position_emb = self.position_emb(pos_tensor) # (batch_size, seq_len, emb_dim)\n",
    "        \n",
    "        # emb = token_emb + token_type_emb + position_emb\n",
    "        emb = token_emb + position_emb\n",
    "        emb = self.dropout(emb)\n",
    "        emb = self.layer_norm(emb) \n",
    "        return emb\n",
    "                \n",
    "    def numeric_position(self, dim, input_tensor): # input_tensor: (batch_size, seq_len)\n",
    "        # dim is the length of the sequence\n",
    "        position_ids = torch.arange(dim, dtype=torch.long, device=device) # create tensor of [0, 1, 2, ..., dim-1]\n",
    "        return position_ids.expand_as(input_tensor) # expand to (batch_size, seq_len)\n",
    "    \n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.head_dim = self.emb_dim // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.emb_dim, f\"Embedding dimension must be divisible by number of heads, got {self.emb_dim} and {self.num_heads}\"\n",
    "        \n",
    "        self.q = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.k = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.v = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "    \n",
    "    def forward(self, input_emb: torch.Tensor, attn_mask:torch.Tensor = None):\n",
    "        B, L, D = input_emb.size() # (L=batch_size, L=seq_len, D=emb_dim)\n",
    "        \n",
    "        # project input embeddings to query, key, value, then split into num_heads, reducing the embedding dimension\n",
    "        query = self.q(input_emb).view(B, L, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, L, head_dim)\n",
    "        key = self.k(input_emb).view(B, L, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, L, head_dim)\n",
    "        value = self.v(input_emb).view(B, L, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, L, head_dim)\n",
    "        \n",
    "        scale_factor = query.size(-1) ** 0.5\n",
    "        attn_scores = torch.matmul(query, key.transpose(-1, -2)) / scale_factor # (B, num_heads, L, L)\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill_(~attn_mask, -1e9) if attn_mask is not None else attn_scores \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1) # (B, num_heads, L, L)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn = torch.matmul(attn_weights, value) # (B, num_heads, L, head_dim)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, L, D) # (B, L, num_heads, head_dim) -> (B, L, D), concatenate the heads\n",
    "        \n",
    "        return attn\n",
    "        \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.hidden_dim),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, self.emb_dim),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        \n",
    "    def forward(self, input_emb: torch.Tensor, attn_mask: torch.Tensor = None):\n",
    "        x = input_emb\n",
    "        attn = self.attention(x, attn_mask)\n",
    "        x = x + attn\n",
    "        x = self.layer_norm(x)\n",
    "        res = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = x + res\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PhenoBERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, vocab_size, antibiotics):\n",
    "        super(PhenoBERT, self).__init__()\n",
    "                \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = None # Can be set later\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.dropout_prob = config['dropout_prob']\n",
    "        \n",
    "        self.embedding = JointEmbedding(config, vocab_size)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(config) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.token_prediction_layer = nn.Linear(self.emb_dim, self.vocab_size) # MLM task\n",
    "        self.softmax = nn.LogSoftmax(dim=-1) # log softmax improves numerical stability, we use NLLLoss later\n",
    "        if antibiotics:\n",
    "            self.classification_layer = [AbPredictor(self.emb_dim).to(device) for _ in range(len(antibiotics))] # classification task\n",
    "        \n",
    "    def forward(self, input_tensor, attn_mask): \n",
    "        embedded = self.embedding(input_tensor)\n",
    "        for layer in self.encoder:\n",
    "            embedded = layer(embedded, attn_mask)\n",
    "        encoded = embedded # ouput of the BERT Encoder\n",
    "        \n",
    "        if self.classification_layer: # ASSUMES MLM AND CLASSIFICATION ARE NOT DONE AT THE SAME TIME\n",
    "            cls_token = encoded[:, 0, :] # (batch_size, emb_dim)\n",
    "            predictions = torch.cat([net(cls_token) for net in self.classification_layer], dim=1) # (batch_size, num_ab)\n",
    "            return predictions\n",
    "        else:\n",
    "            token_prediction = self.token_prediction_layer(encoded) # (batch_size, seq_len, vocab_size)\n",
    "            return self.softmax(token_prediction)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "class AbPredictor(nn.Module): # predicts resistance or susceptibility for an antibiotic\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AbPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, 1), # binary classification\n",
    "        )\n",
    "        # self.classifiers = nn.ModuleList(\n",
    "        #     [nn.Sequential(\n",
    "        #         nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.LayerNorm(self.hidden_dim),\n",
    "        #         nn.Linear(self.hidden_dim, 2), # one value for R and one for S\n",
    "        #     ) for _ in range(self.num_ab)]\n",
    "        # )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X is the CLS token of the BERT model\n",
    "        return self.classifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class PhenotypeDataset(Dataset):      \n",
    "    # df column names\n",
    "    INDICES_MASKED = 'indices_masked' # input to BERT, token indices of the masked sequence\n",
    "    TARGET_RESISTANCES = 'target_resistances' # resistance of the target antibiotics, what we want to predict\n",
    "    TOKEN_MASK = 'token_mask' # True if token is masked, False otherwise\n",
    "    AB_MASK = 'ab_mask' # True if antibiotic is masked, False otherwise\n",
    "    # # if original text is included\n",
    "    ORIGINAL_SEQUENCE = 'original_sequence'\n",
    "    MASKED_SEQUENCE = 'masked_sequence'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ds: pd.DataFrame,\n",
    "                 vocab,\n",
    "                 antibiotics: list,\n",
    "                 specials: dict,\n",
    "                 max_seq_len: int,\n",
    "                 base_dir: Path,\n",
    "                 include_sequences: bool = False,\n",
    "                 random_state: int = 42,\n",
    "                 ):\n",
    "        \n",
    "        os.chdir(base_dir)\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        self.ds = ds.reset_index(drop=True) \n",
    "        self.num_samples = self.ds.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics)\n",
    "        self.ab_to_idx = {ab: i for i, ab in enumerate(self.antibiotics)}\n",
    "        self.enc_res = {'S': 0, 'R': 1}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.CLS = specials['CLS']\n",
    "        self.PAD = specials['PAD']\n",
    "        self.MASK = specials['MASK']\n",
    "        self.UNK = specials['UNK']\n",
    "        self.special_tokens = specials.values()\n",
    "        self.max_seq_len = max_seq_len\n",
    "           \n",
    "        self.include_sequences = include_sequences\n",
    "        if self.include_sequences:\n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_RESISTANCES, self.TOKEN_MASK, self.AB_MASK,\n",
    "                            self.ORIGINAL_SEQUENCE, self.MASKED_SEQUENCE]\n",
    "        else: \n",
    "            self.columns = [self.INDICES_MASKED, self.TARGET_RESISTANCES, self.TOKEN_MASK, self.AB_MASK]        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        \n",
    "        input = torch.tensor(item[self.INDICES_MASKED], dtype=torch.long, device=device)\n",
    "        target_res = torch.tensor(item[self.TARGET_RESISTANCES], dtype=torch.float32, device=device)\n",
    "        token_mask = torch.tensor(item[self.TOKEN_MASK], dtype=torch.bool, device=device)\n",
    "        ab_mask = torch.tensor(item[self.AB_MASK], dtype=torch.bool, device=device)\n",
    "        attn_mask = (input != self.vocab[self.PAD]).unsqueeze(0).unsqueeze(1) # one dim for batch, one for heads\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            original_sequence = item[self.ORIGINAL_SEQUENCE]\n",
    "            masked_sequence = item[self.MASKED_SEQUENCE]\n",
    "            return input, target_res, token_mask, ab_mask, attn_mask, original_sequence, masked_sequence\n",
    "        else:\n",
    "            return input, target_res, token_mask, ab_mask, attn_mask\n",
    "\n",
    "       \n",
    "    def prepare_dataset(self, mask_prob: float = 0.15): # will be called at the start of each epoch (dynamic masking)\n",
    "        sequences, masked_sequences, target_resistances, token_masks, ab_masks = self._construct_masked_sequences(mask_prob)\n",
    "        \n",
    "        indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "        \n",
    "        if self.include_sequences:\n",
    "            rows = zip(indices_masked, target_resistances, token_masks, ab_masks, sequences, masked_sequences)\n",
    "        else:\n",
    "            rows = zip(indices_masked, target_resistances, token_masks, ab_masks)\n",
    "        self.df = pd.DataFrame(rows, columns=self.columns)\n",
    "\n",
    "    \n",
    "    def _encode_sequence(self, seq: list):\n",
    "        dict = {ab: res for ab, res in [token.split('_') for token in seq]}\n",
    "        indices = [self.ab_to_idx[ab] for ab in dict.keys()]\n",
    "        resistances = [self.enc_res[res] for res in dict.values()]\n",
    "        \n",
    "        return indices, resistances\n",
    "    \n",
    "    \n",
    "    def _construct_masked_sequences(self, mask_prob: float):  \n",
    "        # RoBERTa: 80% -> [MASK], 10% -> original token, 10% -> random token\n",
    "        self.mask_prob = mask_prob\n",
    "        sequences = deepcopy(self.ds['phenotypes'].tolist())\n",
    "        masked_sequences = list()\n",
    "        # all_target_indices = list()\n",
    "        all_target_resistances = list()\n",
    "        ab_masks = list() # will be applied to the output of the model, i.e. (batch_size, num_ab)\n",
    "        token_masks = list() # will be applied to the the sequence itself, i.e. (batch_size, seq_len)\n",
    "        for seq in deepcopy(sequences):\n",
    "            seq_len = len(seq)\n",
    "            # target_indices, target_resistances = self._encode_sequence(seq) # if we don't want to indicate masking here, we \n",
    "            # all_target_indices.append(target_indices)                       # encode the whole sequence, and use a token mask\n",
    "            # all_target_resistances.append(target_resistances)\n",
    "            \n",
    "            token_mask = [False] * seq_len # indicates which tokens in the sequence are masked, includes all tokens\n",
    "            ab_mask = [False] * self.num_ab # will indicate which antibiotics are masked, indexed in the order of self.antibiotics\n",
    "            target_resistances = [-1]*self.num_ab # -1 indicates padding, will indicate the target resistance, same indexing as ab_mask\n",
    "            tokens_masked = 0\n",
    "            for i in range(seq_len):\n",
    "                if np.random.rand() < self.mask_prob: \n",
    "                    ab, res = seq[i].split('_')\n",
    "                    ab_idx = self.ab_to_idx[ab]\n",
    "                    tokens_masked += 1\n",
    "                    r = np.random.rand()\n",
    "                    if r < 0.8: \n",
    "                        seq[i] = self.MASK\n",
    "                    elif r < 0.9:\n",
    "                        j = np.random.randint(self.vocab_size-self.num_ab*2, self.vocab_size) # select random pheno token\n",
    "                        seq[i] = self.vocab.lookup_token(j)\n",
    "                    # else: do nothing, since r > 0.9 and we keep the same token\n",
    "                    token_mask[i] = True\n",
    "                    ab_mask[ab_idx] = True # indicate which antibiotic is masked at this position\n",
    "                    target_resistances[ab_idx] = self.enc_res[res] # the target resistance of the antibiotic\n",
    "            if tokens_masked == 0: # mask at least one token\n",
    "                i = np.random.randint(seq_len)\n",
    "                ab, res = seq[i].split('_')\n",
    "                ab_idx = self.ab_to_idx[ab]\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8: \n",
    "                    seq[i] = self.MASK\n",
    "                elif r < 0.9:\n",
    "                    j = np.random.randint(self.vocab_size-self.num_ab*2, self.vocab_size) # select random token, excluding specials\n",
    "                    seq[i] = self.vocab.lookup_token(j)\n",
    "                # else: do nothing, since r > 0.9 and we keep the same token\n",
    "                token_mask[i] = True\n",
    "                ab_mask[ab_idx] = True # indicate which antibiotic is masked at this position\n",
    "                target_resistances[ab_idx] = self.enc_res[res] # the target resistance of the antibiotic\n",
    "                \n",
    "            masked_sequences.append(seq)\n",
    "            token_masks.append(token_mask)\n",
    "            ab_masks.append(ab_mask)\n",
    "            all_target_resistances.append(target_resistances)\n",
    "        \n",
    "        for i in range(len(sequences)):\n",
    "            token_masks[i] = 5*[False] + token_masks[i]\n",
    "            seq_start = [self.CLS, \n",
    "                         str(self.ds['year'].iloc[i]), \n",
    "                         self.ds['country'].iloc[i], \n",
    "                         self.ds['gender'].iloc[i], \n",
    "                         str(int(self.ds['age'].iloc[i]))]\n",
    "            \n",
    "            sequences[i][:0] = seq_start\n",
    "            masked_sequences[i][:0] = seq_start\n",
    "            # all_target_indices[i][:0] = [-1]*5 \n",
    "            \n",
    "            seq_len = len(sequences[i])\n",
    "            if seq_len < self.max_seq_len:\n",
    "                sequences[i].extend([self.PAD] * (self.max_seq_len - seq_len))\n",
    "                masked_sequences[i].extend([self.PAD] * (self.max_seq_len - seq_len))\n",
    "                token_masks[i].extend([False] * (self.max_seq_len - seq_len))\n",
    "            # the antibiotic-specific lists should always be of length num_ab\n",
    "            pheno_len = len(all_target_resistances[i])\n",
    "            all_target_resistances[i].extend([-1] * (self.num_ab - pheno_len))\n",
    "            # all_target_indices[i].extend([-1] * (self.num_ab - pheno_len)) # -1 indicates padding\n",
    "            # ab_mask is defined with correct length\n",
    "                \n",
    "        return sequences, masked_sequences, all_target_resistances, token_masks, ab_masks  \n",
    "    \n",
    "    \n",
    "    def reconstruct_sequence(self, seq_from_batch):\n",
    "        tuple_len = len(seq_from_batch[0])\n",
    "        sequences = list()\n",
    "        for j in range(tuple_len):\n",
    "            sequence = list()\n",
    "            for i in range(self.max_seq_len):\n",
    "                sequence.append(seq_from_batch[i][j])\n",
    "            sequences.append(sequence)\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############################################ Trainer for MLM task ############################################\n",
    "\n",
    "class BertPhenoTrainer(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config: dict,\n",
    "                 model: PhenoBERT,\n",
    "                 antibiotics: list, # list of antibiotics in the dataset\n",
    "                 train_set,\n",
    "                 val_set,\n",
    "                 test_set, # can be None\n",
    "                 results_dir: Path = None,\n",
    "                 ):\n",
    "        super(BertPhenoTrainer, self).__init__()\n",
    "        \n",
    "        self.random_state = config[\"random_state\"]\n",
    "        torch.manual_seed(self.random_state)\n",
    "        torch.cuda.manual_seed(self.random_state)\n",
    "        \n",
    "        self.model = model\n",
    "        self.antibiotics = antibiotics\n",
    "        self.num_ab = len(self.antibiotics) \n",
    "        self.train_set, self.train_size = train_set, len(train_set)\n",
    "        self.train_size = len(self.train_set)      \n",
    "        self.model.max_seq_len = self.train_set.max_seq_len \n",
    "        self.val_set, self.val_size = val_set, len(val_set)\n",
    "        if test_set:\n",
    "            self.test_set, self.test_size = test_set, len(test_set)\n",
    "        self.split = config[\"split\"]\n",
    "        self.project_name = config[\"project_name\"]\n",
    "        self.wandb_name = config[\"name\"] if config[\"name\"] else datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "         \n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.patience = config[\"early_stopping_patience\"]\n",
    "        self.save_model = config[\"save_model\"] if config[\"save_model\"] else False\n",
    "        \n",
    "        self.do_testing = config[\"do_testing\"] if config[\"do_testing\"] else False\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        \n",
    "        self.mask_prob = config[\"mask_prob\"]\n",
    "        self.criterions = [nn.BCEWithLogitsLoss() for _ in range(self.num_ab)] # the list is so that we can introduce individual weights\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.scheduler = None\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.9)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.98)\n",
    "                 \n",
    "        self.current_epoch = 0\n",
    "        self.report_every = config[\"report_every\"] if config[\"report_every\"] else 100\n",
    "        self.print_progress_every = config[\"print_progress_every\"] if config[\"print_progress_every\"] else 1000\n",
    "        self._splitter_size = 70\n",
    "        self.results_dir = results_dir\n",
    "        os.makedirs(self.results_dir) if not os.path.exists(self.results_dir) else None\n",
    "        \n",
    "        \n",
    "    def print_model_summary(self):        \n",
    "        print(\"Model summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Embedding dim: {self.model.emb_dim}\")\n",
    "        print(f\"Hidden dim: {self.model.hidden_dim}\")\n",
    "        print(f\"Number of heads: {self.model.num_heads}\")\n",
    "        print(f\"Number of encoder layers: {self.model.num_layers}\")\n",
    "        print(f\"Max sequence length: {self.model.max_seq_len}\")\n",
    "        print(f\"Vocab size: {len(self.train_set.vocab):,}\")\n",
    "        print(f\"Number of parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "    \n",
    "    def print_trainer_summary(self):\n",
    "        print(\"Trainer summary:\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "        print(f\"Training dataset size: {self.train_size:,}\")\n",
    "        print(f\"Number of antibiotics: {self.num_ab}\")\n",
    "        print(f\"Antibiotics: {self.antibiotics}\")\n",
    "        print(f\"Train-val-test split {self.split[0]:.0%} - {self.split[1]:.0%} - {self.split[2]:.0%}\")\n",
    "        print(f\"Will test? {'Yes' if self.do_testing else 'No'}\")\n",
    "        print(f\"Mask probability: {self.mask_prob:.0%}\")\n",
    "        print(f\"Number of epochs: {self.epochs}\")\n",
    "        print(f\"Early stopping patience: {self.patience}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Number of batches: {self.num_batches:,}\")\n",
    "        print(f\"Dropout probability: {self.model.dropout_prob:.0%}\")\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Weight decay: {self.weight_decay}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def __call__(self):      \n",
    "        self.wandb_run = self._init_wandb()\n",
    "        self.val_set.prepare_dataset(mask_prob=self.mask_prob) \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        if self.do_testing:\n",
    "            self.test_set.prepare_dataset(mask_prob=self.mask_prob) \n",
    "            self.test_loader = DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_seq_accuracies = []\n",
    "        self.val_ab_stats = []\n",
    "        self.val_iso_stats = []\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            self.model.train()\n",
    "            # Dynamic masking: New mask for training set each epoch\n",
    "            self.train_set.prepare_dataset(mask_prob=self.mask_prob)\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            loss = self.train(self.current_epoch) # returns loss, averaged over batches\n",
    "            self.losses.append(loss) \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            # print(\"Evaluating on training set...\")\n",
    "            # _, train_acc = self.evaluate(self.train_loader)\n",
    "            # self.train_accuracies.append(train_acc)\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            results = self.evaluate(self.val_loader, self.val_set)\n",
    "            self._update_val_lists(results)\n",
    "            self._report_epoch_results()\n",
    "            early_stop = self.early_stopping()\n",
    "            if early_stop:\n",
    "                print(f\"Early stopping at epoch {self.current_epoch+1} with validation loss {self.val_losses[-1]:.3f}\")\n",
    "                s = f\"Best validation loss {self.best_val_loss:.3f}\"\n",
    "                s += f\" | Validation accuracy {self.val_accuracies[self.best_epoch]:.2%}\"\n",
    "                s += f\" | Validation sequence accuracy {self.val_seq_accuracies[self.best_epoch]:.2%}\"\n",
    "                s += f\" at epoch {self.best_epoch+1}\"\n",
    "                print(s)\n",
    "                self.wandb_run.log({\"Losses/final_val_loss\": self.best_val_loss, \n",
    "                           \"Accuracies/final_val_acc\":self.val_accuracies[self.best_epoch],\n",
    "                           \"Accuracies/final_val_seq_acc\": self.val_seq_accuracies[self.best_epoch],\n",
    "                           \"final_epoch\": self.best_epoch+1})\n",
    "                print(\"=\"*self._splitter_size)\n",
    "                self.model.load_state_dict(self.best_model_state) \n",
    "                self.current_epoch = self.best_epoch\n",
    "                break\n",
    "            self.scheduler.step() if self.scheduler else None\n",
    "        if not early_stop:    \n",
    "            self.wandb_run.log({\"Losses/final_val_loss\": self.val_losses[-1], \n",
    "                    \"Accuracies/final_val_acc\":self.val_accuracies[-1],\n",
    "                    \"Accuracies/final_val_seq_acc\": self.val_seq_accuracies[-1],\n",
    "                    \"final_epoch\": self.current_epoch+1})\n",
    "        self.save_model(self.results_dir / \"model_state.pt\") if self.save_model else None\n",
    "        train_time = (time.time() - start_time)/60\n",
    "        self.wandb_run.log({\"Training time (min)\": train_time})\n",
    "        disp_time = f\"{train_time//60:.0f}h {train_time % 60:.1f} min\" if train_time > 60 else f\"{train_time:.1f} min\"\n",
    "        print(f\"Training completed in {disp_time}\")\n",
    "        if not early_stop:\n",
    "            s = f\"Final validation loss {self.val_losses[-1]:.3f}\"\n",
    "            s += f\" | Final validation accuracy {self.val_accuracies[-1]:.2%}\"\n",
    "            s += f\" | Final validation sequence accuracy {self.val_seq_accuracies[-1]:.2%}\"\n",
    "            print(s)\n",
    "        \n",
    "        if self.do_testing:\n",
    "            print(\"Evaluating on test set...\")\n",
    "            results = self.evaluate(self.test_loader, self.test_set)\n",
    "            self.test_loss = results[\"loss\"]\n",
    "            self.test_acc = results[\"acc\"]\n",
    "            test_seq_acc = results[\"seq_acc\"]\n",
    "            self.test_ab_stats = results[\"ab_stats\"]\n",
    "            self.test_iso_stats = results[\"iso_stats\"]\n",
    "            self.wandb_run.log({\"Losses/test_loss\": self.test_loss, \n",
    "                                \"Accuracies/test_acc\": self.test_acc,\n",
    "                                \"Accuracies/test_seq_acc\": test_seq_acc})\n",
    "        self._visualize_losses(savepath=self.results_dir / \"losses.png\")\n",
    "        self._visualize_accuracy(savepath=self.results_dir / \"accuracy.png\")\n",
    "        \n",
    "        return self.val_ab_stats, self.val_iso_stats, self.current_epoch\n",
    "        \n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        reporting_loss = 0\n",
    "        printing_loss = 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            batch_index = i + 1\n",
    "            # input, target_res, token_mask, ab_mask, attn_mask, original_seq, masked_seq = batch\n",
    "            # original_seq = self.train_set.reconstruct_sequence(original_seq)\n",
    "            # masked_seq = self.train_set.reconstruct_sequence(masked_seq)\n",
    "            input, target_res, token_mask, ab_mask, attn_mask = batch\n",
    "            \n",
    "            # print(\"original sequence:\", original_seq)\n",
    "            # print(\"masked sequence:\", masked_seq)\n",
    "            \n",
    "            # print(\"input shape:\", input.shape)\n",
    "            # print(\"input:\", input)\n",
    "            # print(\"target_res shape:\", target_res.shape)\n",
    "            # print(\"target_res:\", target_res)\n",
    "            # print(\"attn_mask shape:\", attn_mask.shape)\n",
    "            # print(\"attn_mask:\", attn_mask)\n",
    "            # print(\"token_mask shape:\", token_mask.shape)\n",
    "            # print(\"token_mask:\", token_mask)\n",
    "            # print(\"ab_mask shape:\", ab_mask.shape)\n",
    "            # print(\"ab_mask:\", ab_mask)\n",
    "            \n",
    "            self.optimizer.zero_grad() # zero out gradients\n",
    "            pred_logits = self.model(input, attn_mask) # get predictions for all antibiotics\n",
    "            \n",
    "            # select only the antibtiotics present in the samples - to calculate loss\n",
    "            losses = list()\n",
    "            for j in range(self.num_ab): # for each antibiotic\n",
    "                mask = ab_mask[:, j] # (batch_size,), indicates which samples contain the antibiotic masked\n",
    "                if mask.any(): # if there is at least one masked sample for this antibiotic\n",
    "                    # isolate the predictions and targets for the antibiotic\n",
    "                    ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                    ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                    ab_loss = self.criterions[j](ab_pred_logits, ab_targets)\n",
    "                    losses.append(ab_loss)\n",
    "            loss = sum(losses) / len(losses) # average loss over antibiotics\n",
    "            epoch_loss += loss.item() \n",
    "            reporting_loss += loss.item()\n",
    "            printing_loss += loss.item()\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step() \n",
    "            if batch_index % self.report_every == 0:\n",
    "                self._report_loss_results(batch_index, reporting_loss)\n",
    "                reporting_loss = 0 \n",
    "                \n",
    "            if batch_index % self.print_progress_every == 0:\n",
    "                time_elapsed = time.gmtime(time.time() - time_ref) \n",
    "                self._print_loss_summary(time_elapsed, batch_index, printing_loss) \n",
    "                printing_loss = 0           \n",
    "        avg_epoch_loss = epoch_loss / self.num_batches\n",
    "        return avg_epoch_loss \n",
    "    \n",
    "    def early_stopping(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "        \n",
    "            \n",
    "    def evaluate(self, loader: DataLoader, ds_obj, print_mode: bool = True):\n",
    "        self.model.eval()\n",
    "        # prepare evaluation statistics dataframes\n",
    "        eval_stats_ab, eval_stats_iso = self._init_eval_stats(ds_obj)\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            num_preds = np.zeros((self.num_ab, 2)) # tracks the number of predictions for each antibiotic & resistance\n",
    "            num_correct = np.zeros_like(num_preds) # tracks the number of correct predictions for each antibiotic & resistance\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                input, target_res, token_mask, ab_mask, attn_mask = batch\n",
    "                pred_logits = self.model(input, attn_mask) # get predictions for all antibiotics\n",
    "                pred_res = torch.where(pred_logits > 0, torch.ones_like(pred_logits), torch.zeros_like(pred_logits))\n",
    "                \n",
    "                eval_stats_iso = self._update_iso_stats(batch_idx, pred_res, target_res, ab_mask, eval_stats_iso)\n",
    "                batch_loss = list()\n",
    "                for j in range(self.num_ab): # for each antibiotic\n",
    "                    mask = ab_mask[:, j] # (batch_size,), indicates which samples contain the antibiotic masked\n",
    "                    if mask.any(): # if there is at least one masked sample for this antibiotic\n",
    "                        ab_pred_logits = pred_logits[mask, j] # (num_masked_samples,)\n",
    "                        ab_targets = target_res[mask, j] # (num_masked_samples,)\n",
    "                        num_R = ab_targets.sum().item()\n",
    "                        num_S = ab_targets.shape[0] - num_R\n",
    "                        num_preds[j, :] += [num_S, num_R]\n",
    "                        \n",
    "                        ab_loss = self.criterions[j](ab_pred_logits, ab_targets)\n",
    "                        batch_loss.append(ab_loss.item())\n",
    "                        \n",
    "                        ab_pred_res = pred_res[mask, j] # (num_masked_samples,)\n",
    "                        num_correct[j, :] += self._get_num_correct(ab_pred_res, ab_targets)    \n",
    "                loss += sum(batch_loss) / len(batch_loss) # average loss over antibiotics\n",
    "            loss /= len(loader) # average loss over batches\n",
    "            acc = num_correct.sum() / num_preds.sum() # accuracy over all predictions\n",
    "            seq_acc = eval_stats_iso['correct_all'].sum() / eval_stats_iso.shape[0] # accuracy over all sequences\n",
    "            \n",
    "            eval_stats_ab = self._update_ab_eval_stats(eval_stats_ab, num_preds, num_correct)\n",
    "            \n",
    "            eval_stats_iso['accuracy_S'] = eval_stats_iso.apply(\n",
    "                lambda row: row['correct_S']/row['num_masked_S'] if row['num_masked_S'] > 0 else np.nan, axis=1)\n",
    "            eval_stats_iso['accuracy_R'] = eval_stats_iso.apply(\n",
    "                lambda row: row['correct_R']/row['num_masked_R'] if row['num_masked_R'] > 0 else np.nan, axis=1)\n",
    "        if print_mode:\n",
    "            print(f\"Loss: {loss:.3f} | Accuracy: {acc:.2%} | Sequence accuracy: {seq_acc:.2%}\")\n",
    "            print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        results = {\"loss\": loss, \n",
    "                   \"acc\": acc, \n",
    "                   \"seq_acc\": seq_acc,\n",
    "                   \"ab_stats\": eval_stats_ab,\n",
    "                   \"iso_stats\": eval_stats_iso}\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def _update_val_lists(self, results: dict):\n",
    "        self.val_losses.append(results[\"loss\"])\n",
    "        self.val_accuracies.append(results[\"acc\"])\n",
    "        self.val_seq_accuracies.append(results[\"seq_acc\"])\n",
    "        self.val_ab_stats.append(results[\"ab_stats\"])\n",
    "        self.val_iso_stats.append(results[\"iso_stats\"])\n",
    "    \n",
    "    \n",
    "    def _init_eval_stats(self, ds_obj):\n",
    "        eval_stats_ab = pd.DataFrame(columns=['ab', 'res', 'num_pred', 'num_correct'])\n",
    "        tmp = []\n",
    "        [tmp.extend([ab, ab]) for ab in self.antibiotics]\n",
    "        eval_stats_ab['ab'] = tmp\n",
    "        eval_stats_ab['res'] = ['S', 'R']*self.num_ab\n",
    "        eval_stats_ab['num_pred'], eval_stats_ab['num_correct'] = 0, 0\n",
    "        eval_stats_iso = ds_obj.ds.copy()\n",
    "        eval_stats_iso['num_masked'] = 0\n",
    "        eval_stats_iso['num_masked_S'] = 0\n",
    "        eval_stats_iso['num_masked_R'] = 0\n",
    "        eval_stats_iso['correct_S'] = 0\n",
    "        eval_stats_iso['correct_R'] = 0\n",
    "        eval_stats_iso['correct_all'] = False\n",
    "        # eval_stats_iso['correct_mask'] = [-1]*eval_stats_iso.shape[0] # indicates which antibiotics are -1: not masked, 0: incorrect, 1:correct\n",
    "        eval_stats_iso.drop(columns=['phenotypes'], inplace=True)\n",
    "        \n",
    "        return eval_stats_ab, eval_stats_iso\n",
    "    \n",
    "    \n",
    "    def _update_ab_eval_stats(self, eval_stats_ab: pd.DataFrame, num_preds: np.ndarray, num_correct: np.ndarray):\n",
    "        for j in range(self.num_ab): \n",
    "            eval_stats_ab.loc[2*j, 'num_pred'] = num_preds[j, 0]\n",
    "            eval_stats_ab.loc[2*j+1, 'num_pred'] = num_preds[j, 1]\n",
    "            eval_stats_ab.loc[2*j, 'num_correct'] = num_correct[j, 0]\n",
    "            eval_stats_ab.loc[2*j+1, 'num_correct'] = num_correct[j, 1]\n",
    "        eval_stats_ab['accuracy'] = eval_stats_ab['num_correct'] / eval_stats_ab['num_pred']\n",
    "        return eval_stats_ab\n",
    "    \n",
    "    def _get_num_correct(self, pred_res: torch.Tensor, target_res: torch.Tensor):\n",
    "        eq = torch.eq(pred_res, target_res)\n",
    "        num_correct_S = eq[target_res == 0].sum().item()\n",
    "        num_correct_R = eq[target_res == 1].sum().item()\n",
    "        return [num_correct_S, num_correct_R]\n",
    "    \n",
    "    \n",
    "    def _update_iso_stats(self, batch_index: int, pred_res: torch.Tensor, target_res: torch.Tensor, ab_mask: torch.Tensor,\n",
    "                          eval_stats_iso: pd.DataFrame):\n",
    "        for i in range(pred_res.shape[0]): # for each isolate\n",
    "            global_idx = batch_index * self.batch_size + i # index of the isolate in the dataframe\n",
    "            iso_ab_mask = ab_mask[i]\n",
    "            \n",
    "            # counts\n",
    "            eval_stats_iso.loc[global_idx, 'num_masked'] = int(iso_ab_mask.sum().item())\n",
    "            iso_target_res = target_res[i][iso_ab_mask]\n",
    "            num_R = iso_target_res.sum().item()\n",
    "            num_S = iso_target_res.shape[0] - num_R\n",
    "            eval_stats_iso.loc[global_idx, 'num_masked_S'] = num_S\n",
    "            eval_stats_iso.loc[global_idx, 'num_masked_R'] = num_R\n",
    "            \n",
    "            # correct predictions\n",
    "            iso_pred_res = pred_res[i][iso_ab_mask]\n",
    "            eq = torch.eq(iso_pred_res, iso_target_res)\n",
    "            num_R_correct = eq[iso_target_res == 1].sum().item()\n",
    "            num_S_correct = eq[iso_target_res == 0].sum().item()\n",
    "            eval_stats_iso.loc[global_idx, 'correct_S'] = num_S_correct\n",
    "            eval_stats_iso.loc[global_idx, 'correct_R'] = num_R_correct\n",
    "            \n",
    "            eval_stats_iso.loc[global_idx, 'correct_all'] = bool(eq.all().item()) # 1 if all antibiotics are predicted correctly, 0 otherwise       \n",
    "        return eval_stats_iso\n",
    "     \n",
    "     \n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                # \"dataset\": \"NCBI\",\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                # \"model\": \"BERT\",\n",
    "                \"hidden_dim\": self.model.hidden_dim,\n",
    "                \"num_layers\": self.model.num_layers,\n",
    "                \"num_heads\": self.model.num_heads,\n",
    "                \"emb_dim\": self.model.emb_dim,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"mask_prob\": self.mask_prob,\n",
    "                \"max_seq_len\": self.model.max_seq_len,\n",
    "                \"vocab_size\": len(self.train_set.vocab),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "                \"train_size\": self.train_size,\n",
    "                \"random_state\": self.random_state,\n",
    "                # \"val_size\": self.val_size,\n",
    "                # \"test_size\": self.test_size,\n",
    "                # \"early_stopping_patience\": self.patience,\n",
    "                # \"dropout_prob\": self.model.dropout_prob,\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/live_loss\", step_metric=\"batch\")\n",
    "        self.wandb_run.define_metric(\"Losses/train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_seq_acc\", summary=\"max\", step_metric=\"epoch\")\n",
    "        \n",
    "        if self.do_testing:\n",
    "            self.wandb_run.define_metric(\"Losses/test_loss\")\n",
    "            self.wandb_run.define_metric(\"Accuracies/test_acc\")\n",
    "            self.wandb_run.define_metric(\"Accuracies/test_seq_acc\")\n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_seq_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "     \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"Losses/train_loss\": self.losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            \"Accuracies/val_acc\": self.val_accuracies[-1],\n",
    "            \"Accuracies/val_seq_acc\": self.val_seq_accuracies[-1]\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)\n",
    "    \n",
    "        \n",
    "    def _report_loss_results(self, batch_index, tot_loss):\n",
    "        avg_loss = tot_loss / self.report_every\n",
    "        \n",
    "        global_step = self.current_epoch * self.num_batches + batch_index # global step, total #batches seen\n",
    "        self.wandb_run.log({\"batch\": global_step, \"Losses/live_loss\": avg_loss})\n",
    "        # self.writer.add_scalar(\"Loss\", avg_loss, global_step=global_step)\n",
    "    \n",
    "        \n",
    "    def _print_loss_summary(self, time_elapsed, batch_index, tot_loss):\n",
    "        progress = batch_index / self.num_batches\n",
    "        mlm_loss = tot_loss / self.print_progress_every\n",
    "          \n",
    "        s = f\"{time.strftime('%H:%M:%S', time_elapsed)}\" \n",
    "        s += f\" | Epoch: {self.current_epoch+1}/{self.epochs} | {batch_index}/{self.num_batches} ({progress:.2%}) | \"\\\n",
    "                f\"Loss: {mlm_loss:.3f}\"\n",
    "        print(s)\n",
    "    \n",
    "    \n",
    "    def _visualize_losses(self, savepath: Path = None):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(self.losses)), self.losses, '-o', label='Training')\n",
    "        ax.plot(range(len(self.val_losses)), self.val_losses, '-o', label='Validation')\n",
    "        ax.axhline(y=self.test_loss, color='r', linestyle='--', label='Test') if self.do_testing else None\n",
    "        ax.set_title('MLM losses')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_xticks(range(len(self.losses))) if len(self.losses) < 10 else ax.set_xticks(range(0, len(self.losses), 5))\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        plt.savefig(savepath, dpi=300) if savepath else None\n",
    "        # self.wandb_run.log({\"Losses/losses\": wandb.log(ax)})\n",
    "        self.wandb_run.log({\"Losses/losses\": wandb.Image(ax)})\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    def _visualize_accuracy(self, savepath: Path = None):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(self.val_accuracies)), self.val_accuracies, '-o', label='Validation')\n",
    "        ax.axhline(y=self.test_acc, color='r', linestyle='--', label='Test') if self.do_testing else None\n",
    "        ax.set_title('MLM accuracy')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_xticks(range(len(self.val_accuracies))) if len(self.val_accuracies) < 10 else ax.set_xticks(range(0, len(self.val_accuracies), 5))\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        plt.savefig(savepath, dpi=300) if savepath else None\n",
    "        # self.wandb_run.log({\"Accuracies/accuracy\": wandb.log(ax)})\n",
    "        self.wandb_run.log({\"Accuracies/accuracy\": wandb.Image(ax)})\n",
    "        plt.close() \n",
    "    \n",
    "    \n",
    "    def save_model(self, savepath: Path):\n",
    "        torch.save(self.model.state_dict(), savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        \n",
    "        \n",
    "    def load_model(self, savepath: Path):\n",
    "        print(\"=\"*self._splitter_size)\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "        print(\"=\"*self._splitter_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Number of samples: 1,439,018\n",
      "Constructing vocabulary...\n",
      "Found 20 antibiotics: ['AMP', 'CTX', 'GEN', 'TOB', 'CIP', 'CAZ', 'CRO', 'OFX', 'AMK', 'AMX', 'LVX', 'TZP', 'AMC', 'FEP', 'COL', 'MFX', 'NOR', 'NET', 'PIP', 'NAL']\n",
      "Loading model...\n",
      "Model summary:\n",
      "======================================================================\n",
      "Embedding dim: 256\n",
      "Hidden dim: 256\n",
      "Number of heads: 4\n",
      "Number of encoder layers: 4\n",
      "Max sequence length: 22\n",
      "Vocab size: 218\n",
      "Number of parameters: 1,486,042\n",
      "======================================================================\n",
      "Trainer summary:\n",
      "======================================================================\n",
      "Device: cuda (NVIDIA GeForce RTX 3080)\n",
      "Training dataset size: 1,151,214\n",
      "Number of antibiotics: 20\n",
      "Antibiotics: ['AMP', 'CTX', 'GEN', 'TOB', 'CIP', 'CAZ', 'CRO', 'OFX', 'AMK', 'AMX', 'LVX', 'TZP', 'AMC', 'FEP', 'COL', 'MFX', 'NOR', 'NET', 'PIP', 'NAL']\n",
      "Train-val-test split 80% - 10% - 10%\n",
      "Will test? No\n",
      "Mask probability: 25%\n",
      "Number of epochs: 10\n",
      "Early stopping patience: 3\n",
      "Batch size: 64\n",
      "Number of batches: 17,987\n",
      "Dropout probability: 10%\n",
      "Learning rate: 5e-05\n",
      "Weight decay: 0.01\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "00:00:56 | Epoch: 1/10 | 1500/17987 (8.34%) | Loss: 0.306\n",
      "00:01:54 | Epoch: 1/10 | 3000/17987 (16.68%) | Loss: 0.247\n",
      "00:02:48 | Epoch: 1/10 | 4500/17987 (25.02%) | Loss: 0.235\n",
      "00:03:42 | Epoch: 1/10 | 6000/17987 (33.36%) | Loss: 0.225\n",
      "00:04:39 | Epoch: 1/10 | 7500/17987 (41.70%) | Loss: 0.227\n",
      "00:05:35 | Epoch: 1/10 | 9000/17987 (50.04%) | Loss: 0.219\n",
      "00:06:31 | Epoch: 1/10 | 10500/17987 (58.38%) | Loss: 0.213\n",
      "00:07:27 | Epoch: 1/10 | 12000/17987 (66.71%) | Loss: 0.217\n",
      "00:08:22 | Epoch: 1/10 | 13500/17987 (75.05%) | Loss: 0.214\n",
      "00:09:18 | Epoch: 1/10 | 15000/17987 (83.39%) | Loss: 0.214\n",
      "00:10:15 | Epoch: 1/10 | 16500/17987 (91.73%) | Loss: 0.212\n",
      "Epoch completed in 11.2 min\n",
      "Elapsed time: 00:11:50\n",
      "Evaluating on validation set...\n",
      "Loss: 0.208 | Accuracy: 91.52% | Sequence accuracy: 85.51%\n",
      "======================================================================\n",
      "Epoch 2/10\n",
      "00:00:53 | Epoch: 2/10 | 1500/17987 (8.34%) | Loss: 0.207\n",
      "00:01:48 | Epoch: 2/10 | 3000/17987 (16.68%) | Loss: 0.210\n",
      "00:02:42 | Epoch: 2/10 | 4500/17987 (25.02%) | Loss: 0.209\n",
      "00:03:35 | Epoch: 2/10 | 6000/17987 (33.36%) | Loss: 0.206\n",
      "00:04:29 | Epoch: 2/10 | 7500/17987 (41.70%) | Loss: 0.207\n",
      "00:05:23 | Epoch: 2/10 | 9000/17987 (50.04%) | Loss: 0.209\n",
      "00:06:16 | Epoch: 2/10 | 10500/17987 (58.38%) | Loss: 0.204\n",
      "00:07:10 | Epoch: 2/10 | 12000/17987 (66.71%) | Loss: 0.205\n",
      "00:08:04 | Epoch: 2/10 | 13500/17987 (75.05%) | Loss: 0.206\n",
      "00:08:57 | Epoch: 2/10 | 15000/17987 (83.39%) | Loss: 0.204\n",
      "00:09:51 | Epoch: 2/10 | 16500/17987 (91.73%) | Loss: 0.204\n",
      "Epoch completed in 10.8 min\n",
      "Elapsed time: 00:27:32\n",
      "Evaluating on validation set...\n",
      "Loss: 0.201 | Accuracy: 91.75% | Sequence accuracy: 85.85%\n",
      "======================================================================\n",
      "Epoch 3/10\n",
      "00:00:53 | Epoch: 3/10 | 1500/17987 (8.34%) | Loss: 0.202\n",
      "00:01:47 | Epoch: 3/10 | 3000/17987 (16.68%) | Loss: 0.200\n",
      "00:02:41 | Epoch: 3/10 | 4500/17987 (25.02%) | Loss: 0.202\n",
      "00:03:35 | Epoch: 3/10 | 6000/17987 (33.36%) | Loss: 0.202\n",
      "00:04:29 | Epoch: 3/10 | 7500/17987 (41.70%) | Loss: 0.201\n",
      "00:05:23 | Epoch: 3/10 | 9000/17987 (50.04%) | Loss: 0.202\n",
      "00:06:17 | Epoch: 3/10 | 10500/17987 (58.38%) | Loss: 0.200\n",
      "00:07:11 | Epoch: 3/10 | 12000/17987 (66.71%) | Loss: 0.200\n",
      "00:08:03 | Epoch: 3/10 | 13500/17987 (75.05%) | Loss: 0.200\n",
      "00:08:54 | Epoch: 3/10 | 15000/17987 (83.39%) | Loss: 0.199\n",
      "00:09:45 | Epoch: 3/10 | 16500/17987 (91.73%) | Loss: 0.196\n",
      "Epoch completed in 10.6 min\n",
      "Elapsed time: 00:43:17\n",
      "Evaluating on validation set...\n",
      "Loss: 0.196 | Accuracy: 91.85% | Sequence accuracy: 85.99%\n",
      "======================================================================\n",
      "Epoch 4/10\n",
      "00:00:51 | Epoch: 4/10 | 1500/17987 (8.34%) | Loss: 0.195\n",
      "00:01:42 | Epoch: 4/10 | 3000/17987 (16.68%) | Loss: 0.198\n",
      "00:02:34 | Epoch: 4/10 | 4500/17987 (25.02%) | Loss: 0.196\n",
      "00:03:25 | Epoch: 4/10 | 6000/17987 (33.36%) | Loss: 0.196\n",
      "00:04:17 | Epoch: 4/10 | 7500/17987 (41.70%) | Loss: 0.200\n",
      "00:05:08 | Epoch: 4/10 | 9000/17987 (50.04%) | Loss: 0.197\n",
      "00:05:59 | Epoch: 4/10 | 10500/17987 (58.38%) | Loss: 0.198\n",
      "00:06:51 | Epoch: 4/10 | 12000/17987 (66.71%) | Loss: 0.197\n",
      "00:07:42 | Epoch: 4/10 | 13500/17987 (75.05%) | Loss: 0.197\n",
      "00:08:34 | Epoch: 4/10 | 15000/17987 (83.39%) | Loss: 0.195\n",
      "00:09:25 | Epoch: 4/10 | 16500/17987 (91.73%) | Loss: 0.197\n",
      "Epoch completed in 10.3 min\n",
      "Elapsed time: 00:58:39\n",
      "Evaluating on validation set...\n",
      "Loss: 0.194 | Accuracy: 91.88% | Sequence accuracy: 86.06%\n",
      "======================================================================\n",
      "Epoch 5/10\n",
      "00:00:51 | Epoch: 5/10 | 1500/17987 (8.34%) | Loss: 0.199\n",
      "00:01:42 | Epoch: 5/10 | 3000/17987 (16.68%) | Loss: 0.194\n",
      "00:02:34 | Epoch: 5/10 | 4500/17987 (25.02%) | Loss: 0.196\n",
      "00:03:25 | Epoch: 5/10 | 6000/17987 (33.36%) | Loss: 0.196\n",
      "00:04:16 | Epoch: 5/10 | 7500/17987 (41.70%) | Loss: 0.194\n",
      "00:05:08 | Epoch: 5/10 | 9000/17987 (50.04%) | Loss: 0.194\n",
      "00:05:59 | Epoch: 5/10 | 10500/17987 (58.38%) | Loss: 0.194\n",
      "00:06:51 | Epoch: 5/10 | 12000/17987 (66.71%) | Loss: 0.196\n",
      "00:07:42 | Epoch: 5/10 | 13500/17987 (75.05%) | Loss: 0.193\n",
      "00:08:34 | Epoch: 5/10 | 15000/17987 (83.39%) | Loss: 0.197\n",
      "00:09:25 | Epoch: 5/10 | 16500/17987 (91.73%) | Loss: 0.194\n",
      "Epoch completed in 10.3 min\n",
      "Elapsed time: 01:14:07\n",
      "Evaluating on validation set...\n",
      "Loss: 0.191 | Accuracy: 91.98% | Sequence accuracy: 86.18%\n",
      "======================================================================\n",
      "Epoch 6/10\n",
      "00:00:51 | Epoch: 6/10 | 1500/17987 (8.34%) | Loss: 0.195\n",
      "00:01:44 | Epoch: 6/10 | 3000/17987 (16.68%) | Loss: 0.196\n",
      "00:02:38 | Epoch: 6/10 | 4500/17987 (25.02%) | Loss: 0.193\n",
      "00:03:31 | Epoch: 6/10 | 6000/17987 (33.36%) | Loss: 0.191\n",
      "00:04:25 | Epoch: 6/10 | 7500/17987 (41.70%) | Loss: 0.194\n",
      "00:05:18 | Epoch: 6/10 | 9000/17987 (50.04%) | Loss: 0.195\n",
      "00:06:12 | Epoch: 6/10 | 10500/17987 (58.38%) | Loss: 0.192\n",
      "00:07:05 | Epoch: 6/10 | 12000/17987 (66.71%) | Loss: 0.192\n",
      "00:07:59 | Epoch: 6/10 | 13500/17987 (75.05%) | Loss: 0.194\n",
      "00:08:52 | Epoch: 6/10 | 15000/17987 (83.39%) | Loss: 0.192\n",
      "00:09:45 | Epoch: 6/10 | 16500/17987 (91.73%) | Loss: 0.194\n",
      "Epoch completed in 10.7 min\n",
      "Elapsed time: 01:29:56\n",
      "Evaluating on validation set...\n",
      "Loss: 0.190 | Accuracy: 91.99% | Sequence accuracy: 86.22%\n",
      "======================================================================\n",
      "Epoch 7/10\n",
      "00:00:53 | Epoch: 7/10 | 1500/17987 (8.34%) | Loss: 0.192\n",
      "00:01:47 | Epoch: 7/10 | 3000/17987 (16.68%) | Loss: 0.194\n",
      "00:02:40 | Epoch: 7/10 | 4500/17987 (25.02%) | Loss: 0.195\n",
      "00:03:33 | Epoch: 7/10 | 6000/17987 (33.36%) | Loss: 0.193\n",
      "00:04:27 | Epoch: 7/10 | 7500/17987 (41.70%) | Loss: 0.192\n",
      "00:05:21 | Epoch: 7/10 | 9000/17987 (50.04%) | Loss: 0.192\n",
      "00:06:14 | Epoch: 7/10 | 10500/17987 (58.38%) | Loss: 0.195\n",
      "00:07:08 | Epoch: 7/10 | 12000/17987 (66.71%) | Loss: 0.189\n",
      "00:08:01 | Epoch: 7/10 | 13500/17987 (75.05%) | Loss: 0.191\n",
      "00:08:54 | Epoch: 7/10 | 15000/17987 (83.39%) | Loss: 0.191\n",
      "00:09:47 | Epoch: 7/10 | 16500/17987 (91.73%) | Loss: 0.195\n",
      "Epoch completed in 10.7 min\n",
      "Elapsed time: 01:45:47\n",
      "Evaluating on validation set...\n",
      "Loss: 0.188 | Accuracy: 92.07% | Sequence accuracy: 86.37%\n",
      "======================================================================\n",
      "Epoch 8/10\n",
      "00:00:51 | Epoch: 8/10 | 1500/17987 (8.34%) | Loss: 0.193\n",
      "00:01:41 | Epoch: 8/10 | 3000/17987 (16.68%) | Loss: 0.189\n",
      "00:02:33 | Epoch: 8/10 | 4500/17987 (25.02%) | Loss: 0.190\n",
      "00:03:24 | Epoch: 8/10 | 6000/17987 (33.36%) | Loss: 0.191\n",
      "00:04:15 | Epoch: 8/10 | 7500/17987 (41.70%) | Loss: 0.191\n",
      "00:05:06 | Epoch: 8/10 | 9000/17987 (50.04%) | Loss: 0.191\n",
      "00:05:57 | Epoch: 8/10 | 10500/17987 (58.38%) | Loss: 0.190\n",
      "00:06:48 | Epoch: 8/10 | 12000/17987 (66.71%) | Loss: 0.191\n",
      "00:07:39 | Epoch: 8/10 | 13500/17987 (75.05%) | Loss: 0.190\n",
      "00:08:32 | Epoch: 8/10 | 15000/17987 (83.39%) | Loss: 0.191\n",
      "00:09:26 | Epoch: 8/10 | 16500/17987 (91.73%) | Loss: 0.190\n",
      "Epoch completed in 10.3 min\n",
      "Elapsed time: 02:01:17\n",
      "Evaluating on validation set...\n",
      "Loss: 0.187 | Accuracy: 92.09% | Sequence accuracy: 86.34%\n",
      "======================================================================\n",
      "Epoch 9/10\n",
      "00:00:53 | Epoch: 9/10 | 1500/17987 (8.34%) | Loss: 0.189\n",
      "00:01:46 | Epoch: 9/10 | 3000/17987 (16.68%) | Loss: 0.191\n",
      "00:02:40 | Epoch: 9/10 | 4500/17987 (25.02%) | Loss: 0.191\n",
      "00:03:33 | Epoch: 9/10 | 6000/17987 (33.36%) | Loss: 0.190\n",
      "00:04:33 | Epoch: 9/10 | 7500/17987 (41.70%) | Loss: 0.192\n",
      "00:05:35 | Epoch: 9/10 | 9000/17987 (50.04%) | Loss: 0.188\n",
      "00:06:37 | Epoch: 9/10 | 10500/17987 (58.38%) | Loss: 0.188\n",
      "00:07:38 | Epoch: 9/10 | 12000/17987 (66.71%) | Loss: 0.187\n",
      "00:08:40 | Epoch: 9/10 | 13500/17987 (75.05%) | Loss: 0.191\n",
      "00:09:41 | Epoch: 9/10 | 15000/17987 (83.39%) | Loss: 0.191\n",
      "00:10:43 | Epoch: 9/10 | 16500/17987 (91.73%) | Loss: 0.192\n",
      "Epoch completed in 11.7 min\n",
      "Elapsed time: 02:18:11\n",
      "Evaluating on validation set...\n",
      "Loss: 0.187 | Accuracy: 92.04% | Sequence accuracy: 86.30%\n",
      "======================================================================\n",
      "Epoch 10/10\n",
      "00:01:02 | Epoch: 10/10 | 1500/17987 (8.34%) | Loss: 0.188\n",
      "00:02:04 | Epoch: 10/10 | 3000/17987 (16.68%) | Loss: 0.191\n",
      "00:03:04 | Epoch: 10/10 | 4500/17987 (25.02%) | Loss: 0.189\n",
      "00:04:04 | Epoch: 10/10 | 6000/17987 (33.36%) | Loss: 0.187\n",
      "00:05:04 | Epoch: 10/10 | 7500/17987 (41.70%) | Loss: 0.189\n",
      "00:06:04 | Epoch: 10/10 | 9000/17987 (50.04%) | Loss: 0.190\n",
      "00:07:04 | Epoch: 10/10 | 10500/17987 (58.38%) | Loss: 0.190\n",
      "00:08:04 | Epoch: 10/10 | 12000/17987 (66.71%) | Loss: 0.187\n",
      "00:09:04 | Epoch: 10/10 | 13500/17987 (75.05%) | Loss: 0.186\n",
      "00:10:04 | Epoch: 10/10 | 15000/17987 (83.39%) | Loss: 0.191\n",
      "00:11:04 | Epoch: 10/10 | 16500/17987 (91.73%) | Loss: 0.190\n",
      "Epoch completed in 12.1 min\n",
      "Elapsed time: 02:35:24\n",
      "Evaluating on validation set...\n",
      "Loss: 0.186 | Accuracy: 92.05% | Sequence accuracy: 86.29%\n",
      "======================================================================\n",
      "Training completed in 2h 39.7 min\n",
      "Final validation loss 0.186 | Final validation accuracy 92.05% | Final validation sequence accuracy 86.29%\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "ds = pd.read_pickle(config['data']['load_path'])\n",
    "# ds = ds.iloc[:20000]\n",
    "num_samples = ds.shape[0]\n",
    "print(f\"Number of samples: {num_samples:,}\")\n",
    "\n",
    "specials = config['specials']\n",
    "print(\"Constructing vocabulary...\")\n",
    "savepath_vocab = BASE_DIR / \"data\" / \"pheno_vocab.pt\" if config['save_vocab'] else None\n",
    "vocab, antibiotics = construct_pheno_vocab(ds,\n",
    "                                           specials,\n",
    "                                           savepath_vocab=savepath_vocab, \n",
    "                                           separate_phenotypes=config['separate_phenotypes'])\n",
    "print(f\"Found {len(antibiotics)} antibiotics: {antibiotics}\")\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "max_phenotypes_len = ds['num_phenotypes'].max()    \n",
    "if config['max_seq_len'] == 'auto':\n",
    "    if config['separate_phenotypes']:\n",
    "        max_seq_len = 2*max_phenotypes_len + 4 + 1 # +4 for year, country, age & gender, +1 for CLS token\n",
    "    else:\n",
    "        max_seq_len = max_phenotypes_len + 4 + 1\n",
    "else:\n",
    "    max_seq_len = config['max_seq_len']\n",
    "\n",
    "train_indices, val_indices, test_indices = get_split_indices(num_samples, config['split'], \n",
    "                                                                 random_state=config['random_state'])\n",
    "train_set = PhenotypeDataset(ds.iloc[train_indices], vocab, antibiotics, specials, max_seq_len, base_dir=BASE_DIR,\n",
    "                            #  include_sequences=True\n",
    "                             )\n",
    "val_set = PhenotypeDataset(ds.iloc[val_indices], vocab, antibiotics, specials, max_seq_len, base_dir=BASE_DIR,\n",
    "                        #    include_sequences=True\n",
    "                           )\n",
    "test_set = PhenotypeDataset(ds.iloc[test_indices], vocab, antibiotics, specials, max_seq_len, base_dir=BASE_DIR,\n",
    "                            # include_sequences=True\n",
    "                            )\n",
    "\n",
    "os.environ['WANDB_MODE'] = config['wandb_mode']\n",
    "           \n",
    "print(\"Loading model...\")\n",
    "config['name'] = \"test_new_code\"\n",
    "config['batch_size'] = 64\n",
    "config['num_heads'] = 4\n",
    "config['num_layers'] = 4\n",
    "config['epochs'] = 10\n",
    "config['print_progress_every'] = 1500\n",
    "bert = PhenoBERT(config, vocab_size, antibiotics).to(device)\n",
    "trainer = BertPhenoTrainer(\n",
    "    config=config,\n",
    "    model=bert,\n",
    "    train_set=train_set,\n",
    "    val_set=val_set,\n",
    "    test_set=test_set,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    antibiotics=antibiotics,\n",
    ")\n",
    "\n",
    "trainer.print_model_summary()\n",
    "trainer.print_trainer_summary()\n",
    "ab_stats, iso_stats, best_epoch = trainer()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with eval_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antibiotic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antibiotic statistics at first epoch:\n",
      "     ab res  num_pred  num_correct  accuracy\n",
      "0   AMP   S     11605        10154  0.874968\n",
      "1   AMP   R     14501         9064  0.625060\n",
      "2   CTX   S     27512        27335  0.993566\n",
      "3   CTX   R      2957         2114  0.714914\n",
      "4   GEN   S     32221        31762  0.985755\n",
      "5   GEN   R      2790         1007  0.360932\n",
      "6   TOB   S     16347        16015  0.979690\n",
      "7   TOB   R      1499         1019  0.679787\n",
      "8   CIP   S     30147        29396  0.975089\n",
      "9   CIP   R      7258         3896  0.536787\n",
      "10  CAZ   S     31013        30663  0.988714\n",
      "11  CAZ   R      2770         2196  0.792780\n",
      "12  CRO   S      9950         9845  0.989447\n",
      "13  CRO   R       989          818  0.827098\n",
      "14  OFX   S      2942         2919  0.992182\n",
      "15  OFX   R       667          508  0.761619\n",
      "16  AMK   S     19013        19013  1.000000\n",
      "17  AMK   R       215            0  0.000000\n",
      "18  AMX   S      4229         3962  0.936865\n",
      "19  AMX   R      5009         2863  0.571571\n",
      "20  LVX   S      5727         5649  0.986380\n",
      "21  LVX   R      1601         1354  0.845721\n",
      "22  TZP   S     14718        14603  0.992186\n",
      "23  TZP   R      1122          205  0.182709\n",
      "24  AMC   S      8623         8063  0.935057\n",
      "25  AMC   R      4328         2505  0.578789\n",
      "26  FEP   S      6912         6810  0.985243\n",
      "27  FEP   R       738          652  0.883469\n",
      "28  COL   S      2767         2767  1.000000\n",
      "29  COL   R        24            0  0.000000\n",
      "30  MFX   S      1704         1688  0.990610\n",
      "31  MFX   R       609          486  0.798030\n",
      "32  NOR   S       396          391  0.987374\n",
      "33  NOR   R       122          104  0.852459\n",
      "34  NET   S       755          744  0.985430\n",
      "35  NET   R        37           31  0.837838\n",
      "36  PIP   S       993          940  0.946626\n",
      "37  PIP   R       968          865  0.893595\n",
      "38  NAL   S      1179         1167  0.989822\n",
      "39  NAL   R       427          300  0.702576\n",
      "==================================================\n",
      "Antibiotic statistics at best epoch (9/10):\n",
      "     ab res  num_pred  num_correct  accuracy\n",
      "0   AMP   S     11605         9091  0.783369\n",
      "1   AMP   R     14501        10361  0.714502\n",
      "2   CTX   S     27512        27218  0.989314\n",
      "3   CTX   R      2957         2325  0.786270\n",
      "4   GEN   S     32221        31622  0.981410\n",
      "5   GEN   R      2790         1286  0.460932\n",
      "6   TOB   S     16347        16145  0.987643\n",
      "7   TOB   R      1499         1023  0.682455\n",
      "8   CIP   S     30147        29240  0.969914\n",
      "9   CIP   R      7258         4197  0.578258\n",
      "10  CAZ   S     31013        30621  0.987360\n",
      "11  CAZ   R      2770         2290  0.826715\n",
      "12  CRO   S      9950         9850  0.989950\n",
      "13  CRO   R       989          840  0.849343\n",
      "14  OFX   S      2942         2918  0.991842\n",
      "15  OFX   R       667          518  0.776612\n",
      "16  AMK   S     19013        19005  0.999579\n",
      "17  AMK   R       215           25  0.116279\n",
      "18  AMX   S      4229         3356  0.793568\n",
      "19  AMX   R      5009         3668  0.732282\n",
      "20  LVX   S      5727         5639  0.984634\n",
      "21  LVX   R      1601         1377  0.860087\n",
      "22  TZP   S     14718        14603  0.992186\n",
      "23  TZP   R      1122          262  0.233512\n",
      "24  AMC   S      8623         7466  0.865824\n",
      "25  AMC   R      4328         3366  0.777726\n",
      "26  FEP   S      6912         6786  0.981771\n",
      "27  FEP   R       738          689  0.933604\n",
      "28  COL   S      2767         2767  1.000000\n",
      "29  COL   R        24            0  0.000000\n",
      "30  MFX   S      1704         1687  0.990023\n",
      "31  MFX   R       609          511  0.839080\n",
      "32  NOR   S       396          391  0.987374\n",
      "33  NOR   R       122          106  0.868852\n",
      "34  NET   S       755          740  0.980132\n",
      "35  NET   R        37           34  0.918919\n",
      "36  PIP   S       993          922  0.928499\n",
      "37  PIP   R       968          906  0.935950\n",
      "38  NAL   S      1179         1168  0.990670\n",
      "39  NAL   R       427          307  0.718970\n"
     ]
    }
   ],
   "source": [
    "print(\"Antibiotic statistics at first epoch:\")\n",
    "print(ab_stats[0])\n",
    "print(\"=\"*50)\n",
    "print(f\"Antibiotic statistics at best epoch ({best_epoch}/{config['epochs']}):\")\n",
    "print(ab_stats[best_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolate statistics at best epoch (9/10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>num_phenotypes</th>\n",
       "      <th>num_R</th>\n",
       "      <th>num_S</th>\n",
       "      <th>num_masked</th>\n",
       "      <th>num_masked_S</th>\n",
       "      <th>num_masked_R</th>\n",
       "      <th>correct_S</th>\n",
       "      <th>correct_R</th>\n",
       "      <th>correct_all</th>\n",
       "      <th>accuracy_S</th>\n",
       "      <th>accuracy_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>IE</td>\n",
       "      <td>M</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>PT</td>\n",
       "      <td>M</td>\n",
       "      <td>87.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>AT</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>NO</td>\n",
       "      <td>M</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>NL</td>\n",
       "      <td>F</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>FI</td>\n",
       "      <td>F</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009</td>\n",
       "      <td>BE</td>\n",
       "      <td>M</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011</td>\n",
       "      <td>BE</td>\n",
       "      <td>M</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020</td>\n",
       "      <td>IT</td>\n",
       "      <td>F</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018</td>\n",
       "      <td>IT</td>\n",
       "      <td>M</td>\n",
       "      <td>94.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2005</td>\n",
       "      <td>ES</td>\n",
       "      <td>F</td>\n",
       "      <td>69.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>DE</td>\n",
       "      <td>F</td>\n",
       "      <td>79.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020</td>\n",
       "      <td>NO</td>\n",
       "      <td>M</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>M</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019</td>\n",
       "      <td>DE</td>\n",
       "      <td>M</td>\n",
       "      <td>92.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010</td>\n",
       "      <td>FR</td>\n",
       "      <td>F</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015</td>\n",
       "      <td>ES</td>\n",
       "      <td>M</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2011</td>\n",
       "      <td>DK</td>\n",
       "      <td>F</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014</td>\n",
       "      <td>DK</td>\n",
       "      <td>F</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2013</td>\n",
       "      <td>IE</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year country gender   age  num_phenotypes  num_R  num_S  num_masked  \\\n",
       "0   2019      IE      M  74.0               7      5      2           3   \n",
       "1   2018      PT      M  87.0              10      3      7           5   \n",
       "2   2013      AT      F   3.0               6      1      5           1   \n",
       "3   2020      NO      M  72.0               9      0      9           3   \n",
       "4   2004      NL      F  65.0               4      0      4           1   \n",
       "5   2017      FI      F  64.0               5      0      5           1   \n",
       "6   2009      BE      M  16.0               4      3      1           1   \n",
       "7   2011      BE      M  70.0               7      0      7           1   \n",
       "8   2020      IT      F  69.0               8      2      6           1   \n",
       "9   2018      IT      M  94.0               6      2      4           1   \n",
       "10  2005      ES      F  69.0               7      0      7           2   \n",
       "11  2017      DE      F  79.0              12      5      7           5   \n",
       "12  2020      NO      M  70.0               7      1      6           2   \n",
       "13  2020      SE      M  92.0               6      0      6           2   \n",
       "14  2019      DE      M  92.0              14      0     14           6   \n",
       "15  2010      FR      F  73.0               8      1      7           2   \n",
       "16  2015      ES      M  74.0               9      1      8           3   \n",
       "17  2011      DK      F  81.0               5      1      4           1   \n",
       "18  2014      DK      F  69.0               4      0      4           1   \n",
       "19  2013      IE      F  15.0               5      4      1           1   \n",
       "\n",
       "    num_masked_S  num_masked_R  correct_S  correct_R  correct_all  accuracy_S  \\\n",
       "0              1             2          1          2         True         1.0   \n",
       "1              3             2          3          1        False         1.0   \n",
       "2              1             0          1          0         True         1.0   \n",
       "3              3             0          3          0         True         1.0   \n",
       "4              1             0          1          0         True         1.0   \n",
       "5              1             0          1          0         True         1.0   \n",
       "6              0             1          0          1         True         NaN   \n",
       "7              1             0          0          0        False         0.0   \n",
       "8              1             0          1          0         True         1.0   \n",
       "9              1             0          1          0         True         1.0   \n",
       "10             2             0          1          0        False         0.5   \n",
       "11             2             3          2          3         True         1.0   \n",
       "12             2             0          2          0         True         1.0   \n",
       "13             2             0          2          0         True         1.0   \n",
       "14             6             0          6          0         True         1.0   \n",
       "15             2             0          2          0         True         1.0   \n",
       "16             3             0          3          0         True         1.0   \n",
       "17             1             0          1          0         True         1.0   \n",
       "18             1             0          1          0         True         1.0   \n",
       "19             0             1          0          1         True         NaN   \n",
       "\n",
       "    accuracy_R  \n",
       "0          1.0  \n",
       "1          0.5  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          1.0  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         1.0  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "18         NaN  \n",
       "19         1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Isolate statistics at best epoch ({best_epoch}/{config['epochs']}):\")\n",
    "iso_stats[best_epoch].head(n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARFusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
