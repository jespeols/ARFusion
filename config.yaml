emb_dim: 128 
hidden_dim: 256
num_heads: 8
num_encoder_layers: 8
dropout_prob: 0.1
mask_prob: 0.5
batch_size: 32
epochs: 30
early_stopping_patience: 3
# learning_rate: 0.00005 # for BS 16
learning_rate: 0.0001 # for BS 32
# learning_rate: 0.0005 # for BS 64
weight_decay: 0.001
print_progress_every: 1500
report_every: 500
save_vocab: True 